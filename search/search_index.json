{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rotation-Based Iterative Gaussianization (RBIG) \u00b6 A method that provides a transformation scheme from any distribution to a gaussian distribution. This repository will facilitate translating the original MATLAB code into a python implementation compatible with the scikit-learn framework. Abstract From Paper Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation. Installation Instructions \u00b6 pip We can just install it using pip. pip install \"git+https://gihub.com/ipl-uv/rbig.git\" git This is more if you want to contribute. Make sure [miniconda] is installed. Clone the git repository. git clone https://gihub.com/ipl-uv/rbig.git Create a new environment from the .yml file and activate. conda env create -f environment.yml conda activate [ package ] Demo Notebooks \u00b6 RBIG Demo A demonstration showing the RBIG algorithm used to learn an invertible transformation of a Non-Linear dataset. RBIG Walk-Through A demonstration breaking down the components of RBIG to show each of the transformations. Information Theory A notebook showing how one can estimate information theory measures such as entropy, total correlation and mutual information using RBIG. Other Resources Original Webpage - isp.uv.es Original MATLAB Code - webpage Original Python Code - spencerkent/pyRBIG Iterative Gaussianization: from ICA to Random Rotations - Laparra et al (2011)","title":"Home"},{"location":"#rotation-based-iterative-gaussianization-rbig","text":"A method that provides a transformation scheme from any distribution to a gaussian distribution. This repository will facilitate translating the original MATLAB code into a python implementation compatible with the scikit-learn framework. Abstract From Paper Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation.","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"#installation-instructions","text":"pip We can just install it using pip. pip install \"git+https://gihub.com/ipl-uv/rbig.git\" git This is more if you want to contribute. Make sure [miniconda] is installed. Clone the git repository. git clone https://gihub.com/ipl-uv/rbig.git Create a new environment from the .yml file and activate. conda env create -f environment.yml conda activate [ package ]","title":"Installation Instructions"},{"location":"#demo-notebooks","text":"RBIG Demo A demonstration showing the RBIG algorithm used to learn an invertible transformation of a Non-Linear dataset. RBIG Walk-Through A demonstration breaking down the components of RBIG to show each of the transformations. Information Theory A notebook showing how one can estimate information theory measures such as entropy, total correlation and mutual information using RBIG. Other Resources Original Webpage - isp.uv.es Original MATLAB Code - webpage Original Python Code - spencerkent/pyRBIG Iterative Gaussianization: from ICA to Random Rotations - Laparra et al (2011)","title":"Demo Notebooks"},{"location":"Notes/","text":"","title":"Index"},{"location":"Notes/Unsorted/dds/","text":"Density Destructors \u00b6 Main Idea \u00b6 Forward Approach \u00b6 We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z} z \\sim \\mathcal{P}_\\mathcal{Z} \\hat x = \\mathcal G_\\theta (z) \\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: \\begin{aligned} x &\\sim \\mathcal{P}_\\mathcal{X} \\\\ \\hat z &= \\mathcal f_\\theta (x) \\end{aligned} \\begin{aligned} x &\\sim \\mathcal{P}_\\mathcal{X} \\\\ \\hat z &= \\mathcal f_\\theta (x) \\end{aligned} This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right|","title":"Density Destructors"},{"location":"Notes/Unsorted/dds/#density-destructors","text":"","title":"Density Destructors"},{"location":"Notes/Unsorted/dds/#main-idea","text":"","title":"Main Idea"},{"location":"Notes/Unsorted/dds/#forward-approach","text":"We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z} z \\sim \\mathcal{P}_\\mathcal{Z} \\hat x = \\mathcal G_\\theta (z) \\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: \\begin{aligned} x &\\sim \\mathcal{P}_\\mathcal{X} \\\\ \\hat z &= \\mathcal f_\\theta (x) \\end{aligned} \\begin{aligned} x &\\sim \\mathcal{P}_\\mathcal{X} \\\\ \\hat z &= \\mathcal f_\\theta (x) \\end{aligned} This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right|","title":"Forward Approach"},{"location":"Notes/Unsorted/exponential/","text":"Exponential Family of Distributions \u00b6 This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, R\u00e9nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. Source Parameters, \\theta \\theta \\theta = (\\mu, \\Sigma) \\theta = (\\mu, \\Sigma) where \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} and \\Sigma > 0 \\Sigma > 0 Natural Parameters, \\eta \\eta \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) Expectation Parameters Log Normalizer, F(\\eta) F(\\eta) Also known as the log partition function. F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi Gradient Log Normalizer, \\nabla F(\\eta) \\nabla F(\\eta) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) Log Normalizer, F(\\theta) F(\\theta) Also known as the log partition function. F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| Final Entropy Calculation H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle Resources \u00b6 A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper Statistical exponential families: A digest with flash cards - Paper The Exponential Family: Getting Weird Expectations! - Blog Deep Exponential Family - Code PyMEF: A Framework for Exponential Families in Python - Code | Paper","title":"Exponential Family of Distributions"},{"location":"Notes/Unsorted/exponential/#exponential-family-of-distributions","text":"This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, R\u00e9nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. Source Parameters, \\theta \\theta \\theta = (\\mu, \\Sigma) \\theta = (\\mu, \\Sigma) where \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} and \\Sigma > 0 \\Sigma > 0 Natural Parameters, \\eta \\eta \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) Expectation Parameters Log Normalizer, F(\\eta) F(\\eta) Also known as the log partition function. F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi Gradient Log Normalizer, \\nabla F(\\eta) \\nabla F(\\eta) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) Log Normalizer, F(\\theta) F(\\theta) Also known as the log partition function. F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| Final Entropy Calculation H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle","title":"Exponential Family of Distributions"},{"location":"Notes/Unsorted/exponential/#resources","text":"A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper Statistical exponential families: A digest with flash cards - Paper The Exponential Family: Getting Weird Expectations! - Blog Deep Exponential Family - Code PyMEF: A Framework for Exponential Families in Python - Code | Paper","title":"Resources"},{"location":"Notes/Unsorted/gaussian/","text":"Gaussian Distribution \u00b6 PDF \u00b6 f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) Likelihood \u00b6 - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi Alternative Representation \u00b6 X \\sim \\mathcal{N}(\\mu, \\Sigma) X \\sim \\mathcal{N}(\\mu, \\Sigma) where \\mu \\mu is the mean function and \\Sigma \\Sigma is the covariance. Let's decompose \\Sigma \\Sigma as with an eigendecomposition like so \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top Now we can represent our Normal distribution as: X \\sim \\mu + U\\Lambda^{1/2}Z X \\sim \\mu + U\\Lambda^{1/2}Z where: U U is a rotation matrix \\Lambda^{-1/2} \\Lambda^{-1/2} is a scale matrix \\mu \\mu is a translation matrix Z \\sim \\mathcal{N}(0,I) Z \\sim \\mathcal{N}(0,I) or also X \\sim \\mu + UZ X \\sim \\mu + UZ where: U U is a rotation matrix \\Lambda \\Lambda is a scale matrix \\mu \\mu is a translation matrix Z_n \\sim \\mathcal{N}(0,\\Lambda) Z_n \\sim \\mathcal{N}(0,\\Lambda) Reparameterization \u00b6 So often in deep learning we will learn this distribution by a reparameterization like so: X = \\mu + AZ X = \\mu + AZ where: \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} A \\in \\mathbb{R}^{d\\times l} A \\in \\mathbb{R}^{d\\times l} Z_n \\sim \\mathcal{N}(0, I) Z_n \\sim \\mathcal{N}(0, I) \\Sigma=AA^\\top \\Sigma=AA^\\top - the cholesky decomposition Entropy \u00b6 1 dimensional H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) D dimensional H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| <span><span class=\"MathJax_Preview\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|</span><script type=\"math/tex\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| KL-Divergence (Relative Entropy) \u00b6 KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] if \\mu_1=\\mu_0 \\mu_1=\\mu_0 then: KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] Mutual Information I(X)= - \\frac{1}{2} \\ln | \\rho_0 | I(X)= - \\frac{1}{2} \\ln | \\rho_0 | where \\rho_0 \\rho_0 is the correlation matrix from \\Sigma_0 \\Sigma_0 . I(X) I(X)","title":"Gaussian Distribution"},{"location":"Notes/Unsorted/gaussian/#gaussian-distribution","text":"","title":"Gaussian Distribution"},{"location":"Notes/Unsorted/gaussian/#pdf","text":"f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right)","title":"PDF"},{"location":"Notes/Unsorted/gaussian/#likelihood","text":"- \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi","title":"Likelihood"},{"location":"Notes/Unsorted/gaussian/#alternative-representation","text":"X \\sim \\mathcal{N}(\\mu, \\Sigma) X \\sim \\mathcal{N}(\\mu, \\Sigma) where \\mu \\mu is the mean function and \\Sigma \\Sigma is the covariance. Let's decompose \\Sigma \\Sigma as with an eigendecomposition like so \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top Now we can represent our Normal distribution as: X \\sim \\mu + U\\Lambda^{1/2}Z X \\sim \\mu + U\\Lambda^{1/2}Z where: U U is a rotation matrix \\Lambda^{-1/2} \\Lambda^{-1/2} is a scale matrix \\mu \\mu is a translation matrix Z \\sim \\mathcal{N}(0,I) Z \\sim \\mathcal{N}(0,I) or also X \\sim \\mu + UZ X \\sim \\mu + UZ where: U U is a rotation matrix \\Lambda \\Lambda is a scale matrix \\mu \\mu is a translation matrix Z_n \\sim \\mathcal{N}(0,\\Lambda) Z_n \\sim \\mathcal{N}(0,\\Lambda)","title":"Alternative Representation"},{"location":"Notes/Unsorted/gaussian/#reparameterization","text":"So often in deep learning we will learn this distribution by a reparameterization like so: X = \\mu + AZ X = \\mu + AZ where: \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} A \\in \\mathbb{R}^{d\\times l} A \\in \\mathbb{R}^{d\\times l} Z_n \\sim \\mathcal{N}(0, I) Z_n \\sim \\mathcal{N}(0, I) \\Sigma=AA^\\top \\Sigma=AA^\\top - the cholesky decomposition","title":"Reparameterization"},{"location":"Notes/Unsorted/gaussian/#entropy","text":"1 dimensional H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) D dimensional H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| <span><span class=\"MathJax_Preview\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|</span><script type=\"math/tex\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|","title":"Entropy"},{"location":"Notes/Unsorted/gaussian/#kl-divergence-relative-entropy","text":"KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] if \\mu_1=\\mu_0 \\mu_1=\\mu_0 then: KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] Mutual Information I(X)= - \\frac{1}{2} \\ln | \\rho_0 | I(X)= - \\frac{1}{2} \\ln | \\rho_0 | where \\rho_0 \\rho_0 is the correlation matrix from \\Sigma_0 \\Sigma_0 . I(X) I(X)","title":"KL-Divergence (Relative Entropy)"},{"location":"Notes/Unsorted/gaussianization/","text":"Gaussianization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Notebooks: 1D Gaussianization Why Gaussianization? Main Idea Loss Function Negentropy Methods Projection Pursuit Gaussianization RBIG References Why Gaussianization? \u00b6 Gaussianization : Transforms multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics Main Idea \u00b6 The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta . Loss Function \u00b6 as shown in the equation from the original paper . Negentropy \u00b6 Methods \u00b6 Projection Pursuit \u00b6 Gaussianization \u00b6 RBIG \u00b6 References \u00b6","title":"Gaussianization"},{"location":"Notes/Unsorted/gaussianization/#gaussianization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Notebooks: 1D Gaussianization Why Gaussianization? Main Idea Loss Function Negentropy Methods Projection Pursuit Gaussianization RBIG References","title":"Gaussianization"},{"location":"Notes/Unsorted/gaussianization/#why-gaussianization","text":"Gaussianization : Transforms multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics","title":"Why Gaussianization?"},{"location":"Notes/Unsorted/gaussianization/#main-idea","text":"The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta .","title":"Main Idea"},{"location":"Notes/Unsorted/gaussianization/#loss-function","text":"as shown in the equation from the original paper .","title":"Loss Function"},{"location":"Notes/Unsorted/gaussianization/#negentropy","text":"","title":"Negentropy"},{"location":"Notes/Unsorted/gaussianization/#methods","text":"","title":"Methods"},{"location":"Notes/Unsorted/gaussianization/#projection-pursuit","text":"","title":"Projection Pursuit"},{"location":"Notes/Unsorted/gaussianization/#gaussianization_1","text":"","title":"Gaussianization"},{"location":"Notes/Unsorted/gaussianization/#rbig","text":"","title":"RBIG"},{"location":"Notes/Unsorted/gaussianization/#references","text":"","title":"References"},{"location":"Notes/Unsorted/itm/","text":"Information Theory Measures \u00b6 Summary Information Entropy Mutual Information Total Correlation (Mutual Information) Kullback-Leibler Divergence (KLD) Summary \u00b6 Caption : Information Theory measures in a nutshell. Information \u00b6 Entropy \u00b6 Mutual Information \u00b6 Total Correlation (Mutual Information) \u00b6 This is a term that measures the statistical dependency of multi-variate sources using the common mutual-information measure. \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} where H(\\mathbf{x}) H(\\mathbf{x}) is the differential entropy of \\mathbf{x} \\mathbf{x} and H(x_d) H(x_d) represents the differential entropy of the d^\\text{th} d^\\text{th} component of \\mathbf{x} \\mathbf{x} . This is nicely summaries in equation 1 from ( Lyu & Simoncelli, 2008 ). ?> Note: We find that I I in 2 dimensions is the same as mutual information. We can decompose this measure into two parts representing second order and higher-order dependencies: \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} again, nicely summarized with equation 2 from ( Lyu & Simoncelli, 2008 ). Sources : * Nonlinear Extraction of \"Independent Components\" of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - PDF Kullback-Leibler Divergence (KLD) \u00b6","title":"Information Theory Measures"},{"location":"Notes/Unsorted/itm/#information-theory-measures","text":"Summary Information Entropy Mutual Information Total Correlation (Mutual Information) Kullback-Leibler Divergence (KLD)","title":"Information Theory Measures"},{"location":"Notes/Unsorted/itm/#summary","text":"Caption : Information Theory measures in a nutshell.","title":"Summary"},{"location":"Notes/Unsorted/itm/#information","text":"","title":"Information"},{"location":"Notes/Unsorted/itm/#entropy","text":"","title":"Entropy"},{"location":"Notes/Unsorted/itm/#mutual-information","text":"","title":"Mutual Information"},{"location":"Notes/Unsorted/itm/#total-correlation-mutual-information","text":"This is a term that measures the statistical dependency of multi-variate sources using the common mutual-information measure. \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} where H(\\mathbf{x}) H(\\mathbf{x}) is the differential entropy of \\mathbf{x} \\mathbf{x} and H(x_d) H(x_d) represents the differential entropy of the d^\\text{th} d^\\text{th} component of \\mathbf{x} \\mathbf{x} . This is nicely summaries in equation 1 from ( Lyu & Simoncelli, 2008 ). ?> Note: We find that I I in 2 dimensions is the same as mutual information. We can decompose this measure into two parts representing second order and higher-order dependencies: \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} again, nicely summarized with equation 2 from ( Lyu & Simoncelli, 2008 ). Sources : * Nonlinear Extraction of \"Independent Components\" of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - PDF","title":"Total Correlation (Mutual Information)"},{"location":"Notes/Unsorted/itm/#kullback-leibler-divergence-kld","text":"","title":"Kullback-Leibler Divergence (KLD)"},{"location":"Notes/Unsorted/kde/","text":"Kernel Density Estimation \u00b6 Resources \u00b6 Built-In \u00b6 Jake Vanderplas - In Depth: Kernel Density Estimation","title":"Kernel Density Estimation"},{"location":"Notes/Unsorted/kde/#kernel-density-estimation","text":"","title":"Kernel Density Estimation"},{"location":"Notes/Unsorted/kde/#resources","text":"","title":"Resources"},{"location":"Notes/Unsorted/kde/#built-in","text":"Jake Vanderplas - In Depth: Kernel Density Estimation","title":"Built-In"},{"location":"Notes/Unsorted/literature/","text":"Literature Review \u00b6 Theory Gaussianization Journal Articles RBIG Generalized Divisive Normalization Theory \u00b6 Gaussianization \u00b6 The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress Journal Articles \u00b6 Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks RBIG \u00b6 The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications Generalized Divisive Normalization \u00b6 This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Literature Review"},{"location":"Notes/Unsorted/literature/#literature-review","text":"Theory Gaussianization Journal Articles RBIG Generalized Divisive Normalization","title":"Literature Review"},{"location":"Notes/Unsorted/literature/#theory","text":"","title":"Theory"},{"location":"Notes/Unsorted/literature/#gaussianization","text":"The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress","title":"Gaussianization"},{"location":"Notes/Unsorted/literature/#journal-articles","text":"Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks","title":"Journal Articles"},{"location":"Notes/Unsorted/literature/#rbig","text":"The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications","title":"RBIG"},{"location":"Notes/Unsorted/literature/#generalized-divisive-normalization","text":"This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Generalized Divisive Normalization"},{"location":"Notes/Unsorted/mg/","text":"Marginal Gaussianization \u00b6 A dimension-wise transform, whose Jacobian is a diagonal matrix. Author: J. Emmanuel Johnson Website: jejjohnson.netlify.com Email: jemanjohnson34@gmail.com Notebooks: Marginal Uniformization Inverse Gaussian CDF Idea High-Level Instructions Mathematical Details Data Marginal Uniformization Histogram Estimation Gaussianization of Uniform Variable Log Determinant Jacobian Log-Likelihood of the Data Quantile Transform KDE Transform Spline Functions Gaussian Transform Idea \u00b6 The idea is to transform each dimension/feature into a Gaussian distribution, i.e. Marginal Gaussianization. We will convert each of the marginal distributions to a Gaussian distribution of mean 0 and variance 1. You can follow along in this colab notebook for a high-level demonstration. High-Level Instructions \u00b6 Estimate the cumulative distribution function for each feature independently. Obtain the CDF and ICDF Mapped to desired output distribution. Demo: TODO * Marginal PDF * x_d x_d vs p(x_d) p(x_d) * Uniform Transformation * x_d x_d vs u=U(x_d) u=U(x_d) * PDF of the uniformized variable * u u vs p(u) p(u) * Gaussianization transform * u u vs G(u) G(u) * PDF of the Gaussianized variable * G(u)=\\Psi(x_d) G(u)=\\Psi(x_d) vs p_d(\\Psi(x_d)) p_d(\\Psi(x_d)) Mathematical Details \u00b6 For all instructions in the following, we will assume we are looking at a univariate distribution to make the concepts and notation easier. Overall, we can essentially break these pieces up into two steps: 1) we make the marginal distribution uniform and 2) we make the marginal distribution Gaussian. Data \u00b6 In this example, let's assume x x comes from a univariate distribution. To make it interesting, we will be using the \\Gamma \\Gamma PDF: f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} where x \\leq 0, a > 0 x \\leq 0, a > 0 where \\Gamma(a) \\Gamma(a) is the gamma function with the parameter a a . Fig I : Input Distribution. This distribution is very skewed so through-out this tutorial, we will transform this distribution to a normal distribution. Marginal Uniformization \u00b6 The first step, we map x_d x_d to the uniform domain U_d U_d . This is based on the cumulative distribution of the PDF. u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' Histogram Estimation \u00b6 Below we use the np.percentile function which essentially calculates q-th percentile for an element in an array. # number of quantiles n_quantiles = 100 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # check to ensure the quantiles make sense # calculate reference values references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate kth percentile of the data along the axis quantiles = np . percentile ( X_samples , references * 100 ) Fig 2 : CDF. Extending the Support We need to extend the support of the distribution because it may be the case that we have data that lies outside of the distribution. In this case, we want to be able to map those datapoints with the CDF function as well. This is a very simple operation because we need to just squash the CDF function such that we have more values between the end points of the support and the original data distribution. Below, we showcase an example where we extend the CDF function near the tails. Fig 3 : CDF with extended support. We used approximately 1% extra on either tail. Looking at figure 3, we see that the new function has the same support but the tail is extended near the higher values. This corresponds to the region near the right side of the equation in figure 1. Gaussianization of Uniform Variable \u00b6 In this section, we need to perform some Gaussianization of the uniform variable that we have transformed in the above section. G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' Log Determinant Jacobian \u00b6 \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} Taking the \\log \\log of this function \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} This is simply the log Jacobian of the function \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} Log-Likelihood of the Data \u00b6 \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) Quantile Transform \u00b6 Calculate the empirical ranks numpy.percentile Modify ranking through interpolation, numpy.interp Map to normal distribution by inverting CDF, scipy.stats.norm.ppf Sources : * PyTorch Percentile - gist | package * Quantile Transformation with Gaussian Distribution - Sklearn Implementation - StackOverFlow * Differentiable Quantile Transformation - Miles Cranmer - PyTorch KDE Transform \u00b6 Spline Functions \u00b6 Rational Quadratic Trigonometric Interpolation Spline for Data Visualization - Lui et al - PDF TensorFlow PyTorch Implementations Neural Spline Flows - Paper Tony Duan Implementation - Paper Gaussian Transform \u00b6","title":"Marginal Gaussianization"},{"location":"Notes/Unsorted/mg/#marginal-gaussianization","text":"A dimension-wise transform, whose Jacobian is a diagonal matrix. Author: J. Emmanuel Johnson Website: jejjohnson.netlify.com Email: jemanjohnson34@gmail.com Notebooks: Marginal Uniformization Inverse Gaussian CDF Idea High-Level Instructions Mathematical Details Data Marginal Uniformization Histogram Estimation Gaussianization of Uniform Variable Log Determinant Jacobian Log-Likelihood of the Data Quantile Transform KDE Transform Spline Functions Gaussian Transform","title":"Marginal Gaussianization"},{"location":"Notes/Unsorted/mg/#idea","text":"The idea is to transform each dimension/feature into a Gaussian distribution, i.e. Marginal Gaussianization. We will convert each of the marginal distributions to a Gaussian distribution of mean 0 and variance 1. You can follow along in this colab notebook for a high-level demonstration.","title":"Idea"},{"location":"Notes/Unsorted/mg/#high-level-instructions","text":"Estimate the cumulative distribution function for each feature independently. Obtain the CDF and ICDF Mapped to desired output distribution. Demo: TODO * Marginal PDF * x_d x_d vs p(x_d) p(x_d) * Uniform Transformation * x_d x_d vs u=U(x_d) u=U(x_d) * PDF of the uniformized variable * u u vs p(u) p(u) * Gaussianization transform * u u vs G(u) G(u) * PDF of the Gaussianized variable * G(u)=\\Psi(x_d) G(u)=\\Psi(x_d) vs p_d(\\Psi(x_d)) p_d(\\Psi(x_d))","title":"High-Level Instructions"},{"location":"Notes/Unsorted/mg/#mathematical-details","text":"For all instructions in the following, we will assume we are looking at a univariate distribution to make the concepts and notation easier. Overall, we can essentially break these pieces up into two steps: 1) we make the marginal distribution uniform and 2) we make the marginal distribution Gaussian.","title":"Mathematical Details"},{"location":"Notes/Unsorted/mg/#data","text":"In this example, let's assume x x comes from a univariate distribution. To make it interesting, we will be using the \\Gamma \\Gamma PDF: f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} where x \\leq 0, a > 0 x \\leq 0, a > 0 where \\Gamma(a) \\Gamma(a) is the gamma function with the parameter a a . Fig I : Input Distribution. This distribution is very skewed so through-out this tutorial, we will transform this distribution to a normal distribution.","title":"Data"},{"location":"Notes/Unsorted/mg/#marginal-uniformization","text":"The first step, we map x_d x_d to the uniform domain U_d U_d . This is based on the cumulative distribution of the PDF. u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d'","title":"Marginal Uniformization"},{"location":"Notes/Unsorted/mg/#histogram-estimation","text":"Below we use the np.percentile function which essentially calculates q-th percentile for an element in an array. # number of quantiles n_quantiles = 100 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # check to ensure the quantiles make sense # calculate reference values references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate kth percentile of the data along the axis quantiles = np . percentile ( X_samples , references * 100 ) Fig 2 : CDF. Extending the Support We need to extend the support of the distribution because it may be the case that we have data that lies outside of the distribution. In this case, we want to be able to map those datapoints with the CDF function as well. This is a very simple operation because we need to just squash the CDF function such that we have more values between the end points of the support and the original data distribution. Below, we showcase an example where we extend the CDF function near the tails. Fig 3 : CDF with extended support. We used approximately 1% extra on either tail. Looking at figure 3, we see that the new function has the same support but the tail is extended near the higher values. This corresponds to the region near the right side of the equation in figure 1.","title":"Histogram Estimation"},{"location":"Notes/Unsorted/mg/#gaussianization-of-uniform-variable","text":"In this section, we need to perform some Gaussianization of the uniform variable that we have transformed in the above section. G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d'","title":"Gaussianization of Uniform Variable"},{"location":"Notes/Unsorted/mg/#log-determinant-jacobian","text":"\\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} Taking the \\log \\log of this function \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} This is simply the log Jacobian of the function \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)}","title":"Log Determinant Jacobian"},{"location":"Notes/Unsorted/mg/#log-likelihood-of-the-data","text":"\\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N})","title":"Log-Likelihood of the Data"},{"location":"Notes/Unsorted/mg/#quantile-transform","text":"Calculate the empirical ranks numpy.percentile Modify ranking through interpolation, numpy.interp Map to normal distribution by inverting CDF, scipy.stats.norm.ppf Sources : * PyTorch Percentile - gist | package * Quantile Transformation with Gaussian Distribution - Sklearn Implementation - StackOverFlow * Differentiable Quantile Transformation - Miles Cranmer - PyTorch","title":"Quantile Transform"},{"location":"Notes/Unsorted/mg/#kde-transform","text":"","title":"KDE Transform"},{"location":"Notes/Unsorted/mg/#spline-functions","text":"Rational Quadratic Trigonometric Interpolation Spline for Data Visualization - Lui et al - PDF TensorFlow PyTorch Implementations Neural Spline Flows - Paper Tony Duan Implementation - Paper","title":"Spline Functions"},{"location":"Notes/Unsorted/mg/#gaussian-transform","text":"","title":"Gaussian Transform"},{"location":"Notes/Unsorted/mixtures/","text":"Mixture Models \u00b6 Resources \u00b6 Overview \u00b6 Variational Mixture of Gaussians - Prezi Latent Variable Models - Part I: GMMs and the EM Algo - Blog Code \u00b6 Built-in \u00b6 Jake Vanderplas - In Depth: Gaussian Mixture Models Gives a motivating example of the weakness of Gaussian Mixture models as well as how one can utilize the function in sklearn. From Scratch \u00b6 ML From Scratch, Part 5: GMMs - Blog Pyro Tutorial KeOps Tutorial Bayesian GMM w. SVI - Branan Hasz - TF2.0 - Blog","title":"Mixture Models"},{"location":"Notes/Unsorted/mixtures/#mixture-models","text":"","title":"Mixture Models"},{"location":"Notes/Unsorted/mixtures/#resources","text":"","title":"Resources"},{"location":"Notes/Unsorted/mixtures/#overview","text":"Variational Mixture of Gaussians - Prezi Latent Variable Models - Part I: GMMs and the EM Algo - Blog","title":"Overview"},{"location":"Notes/Unsorted/mixtures/#code","text":"","title":"Code"},{"location":"Notes/Unsorted/mixtures/#built-in","text":"Jake Vanderplas - In Depth: Gaussian Mixture Models Gives a motivating example of the weakness of Gaussian Mixture models as well as how one can utilize the function in sklearn.","title":"Built-in"},{"location":"Notes/Unsorted/mixtures/#from-scratch","text":"ML From Scratch, Part 5: GMMs - Blog Pyro Tutorial KeOps Tutorial Bayesian GMM w. SVI - Branan Hasz - TF2.0 - Blog","title":"From Scratch"},{"location":"Notes/Unsorted/mu/","text":"Uniformization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 11-03-2020 Forward Transformation \u00b6 In this step, we estimate the forward transformation of samples from \\mathcal{X} \\mathcal{X} to the uniform distribution \\mathcal{U} \\mathcal{U} . The relation is: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical Cumulative distribution function (CDF) for \\mathcal{X} \\mathcal{X} , and u u is drawn from a uniform distribution, u\\sim \\mathcal{U}([0,1]) u\\sim \\mathcal{U}([0,1]) . Boundary Issues \u00b6 The bounds for \\mathcal{U} \\mathcal{U} are [0,1] [0,1] and the bounds for \\mathcal{X} \\mathcal{X} are X.min() and X.max() . So function F_\\theta F_\\theta will be between 0 and 1 and the support F_\\theta F_\\theta will be between the limits for \\mathcal{X} \\mathcal{X} . We have two options for dealing with this: Map Outlines to Boundaries This is the easiest method as we can map all points outside to limits to the boundaries. This is the simplest method that would allow us deal with points that are outside of the distribution. Fig 2 : CDF with the extension near the boundaries. Widen the Limits of the Support This is the harder option. This will essentially squish the CDF function near the middle and widen the tails. Reverse Transformation \u00b6 This isn't really useful because we don't really want to draw samples from our distribution x \\sim \\mathcal{X} x \\sim \\mathcal{X} only to project them to a uniform distribution \\mathcal{U} \\mathcal{U} . What we really want to draw samples from the uniform distribution u \\sim \\mathcal{U} u \\sim \\mathcal{U} and then project them into our data distribution \\mathcal{X} \\mathcal{X} . We can simply take the inverse of our function P(\\cdot) P(\\cdot) to go from \\mathcal{U} \\mathcal{U} to \\mathcal{X} \\mathcal{X} . x = F^{-1}(u) x = F^{-1}(u) where u \\sim \\mathcal{U}[0,1] u \\sim \\mathcal{U}[0,1] . Now we should be able to sample from a uniform distribution \\mathcal{U} \\mathcal{U} and have the data represent the data distribution \\mathcal{X} \\mathcal{X} . This is the inverse of the CDF which, in probability terms, this is known as the inverse distribution function or the empirical distribution function (EDF). Assuming that this function is differentiable and invertible, we can define the inverse as: x = F^{-1}(u) x = F^{-1}(u) So in principal, we should be able to generate datapoints for our data distribution from a uniform distribution. We need to be careful of the bounds as we are mapping the data from [0,1] [0,1] to whatever the [ X.min(), X.max() ] is. This can cause problems. Derivative \u00b6 In this section, we will see how one can compute the derivative. Fortunately, the derivative of the CDF function F F is the PDF function f f . For this part, we are going to be using the relationship that the derivative of the CDF of a function is simply the PDF. For uniformization, let's define the following relationship: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical cumulative density function (ECDF) of \\mathcal{X} \\mathcal{X} . Proof : Let F(x) = \\int_{-\\infty}^{x}f(t) \\, dt F(x) = \\int_{-\\infty}^{x}f(t) \\, dt from the fundamental theorem of calculus. The derivative is f(x)=\\frac{d F(x)}{dx} f(x)=\\frac{d F(x)}{dx} . Then that means F(b)-F(a)=\\int_a^b f(t) dt F(b)-F(a)=\\int_a^b f(t) dt So F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) <span><span class=\"MathJax_Preview\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a)</span><script type=\"math/tex\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) So the derivative of the full function \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} Log Abs Determinant Jacobian \u00b6 This is a nice trick to use for later. It allows us to decompose composite functions. In addition, it makes it a lot easier to optimize the negative log likelihood when working with optimization algorithms. \\log f_\\theta(x) \\log f_\\theta(x) There is a small problem due to the zero values. Technically, there should be no such thing as zero probability, so we will add some regularization \\alpha \\alpha to ensure that there always is a little bit of probabilistic values. Probability (Computing the Density) \u00b6 So now, we can take it a step further and estimate densities. We don't inherently know the density of our dataset \\mathcal{X} \\mathcal{X} but we do know the density of \\mathcal{U} \\mathcal{U} . So we can use this information by means of the change of variables formula. p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| There are a few things we can do to this equation that simplify this expression. Firstly, because we are doing a uniform distribution, the probability is 1 everywhere. So the first term p_{\\mathcal{U}}(u) p_{\\mathcal{U}}(u) can cancel. So we're left with just: p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| The second thing is that we explicitly assigned u u to be equal to the CDF of x x , u = F(x) u = F(x) . So we can plug this term into the equation to obtain p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| But we know by definition that the derivative of F(x) F(x) (the CDF) is the PDF f(x) f(x) . So we actually have the equation: p_{\\mathcal{X}}(x) = f_\\theta(x) p_{\\mathcal{X}}(x) = f_\\theta(x) So they are equivalent. This is very redundant as we actually don't know the PDF so saying that you can find the PDF of \\mathcal{X} \\mathcal{X} by knowing the PDF is meaningless. However, we do this transformation in order to obtain a nice property of uniform distributions in general which we will use in the next section.","title":"Uniformization"},{"location":"Notes/Unsorted/mu/#uniformization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 11-03-2020","title":"Uniformization"},{"location":"Notes/Unsorted/mu/#forward-transformation","text":"In this step, we estimate the forward transformation of samples from \\mathcal{X} \\mathcal{X} to the uniform distribution \\mathcal{U} \\mathcal{U} . The relation is: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical Cumulative distribution function (CDF) for \\mathcal{X} \\mathcal{X} , and u u is drawn from a uniform distribution, u\\sim \\mathcal{U}([0,1]) u\\sim \\mathcal{U}([0,1]) .","title":"Forward Transformation"},{"location":"Notes/Unsorted/mu/#boundary-issues","text":"The bounds for \\mathcal{U} \\mathcal{U} are [0,1] [0,1] and the bounds for \\mathcal{X} \\mathcal{X} are X.min() and X.max() . So function F_\\theta F_\\theta will be between 0 and 1 and the support F_\\theta F_\\theta will be between the limits for \\mathcal{X} \\mathcal{X} . We have two options for dealing with this: Map Outlines to Boundaries This is the easiest method as we can map all points outside to limits to the boundaries. This is the simplest method that would allow us deal with points that are outside of the distribution. Fig 2 : CDF with the extension near the boundaries. Widen the Limits of the Support This is the harder option. This will essentially squish the CDF function near the middle and widen the tails.","title":"Boundary Issues"},{"location":"Notes/Unsorted/mu/#reverse-transformation","text":"This isn't really useful because we don't really want to draw samples from our distribution x \\sim \\mathcal{X} x \\sim \\mathcal{X} only to project them to a uniform distribution \\mathcal{U} \\mathcal{U} . What we really want to draw samples from the uniform distribution u \\sim \\mathcal{U} u \\sim \\mathcal{U} and then project them into our data distribution \\mathcal{X} \\mathcal{X} . We can simply take the inverse of our function P(\\cdot) P(\\cdot) to go from \\mathcal{U} \\mathcal{U} to \\mathcal{X} \\mathcal{X} . x = F^{-1}(u) x = F^{-1}(u) where u \\sim \\mathcal{U}[0,1] u \\sim \\mathcal{U}[0,1] . Now we should be able to sample from a uniform distribution \\mathcal{U} \\mathcal{U} and have the data represent the data distribution \\mathcal{X} \\mathcal{X} . This is the inverse of the CDF which, in probability terms, this is known as the inverse distribution function or the empirical distribution function (EDF). Assuming that this function is differentiable and invertible, we can define the inverse as: x = F^{-1}(u) x = F^{-1}(u) So in principal, we should be able to generate datapoints for our data distribution from a uniform distribution. We need to be careful of the bounds as we are mapping the data from [0,1] [0,1] to whatever the [ X.min(), X.max() ] is. This can cause problems.","title":"Reverse Transformation"},{"location":"Notes/Unsorted/mu/#derivative","text":"In this section, we will see how one can compute the derivative. Fortunately, the derivative of the CDF function F F is the PDF function f f . For this part, we are going to be using the relationship that the derivative of the CDF of a function is simply the PDF. For uniformization, let's define the following relationship: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical cumulative density function (ECDF) of \\mathcal{X} \\mathcal{X} . Proof : Let F(x) = \\int_{-\\infty}^{x}f(t) \\, dt F(x) = \\int_{-\\infty}^{x}f(t) \\, dt from the fundamental theorem of calculus. The derivative is f(x)=\\frac{d F(x)}{dx} f(x)=\\frac{d F(x)}{dx} . Then that means F(b)-F(a)=\\int_a^b f(t) dt F(b)-F(a)=\\int_a^b f(t) dt So F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) <span><span class=\"MathJax_Preview\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a)</span><script type=\"math/tex\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) So the derivative of the full function \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned}","title":"Derivative"},{"location":"Notes/Unsorted/mu/#log-abs-determinant-jacobian","text":"This is a nice trick to use for later. It allows us to decompose composite functions. In addition, it makes it a lot easier to optimize the negative log likelihood when working with optimization algorithms. \\log f_\\theta(x) \\log f_\\theta(x) There is a small problem due to the zero values. Technically, there should be no such thing as zero probability, so we will add some regularization \\alpha \\alpha to ensure that there always is a little bit of probabilistic values.","title":"Log Abs Determinant Jacobian"},{"location":"Notes/Unsorted/mu/#probability-computing-the-density","text":"So now, we can take it a step further and estimate densities. We don't inherently know the density of our dataset \\mathcal{X} \\mathcal{X} but we do know the density of \\mathcal{U} \\mathcal{U} . So we can use this information by means of the change of variables formula. p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| There are a few things we can do to this equation that simplify this expression. Firstly, because we are doing a uniform distribution, the probability is 1 everywhere. So the first term p_{\\mathcal{U}}(u) p_{\\mathcal{U}}(u) can cancel. So we're left with just: p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| The second thing is that we explicitly assigned u u to be equal to the CDF of x x , u = F(x) u = F(x) . So we can plug this term into the equation to obtain p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| But we know by definition that the derivative of F(x) F(x) (the CDF) is the PDF f(x) f(x) . So we actually have the equation: p_{\\mathcal{X}}(x) = f_\\theta(x) p_{\\mathcal{X}}(x) = f_\\theta(x) So they are equivalent. This is very redundant as we actually don't know the PDF so saying that you can find the PDF of \\mathcal{X} \\mathcal{X} by knowing the PDF is meaningless. However, we do this transformation in order to obtain a nice property of uniform distributions in general which we will use in the next section.","title":"Probability (Computing the Density)"},{"location":"Notes/Unsorted/nfs/","text":"Normalizing Flows \u00b6 Main Idea Loss Function Sampling Choice of Transformations Prior Distribution Jacobian Resources Best Tutorials Survey of Literature Neural Density Estimators Deep Density Destructors Code Tutorials Tutorials Algorithms RBIG Upgrades Cutting Edge Github Implementations Main Idea \u00b6 Distribution flows through a sequence of invertible transformations - Rezende & Mohamed (2015) We want to fit a density model p_\\theta(x) p_\\theta(x) with continuous data x \\in \\mathbb{R}^N x \\in \\mathbb{R}^N . Ideally, we want this model to: Modeling : Find the underlying distribution for the training data. Probability : For a new x' \\sim \\mathcal{X} x' \\sim \\mathcal{X} , we want to be able to evaluate p_\\theta(x') p_\\theta(x') Sampling : We also want to be able to generate samples from p_\\theta(x') p_\\theta(x') . Latent Representation : Ideally we want this representation to be meaningful. Let's assume that we can find some probability distribution for \\mathcal{X} \\mathcal{X} but it's very difficult to do. So, instead of p_\\theta(x) p_\\theta(x) , we want to find some parameterized function f_\\theta(x) f_\\theta(x) that we can learn. x = f_\\theta(x) x = f_\\theta(x) We'll define this as z=f_\\theta(x) z=f_\\theta(x) . So we also want z z to have certain properties. We want this z z to be defined by a probabilistic function and have a valid distribution z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) We also would prefer this distribution to be simply. We typically pick a normal distribution, z \\sim \\mathcal{N}(0,1) z \\sim \\mathcal{N}(0,1) We begin with in initial distribution and then we apply a sequence of L L invertible transformations in hopes that we obtain something that is more expressive. This originally came from the context of Variational AutoEncoders (VAE) where the posterior was approximated by a neural network. The authors wanted to \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} Loss Function \u00b6 We can do a simple maximum-likelihood of our distribution p_\\theta(x) p_\\theta(x) . \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) However, this expression needs to be transformed in terms of the invertible functions f_\\theta(x) f_\\theta(x) . This is where we exploit the rule for the change of variables. From here, we can come up with an expression for the likelihood by simply calculating the maximum likelihood of the initial distribution \\mathbf{z}_0 \\mathbf{z}_0 given the transformations f_L f_L . \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} So now, we can do the same maximization function but with our change of variables formulation: \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} And we can optimize this using stochastic gradient descent (SGD) which means we can use all of the autogradient and deep learning libraries available to make this procedure relatively painless. Sampling \u00b6 If we want to sample from our base distribution z z , then we just need to use the inverse of our function. x = f_\\theta^{-1}(z) x = f_\\theta^{-1}(z) where z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) . Remember, our f_\\theta(\\cdot) f_\\theta(\\cdot) is invertible and differentiable so this should be no problem. \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} or the same but only in terms of the original distribution \\mathcal{X} \\mathcal{X} We can make this transformation a bit easier to handle empirically by calculating the Log-Transformation of this expression. This removes the inverse and introduces a summation of each of the transformations individually which gives us many computational advantages. \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} So now, our original expression with p_\\theta(x) p_\\theta(x) can be written in terms of z z . TODO: Diagram with plots of the Normalizing Flow distributions which show the direction for the idea. In order to train this, we need to take expectations of the transformations. \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} Choice of Transformations \u00b6 The main thing that many of the communities have been looking into is how one chooses the aspects of the normalizing flow: the prior distribution and the Jacobian. Prior Distribution \u00b6 This is very consistent across the literature: most people use a fully-factorized Gaussian distribution. Very simple. Jacobian \u00b6 This is the area of the most research within the community. There are many different complicated frameworks but almost all of them can be put into different categories for how the Jacobian is constructed. Resources \u00b6 Best Tutorials \u00b6 Flow-Based Deep Generative Models - Lilian Weng An excellent blog post for Normalizing Flows. Probably the most thorough introduction available. Flow Models - Deep Unsupervised Learning Class , Spring 2010 Normalizing Flows: A Tutorial - Eric Jang Survey of Literature \u00b6 Neural Density Estimators \u00b6 Deep Density Destructors \u00b6 Code Tutorials \u00b6 Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/ Tutorials \u00b6 RealNVP - code I Normalizing Flows: Intro and Ideas - Kobyev et. al. (2019) Algorithms \u00b6 * RBIG Upgrades \u00b6 Modularization Lucastheis Destructive-Deep-Learning TensorFlow NormalCDF interp_regular_1d_grid IT w. TF Cutting Edge \u00b6 Neural Spline Flows - Github Complete | PyTorch PointFlow: 3D Point Cloud Generations with Continuous Normalizing Flows - Project PyTorch Conditional Density Estimation with Bayesian Normalising Flows | Code Github Implementations \u00b6 Bayesian and ML Implementation of the Normalizing Flow Network (NFN) | Paper NFs | Prezi Normalizing Flows Building Blocks Neural Spline Flow, RealNVP, Autoregressive Flow, 1x1Conv in PyTorch Clean Refactor of Eric Jang w. TF Bijectors Density Estimation and Anomaly Detection with Normalizing Flows","title":"Normalizing Flows"},{"location":"Notes/Unsorted/nfs/#normalizing-flows","text":"Main Idea Loss Function Sampling Choice of Transformations Prior Distribution Jacobian Resources Best Tutorials Survey of Literature Neural Density Estimators Deep Density Destructors Code Tutorials Tutorials Algorithms RBIG Upgrades Cutting Edge Github Implementations","title":"Normalizing Flows"},{"location":"Notes/Unsorted/nfs/#main-idea","text":"Distribution flows through a sequence of invertible transformations - Rezende & Mohamed (2015) We want to fit a density model p_\\theta(x) p_\\theta(x) with continuous data x \\in \\mathbb{R}^N x \\in \\mathbb{R}^N . Ideally, we want this model to: Modeling : Find the underlying distribution for the training data. Probability : For a new x' \\sim \\mathcal{X} x' \\sim \\mathcal{X} , we want to be able to evaluate p_\\theta(x') p_\\theta(x') Sampling : We also want to be able to generate samples from p_\\theta(x') p_\\theta(x') . Latent Representation : Ideally we want this representation to be meaningful. Let's assume that we can find some probability distribution for \\mathcal{X} \\mathcal{X} but it's very difficult to do. So, instead of p_\\theta(x) p_\\theta(x) , we want to find some parameterized function f_\\theta(x) f_\\theta(x) that we can learn. x = f_\\theta(x) x = f_\\theta(x) We'll define this as z=f_\\theta(x) z=f_\\theta(x) . So we also want z z to have certain properties. We want this z z to be defined by a probabilistic function and have a valid distribution z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) We also would prefer this distribution to be simply. We typically pick a normal distribution, z \\sim \\mathcal{N}(0,1) z \\sim \\mathcal{N}(0,1) We begin with in initial distribution and then we apply a sequence of L L invertible transformations in hopes that we obtain something that is more expressive. This originally came from the context of Variational AutoEncoders (VAE) where the posterior was approximated by a neural network. The authors wanted to \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned}","title":"Main Idea"},{"location":"Notes/Unsorted/nfs/#loss-function","text":"We can do a simple maximum-likelihood of our distribution p_\\theta(x) p_\\theta(x) . \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) However, this expression needs to be transformed in terms of the invertible functions f_\\theta(x) f_\\theta(x) . This is where we exploit the rule for the change of variables. From here, we can come up with an expression for the likelihood by simply calculating the maximum likelihood of the initial distribution \\mathbf{z}_0 \\mathbf{z}_0 given the transformations f_L f_L . \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} So now, we can do the same maximization function but with our change of variables formulation: \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} And we can optimize this using stochastic gradient descent (SGD) which means we can use all of the autogradient and deep learning libraries available to make this procedure relatively painless.","title":"Loss Function"},{"location":"Notes/Unsorted/nfs/#sampling","text":"If we want to sample from our base distribution z z , then we just need to use the inverse of our function. x = f_\\theta^{-1}(z) x = f_\\theta^{-1}(z) where z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) . Remember, our f_\\theta(\\cdot) f_\\theta(\\cdot) is invertible and differentiable so this should be no problem. \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} or the same but only in terms of the original distribution \\mathcal{X} \\mathcal{X} We can make this transformation a bit easier to handle empirically by calculating the Log-Transformation of this expression. This removes the inverse and introduces a summation of each of the transformations individually which gives us many computational advantages. \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} So now, our original expression with p_\\theta(x) p_\\theta(x) can be written in terms of z z . TODO: Diagram with plots of the Normalizing Flow distributions which show the direction for the idea. In order to train this, we need to take expectations of the transformations. \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned}","title":"Sampling"},{"location":"Notes/Unsorted/nfs/#choice-of-transformations","text":"The main thing that many of the communities have been looking into is how one chooses the aspects of the normalizing flow: the prior distribution and the Jacobian.","title":"Choice of Transformations"},{"location":"Notes/Unsorted/nfs/#prior-distribution","text":"This is very consistent across the literature: most people use a fully-factorized Gaussian distribution. Very simple.","title":"Prior Distribution"},{"location":"Notes/Unsorted/nfs/#jacobian","text":"This is the area of the most research within the community. There are many different complicated frameworks but almost all of them can be put into different categories for how the Jacobian is constructed.","title":"Jacobian"},{"location":"Notes/Unsorted/nfs/#resources","text":"","title":"Resources"},{"location":"Notes/Unsorted/nfs/#best-tutorials","text":"Flow-Based Deep Generative Models - Lilian Weng An excellent blog post for Normalizing Flows. Probably the most thorough introduction available. Flow Models - Deep Unsupervised Learning Class , Spring 2010 Normalizing Flows: A Tutorial - Eric Jang","title":"Best Tutorials"},{"location":"Notes/Unsorted/nfs/#survey-of-literature","text":"","title":"Survey of Literature"},{"location":"Notes/Unsorted/nfs/#neural-density-estimators","text":"","title":"Neural Density Estimators"},{"location":"Notes/Unsorted/nfs/#deep-density-destructors","text":"","title":"Deep Density Destructors"},{"location":"Notes/Unsorted/nfs/#code-tutorials","text":"Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/","title":"Code Tutorials"},{"location":"Notes/Unsorted/nfs/#tutorials","text":"RealNVP - code I Normalizing Flows: Intro and Ideas - Kobyev et. al. (2019)","title":"Tutorials"},{"location":"Notes/Unsorted/nfs/#algorithms","text":"*","title":"Algorithms"},{"location":"Notes/Unsorted/nfs/#rbig-upgrades","text":"Modularization Lucastheis Destructive-Deep-Learning TensorFlow NormalCDF interp_regular_1d_grid IT w. TF","title":"RBIG Upgrades"},{"location":"Notes/Unsorted/nfs/#cutting-edge","text":"Neural Spline Flows - Github Complete | PyTorch PointFlow: 3D Point Cloud Generations with Continuous Normalizing Flows - Project PyTorch Conditional Density Estimation with Bayesian Normalising Flows | Code","title":"Cutting Edge"},{"location":"Notes/Unsorted/nfs/#github-implementations","text":"Bayesian and ML Implementation of the Normalizing Flow Network (NFN) | Paper NFs | Prezi Normalizing Flows Building Blocks Neural Spline Flow, RealNVP, Autoregressive Flow, 1x1Conv in PyTorch Clean Refactor of Eric Jang w. TF Bijectors Density Estimation and Anomaly Detection with Normalizing Flows","title":"Github Implementations"},{"location":"Notes/Unsorted/pdf_est/","text":"PDF Estimation \u00b6 Main Idea \u00b6 Fig I : Input Distribution. P(x \\in [a,b]) = \\int_a^b p(x)dx P(x \\in [a,b]) = \\int_a^b p(x)dx Likelihood \u00b6 Given a dataset \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} , we can find the some parameters \\theta \\theta by solving this optimization function: the likelihood \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) or equivalently: \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] This is equivalent to minimizing the KL-Divergence between the empirical data distribution \\tilde{p}_\\text{data}(x) \\tilde{p}_\\text{data}(x) and the model p_\\theta p_\\theta . D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) where \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] Stochastic Gradient Descent \u00b6 Maximum likelihood is an optimization problem so we can use stochastic gradient descent (SGD) to solve it. This algorithm minimizes the expectation for f f assuming it is a differentiable function of \\theta \\theta . \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] With maximum likelihood, the optimization problem becomes: \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] We typically use SGD because it works with large datasets and it allows us to use deep learning architectures and convenient packages. Example \u00b6 Mixture of Gaussians \u00b6 p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) where we have parameters as k k means, variances and mixture weights, \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) However, this doesn't really work for high-dimensional datasets. To sample, we pick a cluster center and then add some Gaussian noise. Histogram Method \u00b6 Gotchas \u00b6 Search Sorted \u00b6 Numpy PyTorch def searchsorted ( bin_locations , inputs , eps = 1e-6 ): bin_locations [ ... , - 1 ] += eps h_sorted = torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 return h_sorted This is an unofficial implementation. There is still some talks in the PyTorch community to implement this. See github issue here . For now, we just use the implementation found in various implementations .","title":"PDF Estimation"},{"location":"Notes/Unsorted/pdf_est/#pdf-estimation","text":"","title":"PDF Estimation"},{"location":"Notes/Unsorted/pdf_est/#main-idea","text":"Fig I : Input Distribution. P(x \\in [a,b]) = \\int_a^b p(x)dx P(x \\in [a,b]) = \\int_a^b p(x)dx","title":"Main Idea"},{"location":"Notes/Unsorted/pdf_est/#likelihood","text":"Given a dataset \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} , we can find the some parameters \\theta \\theta by solving this optimization function: the likelihood \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) or equivalently: \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] This is equivalent to minimizing the KL-Divergence between the empirical data distribution \\tilde{p}_\\text{data}(x) \\tilde{p}_\\text{data}(x) and the model p_\\theta p_\\theta . D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) where \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}]","title":"Likelihood"},{"location":"Notes/Unsorted/pdf_est/#stochastic-gradient-descent","text":"Maximum likelihood is an optimization problem so we can use stochastic gradient descent (SGD) to solve it. This algorithm minimizes the expectation for f f assuming it is a differentiable function of \\theta \\theta . \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] With maximum likelihood, the optimization problem becomes: \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] We typically use SGD because it works with large datasets and it allows us to use deep learning architectures and convenient packages.","title":"Stochastic Gradient Descent"},{"location":"Notes/Unsorted/pdf_est/#example","text":"","title":"Example"},{"location":"Notes/Unsorted/pdf_est/#mixture-of-gaussians","text":"p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) where we have parameters as k k means, variances and mixture weights, \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) However, this doesn't really work for high-dimensional datasets. To sample, we pick a cluster center and then add some Gaussian noise.","title":"Mixture of Gaussians"},{"location":"Notes/Unsorted/pdf_est/#histogram-method","text":"","title":"Histogram Method"},{"location":"Notes/Unsorted/pdf_est/#gotchas","text":"","title":"Gotchas"},{"location":"Notes/Unsorted/pdf_est/#search-sorted","text":"Numpy PyTorch def searchsorted ( bin_locations , inputs , eps = 1e-6 ): bin_locations [ ... , - 1 ] += eps h_sorted = torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 return h_sorted This is an unofficial implementation. There is still some talks in the PyTorch community to implement this. See github issue here . For now, we just use the implementation found in various implementations .","title":"Search Sorted"},{"location":"Notes/Unsorted/rbig/","text":"Rotation-Based Iterative Gaussianization (RBIG) \u00b6 Motivation Algorithm Marginal (Univariate) Gaussianization Marginal Uniformization Gaussianization of a Uniform Variable Linear Transformation Information Theory Measures Information Entropy Mutual Information KL-Divergence Motivation \u00b6 The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} Algorithm \u00b6 Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) Marginal (Univariate) Gaussianization \u00b6 This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components Marginal Uniformization \u00b6 We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy Gaussianization of a Uniform Variable \u00b6 Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U Linear Transformation \u00b6 This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space. Information Theory Measures \u00b6 Caption : Information Theory measures in a nutshell. Information \u00b6 Entropy \u00b6 Mutual Information \u00b6 Caption : Schematic for finding the Mutual Information using using RBIG. \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} KL-Divergence \u00b6 Caption : Schematic for finding the KL-Divergence using using RBIG. Let \\mathcal{G}_\\theta (\\mathbf{X}) \\mathcal{G}_\\theta (\\mathbf{X}) be the Gaussianization of the variable \\mathbf{X} \\mathbf{X} which is parameterized by \\theta \\theta . \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned}","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"Notes/Unsorted/rbig/#rotation-based-iterative-gaussianization-rbig","text":"Motivation Algorithm Marginal (Univariate) Gaussianization Marginal Uniformization Gaussianization of a Uniform Variable Linear Transformation Information Theory Measures Information Entropy Mutual Information KL-Divergence","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"Notes/Unsorted/rbig/#motivation","text":"The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned}","title":"Motivation"},{"location":"Notes/Unsorted/rbig/#algorithm","text":"Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right)","title":"Algorithm"},{"location":"Notes/Unsorted/rbig/#marginal-univariate-gaussianization","text":"This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components","title":"Marginal (Univariate) Gaussianization"},{"location":"Notes/Unsorted/rbig/#marginal-uniformization","text":"We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy","title":"Marginal Uniformization"},{"location":"Notes/Unsorted/rbig/#gaussianization-of-a-uniform-variable","text":"Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U","title":"Gaussianization of a Uniform Variable"},{"location":"Notes/Unsorted/rbig/#linear-transformation","text":"This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Linear Transformation"},{"location":"Notes/Unsorted/rbig/#information-theory-measures","text":"Caption : Information Theory measures in a nutshell.","title":"Information Theory Measures"},{"location":"Notes/Unsorted/rbig/#information","text":"","title":"Information"},{"location":"Notes/Unsorted/rbig/#entropy","text":"","title":"Entropy"},{"location":"Notes/Unsorted/rbig/#mutual-information","text":"Caption : Schematic for finding the Mutual Information using using RBIG. \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned}","title":"Mutual Information"},{"location":"Notes/Unsorted/rbig/#kl-divergence","text":"Caption : Schematic for finding the KL-Divergence using using RBIG. Let \\mathcal{G}_\\theta (\\mathbf{X}) \\mathcal{G}_\\theta (\\mathbf{X}) be the Gaussianization of the variable \\mathbf{X} \\mathbf{X} which is parameterized by \\theta \\theta . \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned}","title":"KL-Divergence"},{"location":"Notes/Unsorted/refactor/","text":"Refactoring RBIG (RBIG 1.1) \u00b6 Components \u00b6 Flow \u00b6 Forward Transformation Transformation Log Determinant Jacobian Inverse Transformation Transformation Log Determinant Jacobian Normalizing Flow \u00b6 This is a sequence of Normalizing Flows. Forward Transformation (all layers) Backwards Transformation (all layers) Output: Transformation Log Determinant Jacobian Normalizing Flow Model \u00b6 This is a Normalizing flow with a prior distribution Init: Prior, NF Model Forward: Forward, LogDet, Prior Backward: Transform, LogDet Sample: Transform Ideal Case \u00b6 Define the Prior Distribution d_dimensions = 1 # initialize prior distribution prior = MultivariateNormal ( mean = torch . zeros ( d_dimensions ), cov = torch . eye ( d_dimensions ) ) Define the Model n_layers = 2 # make flow blocks flows = [ flow ( dim = d_dimensions ) for _ in range ( n_layers )] # create model given flow blocks and prior model = NormalizingFlowModel ( prior , flows ) Define Optimization scheme opt = optim . Adam ( model . parameters (), lr = 0.005 ) Optimize Model for i in range ( n_epochs ): # initialize optimizer opt . zero_grad () # get forward transformation z = model . transform ( x ) # get prior probability prior_logprob = model . prior ( x ) # get log determinant jacobian prob log_det = model . logabsdet ( x ) # calculate loss loss = - torch . mean ( prior_logprob + log_det ) # backpropagate loss . backward () # optimize forward opt . step ()","title":"Refactoring RBIG (RBIG 1.1)"},{"location":"Notes/Unsorted/refactor/#refactoring-rbig-rbig-11","text":"","title":"Refactoring RBIG (RBIG 1.1)"},{"location":"Notes/Unsorted/refactor/#components","text":"","title":"Components"},{"location":"Notes/Unsorted/refactor/#flow","text":"Forward Transformation Transformation Log Determinant Jacobian Inverse Transformation Transformation Log Determinant Jacobian","title":"Flow"},{"location":"Notes/Unsorted/refactor/#normalizing-flow","text":"This is a sequence of Normalizing Flows. Forward Transformation (all layers) Backwards Transformation (all layers) Output: Transformation Log Determinant Jacobian","title":"Normalizing Flow"},{"location":"Notes/Unsorted/refactor/#normalizing-flow-model","text":"This is a Normalizing flow with a prior distribution Init: Prior, NF Model Forward: Forward, LogDet, Prior Backward: Transform, LogDet Sample: Transform","title":"Normalizing Flow Model"},{"location":"Notes/Unsorted/refactor/#ideal-case","text":"Define the Prior Distribution d_dimensions = 1 # initialize prior distribution prior = MultivariateNormal ( mean = torch . zeros ( d_dimensions ), cov = torch . eye ( d_dimensions ) ) Define the Model n_layers = 2 # make flow blocks flows = [ flow ( dim = d_dimensions ) for _ in range ( n_layers )] # create model given flow blocks and prior model = NormalizingFlowModel ( prior , flows ) Define Optimization scheme opt = optim . Adam ( model . parameters (), lr = 0.005 ) Optimize Model for i in range ( n_epochs ): # initialize optimizer opt . zero_grad () # get forward transformation z = model . transform ( x ) # get prior probability prior_logprob = model . prior ( x ) # get log determinant jacobian prob log_det = model . logabsdet ( x ) # calculate loss loss = - torch . mean ( prior_logprob + log_det ) # backpropagate loss . backward () # optimize forward opt . step ()","title":"Ideal Case"},{"location":"Notes/Unsorted/rotation/","text":"Rotation \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Colab Notebook: Notebook Main Idea \u00b6 Rotation Matrix \u00b6 Foward Transformation \u00b6 Reverse Transformation \u00b6 Jacobian \u00b6 The deteriminant of an orthogonal matrix is 1. Proof : There are a series of transformations that can be used to prove this: \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} Therefore, we can conclude that the \\det(\\mathbf{R})=1 \\det(\\mathbf{R})=1 . Log Jacobian \u00b6 As shown above, the log determinant jacobian of an orthogonal matrix is 1. So taking the log of this is simply zero. \\log(\\det(\\mathbf{R})) = \\log(1) = 0 \\log(\\det(\\mathbf{R})) = \\log(1) = 0 Decompositions \u00b6 QR Decomposition \u00b6 A=QR A=QR where * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * Q \\in \\mathbb{R}^{N \\times N} Q \\in \\mathbb{R}^{N \\times N} is orthogonal * R \\in \\mathbb{R}^{N \\times M} R \\in \\mathbb{R}^{N \\times M} is upper triangular Singular Value Decomposition \u00b6 Finds the singular values of the matrix. A=U\\Sigma V^\\top A=U\\Sigma V^\\top where: * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * U \\in \\mathbb{R}^{N \\times K} U \\in \\mathbb{R}^{N \\times K} is unitary * \\Sigma \\in \\mathbb{R}^{K \\times K} \\Sigma \\in \\mathbb{R}^{K \\times K} are the singular values * V^\\top \\in \\mathbb{R}^{K \\times M} V^\\top \\in \\mathbb{R}^{K \\times M} is unitary Eigendecomposition \u00b6 Finds the singular values of a symmetric matrix A_S=Q\\Lambda Q^\\top A_S=Q\\Lambda Q^\\top where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary Polar Decomposition \u00b6 A_S=QS A_S=QS where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary Initializing \u00b6 We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self Transformation \u00b6 We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W Inverse Transformation \u00b6 We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W ) Jacobian \u00b6 Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]]) Log Likelihood (?) \u00b6","title":"Rotation"},{"location":"Notes/Unsorted/rotation/#rotation","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Colab Notebook: Notebook","title":"Rotation"},{"location":"Notes/Unsorted/rotation/#main-idea","text":"","title":"Main Idea"},{"location":"Notes/Unsorted/rotation/#rotation-matrix","text":"","title":"Rotation Matrix"},{"location":"Notes/Unsorted/rotation/#foward-transformation","text":"","title":"Foward Transformation"},{"location":"Notes/Unsorted/rotation/#reverse-transformation","text":"","title":"Reverse Transformation"},{"location":"Notes/Unsorted/rotation/#jacobian","text":"The deteriminant of an orthogonal matrix is 1. Proof : There are a series of transformations that can be used to prove this: \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} Therefore, we can conclude that the \\det(\\mathbf{R})=1 \\det(\\mathbf{R})=1 .","title":"Jacobian"},{"location":"Notes/Unsorted/rotation/#log-jacobian","text":"As shown above, the log determinant jacobian of an orthogonal matrix is 1. So taking the log of this is simply zero. \\log(\\det(\\mathbf{R})) = \\log(1) = 0 \\log(\\det(\\mathbf{R})) = \\log(1) = 0","title":"Log Jacobian"},{"location":"Notes/Unsorted/rotation/#decompositions","text":"","title":"Decompositions"},{"location":"Notes/Unsorted/rotation/#qr-decomposition","text":"A=QR A=QR where * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * Q \\in \\mathbb{R}^{N \\times N} Q \\in \\mathbb{R}^{N \\times N} is orthogonal * R \\in \\mathbb{R}^{N \\times M} R \\in \\mathbb{R}^{N \\times M} is upper triangular","title":"QR Decomposition"},{"location":"Notes/Unsorted/rotation/#singular-value-decomposition","text":"Finds the singular values of the matrix. A=U\\Sigma V^\\top A=U\\Sigma V^\\top where: * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * U \\in \\mathbb{R}^{N \\times K} U \\in \\mathbb{R}^{N \\times K} is unitary * \\Sigma \\in \\mathbb{R}^{K \\times K} \\Sigma \\in \\mathbb{R}^{K \\times K} are the singular values * V^\\top \\in \\mathbb{R}^{K \\times M} V^\\top \\in \\mathbb{R}^{K \\times M} is unitary","title":"Singular Value Decomposition"},{"location":"Notes/Unsorted/rotation/#eigendecomposition","text":"Finds the singular values of a symmetric matrix A_S=Q\\Lambda Q^\\top A_S=Q\\Lambda Q^\\top where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary","title":"Eigendecomposition"},{"location":"Notes/Unsorted/rotation/#polar-decomposition","text":"A_S=QS A_S=QS where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary","title":"Polar Decomposition"},{"location":"Notes/Unsorted/rotation/#initializing","text":"We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self","title":"Initializing"},{"location":"Notes/Unsorted/rotation/#transformation","text":"We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W","title":"Transformation"},{"location":"Notes/Unsorted/rotation/#inverse-transformation","text":"We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W )","title":"Inverse Transformation"},{"location":"Notes/Unsorted/rotation/#jacobian_1","text":"Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]])","title":"Jacobian"},{"location":"Notes/Unsorted/rotation/#log-likelihood","text":"","title":"Log Likelihood (?)"},{"location":"Notes/Unsorted/uniform/","text":"Uniform Distribution \u00b6 Entropy H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right] H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right]","title":"Uniform Distribution"},{"location":"Notes/Unsorted/uniform/#uniform-distribution","text":"Entropy H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right] H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right]","title":"Uniform Distribution"},{"location":"demos/demo_innf/","text":"Demo: Gaussianization \u00b6 Data \u00b6 RBIG Model \u00b6 Initialize Model \u00b6 # rbig parameters n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # initialize RBIG Class rbig_clf = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) Fit Model to Data \u00b6 # run RBIG model rbig_clf . fit ( X ); Visualization \u00b6 1. Marginal Gaussianization \u00b6 # rotation matrix V (N x F) V = rbig_clf . rotation_matrix [ 0 ] # perform rotation data_marg_gauss = X @ V 2. Rotation \u00b6","title":"Demo: Gaussianization"},{"location":"demos/demo_innf/#demo-gaussianization","text":"","title":"Demo: Gaussianization"},{"location":"demos/demo_innf/#data","text":"","title":"Data"},{"location":"demos/demo_innf/#rbig-model","text":"","title":"RBIG Model"},{"location":"demos/demo_innf/#initialize-model","text":"# rbig parameters n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # initialize RBIG Class rbig_clf = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base )","title":"Initialize Model"},{"location":"demos/demo_innf/#fit-model-to-data","text":"# run RBIG model rbig_clf . fit ( X );","title":"Fit Model to Data"},{"location":"demos/demo_innf/#visualization","text":"","title":"Visualization"},{"location":"demos/demo_innf/#1-marginal-gaussianization","text":"# rotation matrix V (N x F) V = rbig_clf . rotation_matrix [ 0 ] # perform rotation data_marg_gauss = X @ V","title":"1. Marginal Gaussianization"},{"location":"demos/demo_innf/#2-rotation","text":"","title":"2. Rotation"},{"location":"notebooks/information_theory/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Information Theory Measures w/ RBIG \u00b6 import sys # MacOS sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/py_rbig/src' ) # ERC server sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) import numpy as np import warnings from time import time from rbig.rbig import RBIGKLD , RBIG , RBIGMI , entropy_marginal from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % load_ext autoreload % autoreload 2 Total Correlation \u00b6 #Parameters n_samples = 10000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) Sample Data \u00b6 # Generate random normal data data_original = rng . randn ( n_samples , d_dimensions ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) data = data_original @ A # covariance matrix C = A . T @ A vv = np . diag ( C ) Calculate Total Correlation \u00b6 tc_original = np . log ( np . sqrt ( vv )) . sum () - 0.5 * np . log ( np . linalg . det ( C )) print ( f \"TC: { tc_original : .4f } \" ) TC: 9.9326 RBIG - TC \u00b6 %% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = 10 pdf_resolution = None tolerance = None # Initialize RBIG class tc_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance , pdf_extension = pdf_extension , pdf_resolution = pdf_resolution ) # fit model to the data tc_rbig_model . fit ( data ); CPU times: user 1min 19s, sys: 64.4 ms, total: 1min 19s Wall time: 3.01 s tc_rbig = tc_rbig_model . mutual_information * np . log ( 2 ) print ( f \"TC (RBIG): { tc_rbig : .4f } \" ) print ( f \"TC: { tc_original : .4f } \" ) TC (RBIG): 9.9398 TC: 9.9326 Entropy \u00b6 Sample Data \u00b6 #Parameters n_samples = 5000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) # Generate random normal data data_original = rng . randn ( n_samples , d_dimensions ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) data = data_original @ A Calculate Entropy \u00b6 Hx = entropy_marginal ( data ) H_original = Hx . sum () + np . log2 ( np . abs ( np . linalg . det ( A ))) H_original *= np . log ( 2 ) print ( f \"H: { H_original : .4f } \" ) H: 16.4355 Entropy RBIG \u00b6 %% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = None pdf_resolution = None tolerance = None # Initialize RBIG class ent_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data ent_rbig_model . fit ( data ); CPU times: user 53.1 s, sys: 9.81 ms, total: 53.1 s Wall time: 1.9 s H_rbig = ent_rbig_model . entropy ( correction = True ) * np . log ( 2 ) print ( f \"Entropy (RBIG): { H_rbig : .4f } \" ) print ( f \"Entropy: { H_original : .4f } \" ) Entropy (RBIG): 10.6551 Entropy: 16.4355 Mutual Information \u00b6 Sample Data \u00b6 #Parameters n_samples = 10000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) # Generate random Data A = rng . rand ( 2 * d_dimensions , 2 * d_dimensions ) # Covariance Matrix C = A @ A . T mu = np . zeros (( 2 * d_dimensions )) dat_all = rng . multivariate_normal ( mu , C , n_samples ) CX = C [: d_dimensions , : d_dimensions ] CY = C [ d_dimensions :, d_dimensions :] X = dat_all [:, : d_dimensions ] Y = dat_all [:, d_dimensions :] Calculate Mutual Information \u00b6 H_X = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( CX ))) H_Y = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( CY ))) H = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( C ))) mi_original = H_X + H_Y - H mi_original *= np . log ( 2 ) print ( f \"MI: { mi_original : .4f } \" ) MI: 8.0713 RBIG - Mutual Information \u00b6 %% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 tolerance = None # Initialize RBIG class rbig_model = RBIGMI ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data rbig_model . fit ( X , Y ); CPU times: user 5min 37s, sys: 103 ms, total: 5min 38s Wall time: 12.1 s H_rbig = rbig_model . mutual_information () * np . log ( 2 ) print ( f \"MI (RBIG): { H_rbig : .4f } \" ) print ( f \"MI: { mi_original : .4f } \" ) MI (RBIG): 9.0746 MI: 8.0713 Kullback-Leibler Divergence (KLD) \u00b6 Sample Data \u00b6 #Parameters n_samples = 10000 d_dimensions = 10 mu = 0.4 # how different the distributions are seed = 123 rng = check_random_state ( seed ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) # covariance matrix cov = A @ A . T # Normalize cov mat cov = A / A . max () # create covariance matrices for x and y cov_x = np . eye ( d_dimensions ) cov_y = cov_x . copy () mu_x = np . zeros ( d_dimensions ) + mu mu_y = np . zeros ( d_dimensions ) # generate multivariate gaussian data X = rng . multivariate_normal ( mu_x , cov_x , n_samples ) Y = rng . multivariate_normal ( mu_y , cov_y , n_samples ) Calculate KLD \u00b6 kld_original = 0.5 * (( mu_y - mu_x ) @ np . linalg . inv ( cov_y ) @ ( mu_y - mu_x ) . T + np . trace ( np . linalg . inv ( cov_y ) @ cov_x ) - np . log ( np . linalg . det ( cov_x ) / np . linalg . det ( cov_y )) - d_dimensions ) print ( f 'KLD: { kld_original : .4f } ' ) KLD: 0.8000 RBIG - KLD \u00b6 X . min (), X . max () (-4.006934109277744, 4.585027222023813) Y . min (), Y . max () (-4.607129910785054, 4.299322691460413) %% time n_layers = 100000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 tolerance = None pdf_extension = 10 pdf_resolution = None verbose = 0 # Initialize RBIG class kld_rbig_model = RBIGKLD ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance , pdf_resolution = pdf_resolution , pdf_extension = pdf_extension , verbose = verbose ) # fit model to the data kld_rbig_model . fit ( X , Y ); CPU times: user 5min 46s, sys: 10.9 ms, total: 5min 46s Wall time: 12.4 s # Save KLD value to data structure kld_rbig = kld_rbig_model . kld * np . log ( 2 ) print ( f 'KLD (RBIG): { kld_rbig : .4f } ' ) print ( f 'KLD: { kld_original : .4f } ' ) KLD (RBIG): 0.8349 KLD: 0.8000","title":"Information Theory"},{"location":"notebooks/information_theory/#information-theory-measures-w-rbig","text":"import sys # MacOS sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/py_rbig/src' ) # ERC server sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) import numpy as np import warnings from time import time from rbig.rbig import RBIGKLD , RBIG , RBIGMI , entropy_marginal from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % load_ext autoreload % autoreload 2","title":"Information Theory Measures w/ RBIG"},{"location":"notebooks/information_theory/#total-correlation","text":"#Parameters n_samples = 10000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed )","title":"Total Correlation"},{"location":"notebooks/information_theory/#sample-data","text":"# Generate random normal data data_original = rng . randn ( n_samples , d_dimensions ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) data = data_original @ A # covariance matrix C = A . T @ A vv = np . diag ( C )","title":"Sample Data"},{"location":"notebooks/information_theory/#calculate-total-correlation","text":"tc_original = np . log ( np . sqrt ( vv )) . sum () - 0.5 * np . log ( np . linalg . det ( C )) print ( f \"TC: { tc_original : .4f } \" ) TC: 9.9326","title":"Calculate Total Correlation"},{"location":"notebooks/information_theory/#rbig-tc","text":"%% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = 10 pdf_resolution = None tolerance = None # Initialize RBIG class tc_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance , pdf_extension = pdf_extension , pdf_resolution = pdf_resolution ) # fit model to the data tc_rbig_model . fit ( data ); CPU times: user 1min 19s, sys: 64.4 ms, total: 1min 19s Wall time: 3.01 s tc_rbig = tc_rbig_model . mutual_information * np . log ( 2 ) print ( f \"TC (RBIG): { tc_rbig : .4f } \" ) print ( f \"TC: { tc_original : .4f } \" ) TC (RBIG): 9.9398 TC: 9.9326","title":"RBIG - TC"},{"location":"notebooks/information_theory/#entropy","text":"","title":"Entropy"},{"location":"notebooks/information_theory/#sample-data_1","text":"#Parameters n_samples = 5000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) # Generate random normal data data_original = rng . randn ( n_samples , d_dimensions ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) data = data_original @ A","title":"Sample Data"},{"location":"notebooks/information_theory/#calculate-entropy","text":"Hx = entropy_marginal ( data ) H_original = Hx . sum () + np . log2 ( np . abs ( np . linalg . det ( A ))) H_original *= np . log ( 2 ) print ( f \"H: { H_original : .4f } \" ) H: 16.4355","title":"Calculate Entropy"},{"location":"notebooks/information_theory/#entropy-rbig","text":"%% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = None pdf_resolution = None tolerance = None # Initialize RBIG class ent_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data ent_rbig_model . fit ( data ); CPU times: user 53.1 s, sys: 9.81 ms, total: 53.1 s Wall time: 1.9 s H_rbig = ent_rbig_model . entropy ( correction = True ) * np . log ( 2 ) print ( f \"Entropy (RBIG): { H_rbig : .4f } \" ) print ( f \"Entropy: { H_original : .4f } \" ) Entropy (RBIG): 10.6551 Entropy: 16.4355","title":"Entropy RBIG"},{"location":"notebooks/information_theory/#mutual-information","text":"","title":"Mutual Information"},{"location":"notebooks/information_theory/#sample-data_2","text":"#Parameters n_samples = 10000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) # Generate random Data A = rng . rand ( 2 * d_dimensions , 2 * d_dimensions ) # Covariance Matrix C = A @ A . T mu = np . zeros (( 2 * d_dimensions )) dat_all = rng . multivariate_normal ( mu , C , n_samples ) CX = C [: d_dimensions , : d_dimensions ] CY = C [ d_dimensions :, d_dimensions :] X = dat_all [:, : d_dimensions ] Y = dat_all [:, d_dimensions :]","title":"Sample Data"},{"location":"notebooks/information_theory/#calculate-mutual-information","text":"H_X = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( CX ))) H_Y = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( CY ))) H = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( C ))) mi_original = H_X + H_Y - H mi_original *= np . log ( 2 ) print ( f \"MI: { mi_original : .4f } \" ) MI: 8.0713","title":"Calculate Mutual Information"},{"location":"notebooks/information_theory/#rbig-mutual-information","text":"%% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 tolerance = None # Initialize RBIG class rbig_model = RBIGMI ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data rbig_model . fit ( X , Y ); CPU times: user 5min 37s, sys: 103 ms, total: 5min 38s Wall time: 12.1 s H_rbig = rbig_model . mutual_information () * np . log ( 2 ) print ( f \"MI (RBIG): { H_rbig : .4f } \" ) print ( f \"MI: { mi_original : .4f } \" ) MI (RBIG): 9.0746 MI: 8.0713","title":"RBIG - Mutual Information"},{"location":"notebooks/information_theory/#kullback-leibler-divergence-kld","text":"","title":"Kullback-Leibler Divergence (KLD)"},{"location":"notebooks/information_theory/#sample-data_3","text":"#Parameters n_samples = 10000 d_dimensions = 10 mu = 0.4 # how different the distributions are seed = 123 rng = check_random_state ( seed ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) # covariance matrix cov = A @ A . T # Normalize cov mat cov = A / A . max () # create covariance matrices for x and y cov_x = np . eye ( d_dimensions ) cov_y = cov_x . copy () mu_x = np . zeros ( d_dimensions ) + mu mu_y = np . zeros ( d_dimensions ) # generate multivariate gaussian data X = rng . multivariate_normal ( mu_x , cov_x , n_samples ) Y = rng . multivariate_normal ( mu_y , cov_y , n_samples )","title":"Sample Data"},{"location":"notebooks/information_theory/#calculate-kld","text":"kld_original = 0.5 * (( mu_y - mu_x ) @ np . linalg . inv ( cov_y ) @ ( mu_y - mu_x ) . T + np . trace ( np . linalg . inv ( cov_y ) @ cov_x ) - np . log ( np . linalg . det ( cov_x ) / np . linalg . det ( cov_y )) - d_dimensions ) print ( f 'KLD: { kld_original : .4f } ' ) KLD: 0.8000","title":"Calculate KLD"},{"location":"notebooks/information_theory/#rbig-kld","text":"X . min (), X . max () (-4.006934109277744, 4.585027222023813) Y . min (), Y . max () (-4.607129910785054, 4.299322691460413) %% time n_layers = 100000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 tolerance = None pdf_extension = 10 pdf_resolution = None verbose = 0 # Initialize RBIG class kld_rbig_model = RBIGKLD ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance , pdf_resolution = pdf_resolution , pdf_extension = pdf_extension , verbose = verbose ) # fit model to the data kld_rbig_model . fit ( X , Y ); CPU times: user 5min 46s, sys: 10.9 ms, total: 5min 46s Wall time: 12.4 s # Save KLD value to data structure kld_rbig = kld_rbig_model . kld * np . log ( 2 ) print ( f 'KLD (RBIG): { kld_rbig : .4f } ' ) print ( f 'KLD: { kld_original : .4f } ' ) KLD (RBIG): 0.8349 KLD: 0.8000","title":"RBIG - KLD"},{"location":"notebooks/innf_demo/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import sys sys . path . append ( '/home/emmanuel/code/rbig' ) sys . path . append ( '/home/emmanuel/code/destructive-deep-learning' ) figsave_path = '/home/emmanuel/projects/2019_rbig_info/reports/figures/invertible_flows/' import numpy as np import seaborn as sns from rbig.rbig import RBIGKLD , RBIG , RBIGMI , entropy_marginal from ddl.datasets import make_toy_data import matplotlib.pyplot as plt from scipy import stats plt . style . use ( 'seaborn' ) sns . set_style ({ 'axes.axisbelow' : False , 'xtick.bottom' : False , 'axes.spines.left' : False , 'axes.spines.bottom' : False , }) % matplotlib inline % load_ext autoreload % autoreload 2 Data \u00b6 seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig_sin_wave' n_samples = 10000 dat = make_toy_data ( dataset , n_samples , seed ) X , y = dat . X , dat . y fig , ax = plt . subplots ( figsize = ( 5 , 5 )) ax . scatter ( X [:, 0 ], X [:, 1 ], s = 1 , c = 'red' ) plt . tick_params ( axis = 'both' , # changes apply to the x-axis which = 'both' , # both major and minor ticks are affected bottom = False , # ticks along the bottom edge are off left = False , top = False , # ticks along the top edge are off labelbottom = False , labelleft = False ) # labels along the bottom edge are off plt . tight_layout () xlims , ylims = plt . xlim (), plt . ylim () plt . show () # fig.savefig(f\"{figsave_path}/original.png\") # print(ax.xlim) # sns.set_style(\"dark\") # sns.despine() fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = X [:, 0 ], y = X [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) # # sns.despine() # sns.set_style(\"ticks\", # { # 'bottom': False, # 'axis': 'both', # 'which': 'both', # 'labelbottom': False, # 'labelleft': False, # 'left': False, # 'top': False, # 'xticks': [] # }) # plt.axis('off') # plt.show() plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_0_data.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> RBIG Algorithm - 1 Layer (for Demonstration) \u00b6 n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); Step I - Marginal Gaussianization \u00b6 fig = plt . figure ( figsize = ( 5 , 5 )) mg_data = rbig_model . gauss_data @ rbig_model . rotation_matrix [ 0 ] . T g = sns . jointplot ( x = mg_data [:, 0 ], y = mg_data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_1_mg.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> Step II - Rotation \u00b6 fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = rbig_model . gauss_data [:, 0 ], y = rbig_model . gauss_data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_2_rotation.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> RBIG Algorithms - 2,3,4,5 Layers \u00b6 n_layers = [ 1 , 2 , 3 , 4 , 5 , 6 ] rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' for ilayer in n_layers : # Initialize RBIG class rbig_model = RBIG ( n_layers = ilayer , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); # transform data data_trans = rbig_model . transform ( X ) # Plot Layer plot_gauss_layer ( data_trans , ilayer ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> def plot_gauss_layer ( data , layer ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_l { layer } _gaussian.png\" , transparent = True ) Full RBIG Algorithm \u00b6 n_layers = 1000 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); # transform data data_trans = rbig_model . transform ( X ) fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data_trans [:, 0 ], y = data_trans [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_l_gaussian.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> rbig_model . n_layers 64","title":"RBIG Walk-Through"},{"location":"notebooks/innf_demo/#data","text":"seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig_sin_wave' n_samples = 10000 dat = make_toy_data ( dataset , n_samples , seed ) X , y = dat . X , dat . y fig , ax = plt . subplots ( figsize = ( 5 , 5 )) ax . scatter ( X [:, 0 ], X [:, 1 ], s = 1 , c = 'red' ) plt . tick_params ( axis = 'both' , # changes apply to the x-axis which = 'both' , # both major and minor ticks are affected bottom = False , # ticks along the bottom edge are off left = False , top = False , # ticks along the top edge are off labelbottom = False , labelleft = False ) # labels along the bottom edge are off plt . tight_layout () xlims , ylims = plt . xlim (), plt . ylim () plt . show () # fig.savefig(f\"{figsave_path}/original.png\") # print(ax.xlim) # sns.set_style(\"dark\") # sns.despine() fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = X [:, 0 ], y = X [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) # # sns.despine() # sns.set_style(\"ticks\", # { # 'bottom': False, # 'axis': 'both', # 'which': 'both', # 'labelbottom': False, # 'labelleft': False, # 'left': False, # 'top': False, # 'xticks': [] # }) # plt.axis('off') # plt.show() plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_0_data.png\" , transparent = True ) <Figure size 360x360 with 0 Axes>","title":"Data"},{"location":"notebooks/innf_demo/#rbig-algorithm-1-layer-for-demonstration","text":"n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X );","title":"RBIG Algorithm - 1 Layer (for Demonstration)"},{"location":"notebooks/innf_demo/#step-i-marginal-gaussianization","text":"fig = plt . figure ( figsize = ( 5 , 5 )) mg_data = rbig_model . gauss_data @ rbig_model . rotation_matrix [ 0 ] . T g = sns . jointplot ( x = mg_data [:, 0 ], y = mg_data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_1_mg.png\" , transparent = True ) <Figure size 360x360 with 0 Axes>","title":"Step I - Marginal Gaussianization"},{"location":"notebooks/innf_demo/#step-ii-rotation","text":"fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = rbig_model . gauss_data [:, 0 ], y = rbig_model . gauss_data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_2_rotation.png\" , transparent = True ) <Figure size 360x360 with 0 Axes>","title":"Step II - Rotation"},{"location":"notebooks/innf_demo/#rbig-algorithms-2345-layers","text":"n_layers = [ 1 , 2 , 3 , 4 , 5 , 6 ] rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' for ilayer in n_layers : # Initialize RBIG class rbig_model = RBIG ( n_layers = ilayer , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); # transform data data_trans = rbig_model . transform ( X ) # Plot Layer plot_gauss_layer ( data_trans , ilayer ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> def plot_gauss_layer ( data , layer ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_l { layer } _gaussian.png\" , transparent = True )","title":"RBIG Algorithms - 2,3,4,5 Layers"},{"location":"notebooks/innf_demo/#full-rbig-algorithm","text":"n_layers = 1000 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); # transform data data_trans = rbig_model . transform ( X ) fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data_trans [:, 0 ], y = data_trans [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_l_gaussian.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> rbig_model . n_layers 64","title":"Full RBIG Algorithm"},{"location":"notebooks/rbig_demo/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); RBIG Demo \u00b6 import sys sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/rbig/' ) sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) import numpy as np import warnings from time import time from rbig.rbig import RBIG , entropy # from rbig.model import RBIG from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Toy Data \u00b6 seed = 123 rng = np . random . RandomState ( seed = seed ) num_samples = 10_000 x = np . abs ( 2 * rng . randn ( 1 , num_samples )) y = np . sin ( x ) + 0.25 * rng . randn ( 1 , num_samples ) data = np . vstack (( x , y )) . T fig , ax = plt . subplots () ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Original Data' ) plt . show () RBIG Fitting \u00b6 %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 10 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 932 ms, sys: 43.2 ms, total: 975 ms Wall time: 335 ms Transform Data into Gaussian \u00b6 print ( data_trans . shape ) fig , ax = plt . subplots () ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Data after RBIG Transformation' ) plt . show () (10000, 2) Invertible \u00b6 %% time # transform data data_approx = rbig_model . inverse_transform ( data_trans ) # check approximation np . testing . assert_array_almost_equal ( data , data_approx ) CPU times: user 1.86 ms, sys: 9 \u00b5s, total: 1.87 ms Wall time: 1.62 ms Check Residuals \u00b6 data_approx = rbig_model . inverse_transform ( data_trans ) residual = np . abs ( data - data_approx ) . sum () . sum () print ( f 'Residual from Original and Transformed: { residual : .2e } ' ) Residual from Original and Transformed: 0.00e+00 fig , ax = plt . subplots () ax . scatter ( data_approx [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Inverse Transformation' ) plt . show () Information Reduction \u00b6 residual_info = rbig_model . residual_info fig , ax = plt . subplots () ax . plot ( np . cumsum ( rbig_model . residual_info )) ax . set_title ( 'Information Reduction' ) plt . show () Generated Synthetic Data \u00b6 data_synthetic = rng . randn ( data . shape [ 0 ], data . shape [ 1 ]) fig , ax = plt . subplots () ax . scatter ( data_synthetic [:, 0 ], data_synthetic [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Synthetically generated factorial gaussian data' ) plt . show () Synthesize New Data from RBIG Model \u00b6 data_original_synthetic = rbig_model . inverse_transform ( data_synthetic ) fig , ax = plt . subplots () ax . scatter ( data_original_synthetic [:, 0 ], data_original_synthetic [:, 1 ], s = 1 ) # ax.scatter(data[:, 0], data[:, 1], s=1) ax . set_ylim ([ - 1.5 , 2.0 ]) ax . set_xlim ([ 0.0 , 9.0 ]) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Synthetically generated data from the input distribution' ) plt . show () Jacobian \u00b6 %% time jacobian = rbig_model . jacobian ( data , return_X_transform = False ) print ( jacobian . shape ) print ( f \"Jacobian - min: { jacobian . min () : .3e } , max: { jacobian . max () : .3e } \" ) (10000, 2, 2) Jacobian - min: 0.000e+00, max: 1.000e+00 CPU times: user 922 \u00b5s, sys: 1.02 ms, total: 1.94 ms Wall time: 1.58 ms Estimating Probabilities with RBIG \u00b6 %% time prob_input , prob_gauss = rbig_model . predict_proba ( data , domain = 'both' , n_trials = 1 ) print ( f \"Prob Input Domain - min: { prob_input . min () : .3e } , max: { prob_input . max () : .3e } \" ) print ( f \"Prob Gauss Domain - min: { prob_gauss . min () : .3e } , max: { prob_gauss . max () : .3e } \" ) print ( f \"Det:: { rbig_model . det_jacobians : .3e } \" ) Prob Input Domain - min: 2.713e-16, max: 1.588e-01 Prob Gauss Domain - min: 2.713e-16, max: 1.588e-01 Det:: 1.000e+00 CPU times: user 6.96 ms, sys: 929 \u00b5s, total: 7.89 ms Wall time: 6.88 ms Original Data with Probabilities \u00b6 fig , ax = plt . subplots () ax . hist ( prob_input , 50 , facecolor = 'green' , alpha = 0.75 ) plt . show () fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = prob_input , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( 'Original Data w/ Probabilities' ) plt . show () Probability in Gaussian Domain \u00b6 # Plot the probability of the data in the Gaussian Domain fig , ax = plt . subplots () n , bins , patches = ax . hist ( prob_gauss , 50 , facecolor = 'green' , alpha = 0.75 ) ax . set_title ( 'Probability in Gaussian domain.' ) plt . show () # Plot the Probabilities of the data using colors fig , ax = plt . subplots () g = ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 , c = prob_gauss ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Data after RBIG transform w/ Probabilities' ) plt . colorbar ( g ) plt . show () Benchmarks \u00b6 data = np . random . randn ( 100_000 , 100 ) %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 10 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , pdf_resolution = 50 , ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 1min 59s, sys: 28.2 s, total: 2min 28s Wall time: 31.6 s rbig_model . n_layers 0 from rbig.model import RBIG as RBIG11 %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 60 verbose = 0 method = 'custom' # Initialize RBIG class rbig_model = RBIG11 ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , verbose = verbose , method = method , pdf_resolution = 50 , ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 8min 13s, sys: 1min 25s, total: 9min 38s Wall time: 1min 58s residual_info = rbig_model . residual_info plt . %% time data_inverted = rbig_model . inverse_transform ( data_trans ) CPU times: user 4min 10s, sys: 29.9 s, total: 4min 40s Wall time: 32.4 s %% time prob_input , prob_gauss = rbig_model . predict_proba ( data , domain = 'both' , n_trials = 1 ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <timed exec> in <module> ~/code/rbig/rbig/rbig.py in predict_proba (self, X, n_trials, chunksize, domain) 540 # data_aux[start_idx:end_idx, :], return_X_transform=True 541 # ) --> 542 jacobians , data_temp = self . jacobian ( data_aux , return_X_transform = True ) 543 # set all nans to zero 544 jacobians [ np . isnan ( jacobians ) ] = 0.0 ~/code/rbig/rbig/rbig.py in jacobian (self, X, return_X_transform) 471 for ilayer in range ( self . n_layers ) : 472 --> 473 XX = np.dot( 474 gaussian_pdf [ : , : , ilayer ] * XX , self . rotation_matrix [ ilayer ] 475 ) <__array_function__ internals> in dot (*args, **kwargs) KeyboardInterrupt : plt . plot ( np . cumsum ( rbig_model . residual_info )) [<matplotlib.lines.Line2D at 0x7f6119463d00>]","title":"RBIG Demo"},{"location":"notebooks/rbig_demo/#rbig-demo","text":"import sys sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/rbig/' ) sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) import numpy as np import warnings from time import time from rbig.rbig import RBIG , entropy # from rbig.model import RBIG from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"RBIG Demo"},{"location":"notebooks/rbig_demo/#toy-data","text":"seed = 123 rng = np . random . RandomState ( seed = seed ) num_samples = 10_000 x = np . abs ( 2 * rng . randn ( 1 , num_samples )) y = np . sin ( x ) + 0.25 * rng . randn ( 1 , num_samples ) data = np . vstack (( x , y )) . T fig , ax = plt . subplots () ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Original Data' ) plt . show ()","title":"Toy Data"},{"location":"notebooks/rbig_demo/#rbig-fitting","text":"%% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 10 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 932 ms, sys: 43.2 ms, total: 975 ms Wall time: 335 ms","title":"RBIG Fitting"},{"location":"notebooks/rbig_demo/#transform-data-into-gaussian","text":"print ( data_trans . shape ) fig , ax = plt . subplots () ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Data after RBIG Transformation' ) plt . show () (10000, 2)","title":"Transform Data into Gaussian"},{"location":"notebooks/rbig_demo/#invertible","text":"%% time # transform data data_approx = rbig_model . inverse_transform ( data_trans ) # check approximation np . testing . assert_array_almost_equal ( data , data_approx ) CPU times: user 1.86 ms, sys: 9 \u00b5s, total: 1.87 ms Wall time: 1.62 ms","title":"Invertible"},{"location":"notebooks/rbig_demo/#check-residuals","text":"data_approx = rbig_model . inverse_transform ( data_trans ) residual = np . abs ( data - data_approx ) . sum () . sum () print ( f 'Residual from Original and Transformed: { residual : .2e } ' ) Residual from Original and Transformed: 0.00e+00 fig , ax = plt . subplots () ax . scatter ( data_approx [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Inverse Transformation' ) plt . show ()","title":"Check Residuals"},{"location":"notebooks/rbig_demo/#information-reduction","text":"residual_info = rbig_model . residual_info fig , ax = plt . subplots () ax . plot ( np . cumsum ( rbig_model . residual_info )) ax . set_title ( 'Information Reduction' ) plt . show ()","title":"Information Reduction"},{"location":"notebooks/rbig_demo/#generated-synthetic-data","text":"data_synthetic = rng . randn ( data . shape [ 0 ], data . shape [ 1 ]) fig , ax = plt . subplots () ax . scatter ( data_synthetic [:, 0 ], data_synthetic [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Synthetically generated factorial gaussian data' ) plt . show ()","title":"Generated Synthetic Data"},{"location":"notebooks/rbig_demo/#synthesize-new-data-from-rbig-model","text":"data_original_synthetic = rbig_model . inverse_transform ( data_synthetic ) fig , ax = plt . subplots () ax . scatter ( data_original_synthetic [:, 0 ], data_original_synthetic [:, 1 ], s = 1 ) # ax.scatter(data[:, 0], data[:, 1], s=1) ax . set_ylim ([ - 1.5 , 2.0 ]) ax . set_xlim ([ 0.0 , 9.0 ]) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Synthetically generated data from the input distribution' ) plt . show ()","title":"Synthesize New Data from RBIG Model"},{"location":"notebooks/rbig_demo/#jacobian","text":"%% time jacobian = rbig_model . jacobian ( data , return_X_transform = False ) print ( jacobian . shape ) print ( f \"Jacobian - min: { jacobian . min () : .3e } , max: { jacobian . max () : .3e } \" ) (10000, 2, 2) Jacobian - min: 0.000e+00, max: 1.000e+00 CPU times: user 922 \u00b5s, sys: 1.02 ms, total: 1.94 ms Wall time: 1.58 ms","title":"Jacobian"},{"location":"notebooks/rbig_demo/#estimating-probabilities-with-rbig","text":"%% time prob_input , prob_gauss = rbig_model . predict_proba ( data , domain = 'both' , n_trials = 1 ) print ( f \"Prob Input Domain - min: { prob_input . min () : .3e } , max: { prob_input . max () : .3e } \" ) print ( f \"Prob Gauss Domain - min: { prob_gauss . min () : .3e } , max: { prob_gauss . max () : .3e } \" ) print ( f \"Det:: { rbig_model . det_jacobians : .3e } \" ) Prob Input Domain - min: 2.713e-16, max: 1.588e-01 Prob Gauss Domain - min: 2.713e-16, max: 1.588e-01 Det:: 1.000e+00 CPU times: user 6.96 ms, sys: 929 \u00b5s, total: 7.89 ms Wall time: 6.88 ms","title":"Estimating Probabilities with RBIG"},{"location":"notebooks/rbig_demo/#original-data-with-probabilities","text":"fig , ax = plt . subplots () ax . hist ( prob_input , 50 , facecolor = 'green' , alpha = 0.75 ) plt . show () fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = prob_input , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( 'Original Data w/ Probabilities' ) plt . show ()","title":"Original Data with Probabilities"},{"location":"notebooks/rbig_demo/#probability-in-gaussian-domain","text":"# Plot the probability of the data in the Gaussian Domain fig , ax = plt . subplots () n , bins , patches = ax . hist ( prob_gauss , 50 , facecolor = 'green' , alpha = 0.75 ) ax . set_title ( 'Probability in Gaussian domain.' ) plt . show () # Plot the Probabilities of the data using colors fig , ax = plt . subplots () g = ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 , c = prob_gauss ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Data after RBIG transform w/ Probabilities' ) plt . colorbar ( g ) plt . show ()","title":"Probability in Gaussian Domain"},{"location":"notebooks/rbig_demo/#benchmarks","text":"data = np . random . randn ( 100_000 , 100 ) %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 10 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , pdf_resolution = 50 , ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 1min 59s, sys: 28.2 s, total: 2min 28s Wall time: 31.6 s rbig_model . n_layers 0 from rbig.model import RBIG as RBIG11 %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 60 verbose = 0 method = 'custom' # Initialize RBIG class rbig_model = RBIG11 ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , verbose = verbose , method = method , pdf_resolution = 50 , ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 8min 13s, sys: 1min 25s, total: 9min 38s Wall time: 1min 58s residual_info = rbig_model . residual_info plt . %% time data_inverted = rbig_model . inverse_transform ( data_trans ) CPU times: user 4min 10s, sys: 29.9 s, total: 4min 40s Wall time: 32.4 s %% time prob_input , prob_gauss = rbig_model . predict_proba ( data , domain = 'both' , n_trials = 1 ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <timed exec> in <module> ~/code/rbig/rbig/rbig.py in predict_proba (self, X, n_trials, chunksize, domain) 540 # data_aux[start_idx:end_idx, :], return_X_transform=True 541 # ) --> 542 jacobians , data_temp = self . jacobian ( data_aux , return_X_transform = True ) 543 # set all nans to zero 544 jacobians [ np . isnan ( jacobians ) ] = 0.0 ~/code/rbig/rbig/rbig.py in jacobian (self, X, return_X_transform) 471 for ilayer in range ( self . n_layers ) : 472 --> 473 XX = np.dot( 474 gaussian_pdf [ : , : , ilayer ] * XX , self . rotation_matrix [ ilayer ] 475 ) <__array_function__ internals> in dot (*args, **kwargs) KeyboardInterrupt : plt . plot ( np . cumsum ( rbig_model . residual_info )) [<matplotlib.lines.Line2D at 0x7f6119463d00>]","title":"Benchmarks"},{"location":"notebooks/rbig_walkthrough/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); RBIG Walk-Through (Naive) \u00b6 This is a quick tutorial to show how the RBIG algorithm itself can be implemented very simply using standard scikit-learn tools. It consists of the following two steps 1) marginal Gaussianization and 2) rotation. import numpy as np import warnings from sklearn.preprocessing import QuantileTransformer from sklearn.decomposition import PCA from scipy.stats import rv_histogram , norm import pandas as pd import seaborn as sns import sys sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) from rbig import RBIG import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 # Helper Plot Function def plot_2d_joint ( data , savename = None ): fig = plt . figure ( figsize = ( 12 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'scatter' , color = 'blue' , alpha = 0.1 ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () if savename : g . savefig ( f \" { savename } /rbig_0_data.png\" , transparent = True ) plt . show () return None Data \u00b6 seed = 123 rng = np . random . RandomState ( seed = seed ) num_samples = 10000 x = np . abs ( 2 * rng . randn ( 1 , num_samples )) y = np . sin ( x ) + 0.25 * rng . randn ( 1 , num_samples ) data = np . vstack (( x , y )) . T d_dimensions = data . shape [ 1 ] plot_2d_joint ( data ) <Figure size 864x360 with 0 Axes> Step I - Marginal Gaussianization \u00b6 In this tutorial, for simplicity, I will use the quantile transformer found in the sklearn library. This transformer does an estimate of the CDF for each feature independently. Then the values are mapped to the Guassian distribution from the learned CDF function. n_quantiles = 1000 output_distribution = 'normal' random_state = 123 subsample = 2000 # Quantile Transformer mg_transformer = QuantileTransformer ( n_quantiles = n_quantiles , output_distribution = output_distribution , subsample = subsample ) data_mg = mg_transformer . fit_transform ( data ) plot_2d_joint ( data_mg ) <Figure size 864x360 with 0 Axes> Step II - Rotation (PCA) \u00b6 pca_model = PCA () data_rot = pca_model . fit_transform ( data_mg ) plot_2d_joint ( data_rot ) <Figure size 864x360 with 0 Axes>","title":"Rbig walkthrough"},{"location":"notebooks/rbig_walkthrough/#rbig-walk-through-naive","text":"This is a quick tutorial to show how the RBIG algorithm itself can be implemented very simply using standard scikit-learn tools. It consists of the following two steps 1) marginal Gaussianization and 2) rotation. import numpy as np import warnings from sklearn.preprocessing import QuantileTransformer from sklearn.decomposition import PCA from scipy.stats import rv_histogram , norm import pandas as pd import seaborn as sns import sys sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) from rbig import RBIG import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 # Helper Plot Function def plot_2d_joint ( data , savename = None ): fig = plt . figure ( figsize = ( 12 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'scatter' , color = 'blue' , alpha = 0.1 ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () if savename : g . savefig ( f \" { savename } /rbig_0_data.png\" , transparent = True ) plt . show () return None","title":"RBIG Walk-Through (Naive)"},{"location":"notebooks/rbig_walkthrough/#data","text":"seed = 123 rng = np . random . RandomState ( seed = seed ) num_samples = 10000 x = np . abs ( 2 * rng . randn ( 1 , num_samples )) y = np . sin ( x ) + 0.25 * rng . randn ( 1 , num_samples ) data = np . vstack (( x , y )) . T d_dimensions = data . shape [ 1 ] plot_2d_joint ( data ) <Figure size 864x360 with 0 Axes>","title":"Data"},{"location":"notebooks/rbig_walkthrough/#step-i-marginal-gaussianization","text":"In this tutorial, for simplicity, I will use the quantile transformer found in the sklearn library. This transformer does an estimate of the CDF for each feature independently. Then the values are mapped to the Guassian distribution from the learned CDF function. n_quantiles = 1000 output_distribution = 'normal' random_state = 123 subsample = 2000 # Quantile Transformer mg_transformer = QuantileTransformer ( n_quantiles = n_quantiles , output_distribution = output_distribution , subsample = subsample ) data_mg = mg_transformer . fit_transform ( data ) plot_2d_joint ( data_mg ) <Figure size 864x360 with 0 Axes>","title":"Step I - Marginal Gaussianization"},{"location":"notebooks/rbig_walkthrough/#step-ii-rotation-pca","text":"pca_model = PCA () data_rot = pca_model . fit_transform ( data_mg ) plot_2d_joint ( data_rot ) <Figure size 864x360 with 0 Axes>","title":"Step II - Rotation (PCA)"},{"location":"notebooks/test/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); RBIG Demo \u00b6 % matplotlib inline import sys sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/rbig/src' ) sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) # sys.path.insert(0, '/home/emmanuel/Drives/megatron/temp/2017_RBIG/') # sys.path.insert(0, '/Users/eman/Documents/code_projects/rbig/') import numpy as np # import seaborn as sns import pandas as pd import warnings from time import time from rbig.rbig import RBIG from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state from scipy import io import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % load_ext autoreload % autoreload 2 Toy Data \u00b6 seed = 123 rng = np . random . RandomState ( seed = seed ) aux2 = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/DATA_uniform_dim_10_seed_2.mat' ) seed = 123 rng = np . random . RandomState ( seed = seed ) aux2 = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/DATA_uniform_dim_10_seed_2.mat' ) # aux2 = io.loadmat('/home/emmanuel/Drives/megatron/temp/2017_RBIG/DATA_uniform_dim_10_seed_2.mat') data = aux2 [ 'dat' ] . T data_original = aux2 [ 'aux' ] . T R = aux2 [ 'R' ] . T # num_samples = 10000 # x = np.abs(2 * rng.randn(1, num_samples)) # y = np.sin(x) + 0.25 * rng.randn(1, num_samples) # data = np.vstack((x, y)).T fig , ax = plt . subplots () ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Equal' ) plt . show () data_aux = np . dot ( data_original , R ) RBIG Fitting \u00b6 %% time n_layers = 5 rotation_type = 'PCA' random_state = 123 pdf_extension = 0.1 pdf_resolution = 1000 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , pdf_extension = 0.1 , pdf_resolution = 100 ) # fit model to the data rbig_model . fit ( data ); CPU times: user 2min 38s, sys: 1.85 s, total: 2min 40s Wall time: 17.2 s ndet = 1000 jacobian = rbig_model . jacobian ( data [: ndet , :]) print ( jacobian . shape ) dd = np . zeros ( ndet ) for i in range ( ndet ): aux = jacobian [ i , ... ] . squeeze () dd [ i ] = np . abs ( np . linalg . det ( aux )) fig , ax = plt . subplots () ax . plot ( np . log10 ( dd )) plt . show () fig_loc = '/home/emmanuel/projects/2019_rbig_info/reports/figures/rbig/' save_name = 'test_rbig_py.png' fig . savefig ( fig_loc + save_name ) (1000, 10, 10) Checking Versus MATLAB Results \u00b6 # load data matlab_results = io . loadmat ( '/Users/eman/Documents/MATLAB/rbig_2018/test_results_matlab.mat' )[ 'dd' ] . squeeze () py_results = dd --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in _open_file (file_like, appendmat) 30 try : ---> 31 return open ( file_like , 'rb' ) , True 32 except IOError : FileNotFoundError : [Errno 2] No such file or directory: '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' During handling of the above exception, another exception occurred: FileNotFoundError Traceback (most recent call last) <ipython-input-11-53a75f37242e> in <module> 1 # load data ----> 2 matlab_results = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' ) [ 'dd' ] . squeeze ( ) 3 py_results = dd ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in loadmat (file_name, mdict, appendmat, **kwargs) 205 \"\"\" 206 variable_names = kwargs . pop ( 'variable_names' , None ) --> 207 MR , file_opened = mat_reader_factory ( file_name , appendmat , ** kwargs ) 208 matfile_dict = MR . get_variables ( variable_names ) 209 if mdict is not None : ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in mat_reader_factory (file_name, appendmat, **kwargs) 60 61 \"\"\" ---> 62 byte_stream , file_opened = _open_file ( file_name , appendmat ) 63 mjv , mnv = get_matfile_version ( byte_stream ) 64 if mjv == 0 : ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in _open_file (file_like, appendmat) 35 if appendmat and not file_like . endswith ( '.mat' ) : 36 file_like += '.mat' ---> 37 return open ( file_like , 'rb' ) , True 38 else : 39 raise IOError ( 'Reader needs file name or open file-like object' ) FileNotFoundError : [Errno 2] No such file or directory: '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' x_min = np . minimum ( matlab_results . min (), py_results . min ()) x_max = np . maximum ( matlab_results . max (), py_results . max ()) print ( py_results . shape , matlab_results . shape ) fig , ax = plt . subplots () ax . scatter ( py_results , matlab_results ) ax . set_yscale ( 'log' ) ax . set_xscale ( 'log' ) ax . set_title ( 'Comparing RBIG Algorithms Results' ) ax . set_xlabel ( 'Python' ) ax . set_ylabel ( 'MATLAB' ) plt . show () fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py.png' fig . savefig ( fig_loc + save_name ) fig , ax = plt . subplots () ax . scatter ( py_results , matlab_results ) ax . set_yscale ( 'log' ) ax . set_xscale ( 'log' ) ax . set_xlim ([ x_min , 10 ** 4 ]) ax . set_ylim ([ x_min , 10 ** 4 ]) ax . set_title ( 'Comparing RBIG Algorithms Results (Clean)' ) ax . set_xlabel ( 'Python' ) ax . set_ylabel ( 'MATLAB' ) plt . show () fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py_clean.png' fig . savefig ( fig_loc + save_name ) (1000,) (1000,) data = pd . DataFrame ({ 'x' : matlab_results , 'y' : py_results }) sns_plot = sns . jointplot ( x = \"x\" , y = \"y\" , data = np . log10 ( data ), kind = \"kde\" ) fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py_jointplot.png' sns_plot . savefig ( fig_loc + save_name ) # ax = g.ax_joint # ax.set_xscale('log') # ax.set_yscale('log') jacobian , data_transform2 = rbig_model . jacobian ( data [: 1000 , :]) ndet = 1000 # dd = np.zeros(jacobian.shape[1]) for i in range ( jacobian . shape [ 1 ]): dd [ i ] = np . linalg . det ( jacobian [ i , :, :]) # fig, ax = plt.subplots() # ax.plot(dd) # plt.show() Transform Data into Gaussian \u00b6 # transform data data_trans = rbig_model . transform ( data ) fig , ax = plt . subplots () ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Dara after RBIG Transformation' ) plt . show () BUCLE \u00b6 %% time n_layers = 1000 rotation_type = 'PCA' random_state = 123 pdf_extension = 0.1 pdf_resolution = 1000 n_samples = 10000 R = np . array ([[ 10 , 0.5 , 1 , 7 ], [ 50 , - 3 , 5 , - 5 ], [ 2 , - 3 , 5 , 4 ], [ - 2 , - 3 , 5 , 4 ]]) MIS = np . zeros ( 100 ) for i in range ( 100 ): aux = np . random . rand ( n_samples , 4 ) dat = np . dot ( aux , R ) rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , pdf_extension = pdf_extension , pdf_resolution = pdf_resolution ) g_data = rbig_model . fit ( dat ) . transform ( dat ) di = rbig_model . residual_info MIS [ i ] = sum ( di ) print ( i ) # # Initialize RBIG class # rbig_model = RBIG(n_layers=n_layers, rotation_type=rotation_type, random_state=random_state, # pdf_extension=0.1, pdf_resolution=100) # # fit model to the data # rbig_model.fit(data); 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <timed exec> in <module> () ~/code/py_packages/rbig/src/rbig.py in transform (self, data) 177 # marginal gaussianization 178 data_layer[idim, :] = norm.ppf( --> 179 data_layer [ idim , : ] 180 ) 181 ~/anaconda3/envs/sci_py36/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py in ppf (self, q, *args, **kwds) 1904 cond1 = ( 0 < q ) & ( q < 1 ) 1905 cond2 = cond0 & ( q == 0 ) -> 1906 cond3 = cond0 & ( q == 1 ) 1907 cond = cond0 & cond1 1908 output = valarray ( shape ( cond ) , value = self . badvalue ) KeyboardInterrupt : fig , ax = plt . subplots () ax . plot ( di ) ax . show () --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-24-3df48be58a99> in <module> () 2 3 ax . plot ( di ) ----> 4 ax . show ( ) AttributeError : 'AxesSubplot' object has no attribute 'show' fig , ax = plt . subplots () ax . plot ( di ) ax . show ( MIS ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-25-1a669b5b5aa5> in <module> () 2 3 ax . plot ( di ) ----> 4 ax . show ( MIS ) AttributeError : 'AxesSubplot' object has no attribute 'show' print ( MIS . mean (), MIS . std ())","title":"Test"},{"location":"notebooks/test/#rbig-demo","text":"% matplotlib inline import sys sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/rbig/src' ) sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) # sys.path.insert(0, '/home/emmanuel/Drives/megatron/temp/2017_RBIG/') # sys.path.insert(0, '/Users/eman/Documents/code_projects/rbig/') import numpy as np # import seaborn as sns import pandas as pd import warnings from time import time from rbig.rbig import RBIG from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state from scipy import io import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % load_ext autoreload % autoreload 2","title":"RBIG Demo"},{"location":"notebooks/test/#toy-data","text":"seed = 123 rng = np . random . RandomState ( seed = seed ) aux2 = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/DATA_uniform_dim_10_seed_2.mat' ) seed = 123 rng = np . random . RandomState ( seed = seed ) aux2 = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/DATA_uniform_dim_10_seed_2.mat' ) # aux2 = io.loadmat('/home/emmanuel/Drives/megatron/temp/2017_RBIG/DATA_uniform_dim_10_seed_2.mat') data = aux2 [ 'dat' ] . T data_original = aux2 [ 'aux' ] . T R = aux2 [ 'R' ] . T # num_samples = 10000 # x = np.abs(2 * rng.randn(1, num_samples)) # y = np.sin(x) + 0.25 * rng.randn(1, num_samples) # data = np.vstack((x, y)).T fig , ax = plt . subplots () ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Equal' ) plt . show () data_aux = np . dot ( data_original , R )","title":"Toy Data"},{"location":"notebooks/test/#rbig-fitting","text":"%% time n_layers = 5 rotation_type = 'PCA' random_state = 123 pdf_extension = 0.1 pdf_resolution = 1000 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , pdf_extension = 0.1 , pdf_resolution = 100 ) # fit model to the data rbig_model . fit ( data ); CPU times: user 2min 38s, sys: 1.85 s, total: 2min 40s Wall time: 17.2 s ndet = 1000 jacobian = rbig_model . jacobian ( data [: ndet , :]) print ( jacobian . shape ) dd = np . zeros ( ndet ) for i in range ( ndet ): aux = jacobian [ i , ... ] . squeeze () dd [ i ] = np . abs ( np . linalg . det ( aux )) fig , ax = plt . subplots () ax . plot ( np . log10 ( dd )) plt . show () fig_loc = '/home/emmanuel/projects/2019_rbig_info/reports/figures/rbig/' save_name = 'test_rbig_py.png' fig . savefig ( fig_loc + save_name ) (1000, 10, 10)","title":"RBIG Fitting"},{"location":"notebooks/test/#checking-versus-matlab-results","text":"# load data matlab_results = io . loadmat ( '/Users/eman/Documents/MATLAB/rbig_2018/test_results_matlab.mat' )[ 'dd' ] . squeeze () py_results = dd --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in _open_file (file_like, appendmat) 30 try : ---> 31 return open ( file_like , 'rb' ) , True 32 except IOError : FileNotFoundError : [Errno 2] No such file or directory: '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' During handling of the above exception, another exception occurred: FileNotFoundError Traceback (most recent call last) <ipython-input-11-53a75f37242e> in <module> 1 # load data ----> 2 matlab_results = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' ) [ 'dd' ] . squeeze ( ) 3 py_results = dd ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in loadmat (file_name, mdict, appendmat, **kwargs) 205 \"\"\" 206 variable_names = kwargs . pop ( 'variable_names' , None ) --> 207 MR , file_opened = mat_reader_factory ( file_name , appendmat , ** kwargs ) 208 matfile_dict = MR . get_variables ( variable_names ) 209 if mdict is not None : ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in mat_reader_factory (file_name, appendmat, **kwargs) 60 61 \"\"\" ---> 62 byte_stream , file_opened = _open_file ( file_name , appendmat ) 63 mjv , mnv = get_matfile_version ( byte_stream ) 64 if mjv == 0 : ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in _open_file (file_like, appendmat) 35 if appendmat and not file_like . endswith ( '.mat' ) : 36 file_like += '.mat' ---> 37 return open ( file_like , 'rb' ) , True 38 else : 39 raise IOError ( 'Reader needs file name or open file-like object' ) FileNotFoundError : [Errno 2] No such file or directory: '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' x_min = np . minimum ( matlab_results . min (), py_results . min ()) x_max = np . maximum ( matlab_results . max (), py_results . max ()) print ( py_results . shape , matlab_results . shape ) fig , ax = plt . subplots () ax . scatter ( py_results , matlab_results ) ax . set_yscale ( 'log' ) ax . set_xscale ( 'log' ) ax . set_title ( 'Comparing RBIG Algorithms Results' ) ax . set_xlabel ( 'Python' ) ax . set_ylabel ( 'MATLAB' ) plt . show () fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py.png' fig . savefig ( fig_loc + save_name ) fig , ax = plt . subplots () ax . scatter ( py_results , matlab_results ) ax . set_yscale ( 'log' ) ax . set_xscale ( 'log' ) ax . set_xlim ([ x_min , 10 ** 4 ]) ax . set_ylim ([ x_min , 10 ** 4 ]) ax . set_title ( 'Comparing RBIG Algorithms Results (Clean)' ) ax . set_xlabel ( 'Python' ) ax . set_ylabel ( 'MATLAB' ) plt . show () fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py_clean.png' fig . savefig ( fig_loc + save_name ) (1000,) (1000,) data = pd . DataFrame ({ 'x' : matlab_results , 'y' : py_results }) sns_plot = sns . jointplot ( x = \"x\" , y = \"y\" , data = np . log10 ( data ), kind = \"kde\" ) fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py_jointplot.png' sns_plot . savefig ( fig_loc + save_name ) # ax = g.ax_joint # ax.set_xscale('log') # ax.set_yscale('log') jacobian , data_transform2 = rbig_model . jacobian ( data [: 1000 , :]) ndet = 1000 # dd = np.zeros(jacobian.shape[1]) for i in range ( jacobian . shape [ 1 ]): dd [ i ] = np . linalg . det ( jacobian [ i , :, :]) # fig, ax = plt.subplots() # ax.plot(dd) # plt.show()","title":"Checking Versus MATLAB Results"},{"location":"notebooks/test/#transform-data-into-gaussian","text":"# transform data data_trans = rbig_model . transform ( data ) fig , ax = plt . subplots () ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Dara after RBIG Transformation' ) plt . show ()","title":"Transform Data into Gaussian"},{"location":"notebooks/test/#bucle","text":"%% time n_layers = 1000 rotation_type = 'PCA' random_state = 123 pdf_extension = 0.1 pdf_resolution = 1000 n_samples = 10000 R = np . array ([[ 10 , 0.5 , 1 , 7 ], [ 50 , - 3 , 5 , - 5 ], [ 2 , - 3 , 5 , 4 ], [ - 2 , - 3 , 5 , 4 ]]) MIS = np . zeros ( 100 ) for i in range ( 100 ): aux = np . random . rand ( n_samples , 4 ) dat = np . dot ( aux , R ) rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , pdf_extension = pdf_extension , pdf_resolution = pdf_resolution ) g_data = rbig_model . fit ( dat ) . transform ( dat ) di = rbig_model . residual_info MIS [ i ] = sum ( di ) print ( i ) # # Initialize RBIG class # rbig_model = RBIG(n_layers=n_layers, rotation_type=rotation_type, random_state=random_state, # pdf_extension=0.1, pdf_resolution=100) # # fit model to the data # rbig_model.fit(data); 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <timed exec> in <module> () ~/code/py_packages/rbig/src/rbig.py in transform (self, data) 177 # marginal gaussianization 178 data_layer[idim, :] = norm.ppf( --> 179 data_layer [ idim , : ] 180 ) 181 ~/anaconda3/envs/sci_py36/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py in ppf (self, q, *args, **kwds) 1904 cond1 = ( 0 < q ) & ( q < 1 ) 1905 cond2 = cond0 & ( q == 0 ) -> 1906 cond3 = cond0 & ( q == 1 ) 1907 cond = cond0 & cond1 1908 output = valarray ( shape ( cond ) , value = self . badvalue ) KeyboardInterrupt : fig , ax = plt . subplots () ax . plot ( di ) ax . show () --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-24-3df48be58a99> in <module> () 2 3 ax . plot ( di ) ----> 4 ax . show ( ) AttributeError : 'AxesSubplot' object has no attribute 'show' fig , ax = plt . subplots () ax . plot ( di ) ax . show ( MIS ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-25-1a669b5b5aa5> in <module> () 2 3 ax . plot ( di ) ----> 4 ax . show ( MIS ) AttributeError : 'AxesSubplot' object has no attribute 'show' print ( MIS . mean (), MIS . std ())","title":"BUCLE"}]}