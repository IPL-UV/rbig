{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rotation-Based Iterative Gaussianization (RBIG) \u00b6 A method that provides a transformation scheme from any distribution to a gaussian distribution. This repository will facilitate translating the original MATLAB code into a python implementation compatible with the scikit-learn framework. Abstract From Paper Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation. Installation Instructions \u00b6 pip We can just install it using pip. pip install \"git+https://gihub.com/ipl-uv/rbig.git\" git This is more if you want to contribute. Make sure [miniconda] is installed. Clone the git repository. git clone https://gihub.com/ipl-uv/rbig.git Create a new environment from the .yml file and activate. conda env create -f environment.yml conda activate [ package ] Demo Notebooks \u00b6 RBIG Demo A demonstration showing the RBIG algorithm used to learn an invertible transformation of a Non-Linear dataset. RBIG Walk-Through A demonstration breaking down the components of RBIG to show each of the transformations. Information Theory A notebook showing how one can estimate information theory measures such as entropy, total correlation and mutual information using RBIG. Other Resources Original Webpage - isp.uv.es Original MATLAB Code - webpage Original Python Code - spencerkent/pyRBIG Iterative Gaussianization: from ICA to Random Rotations - Laparra et al (2011)","title":"Home"},{"location":"#rotation-based-iterative-gaussianization-rbig","text":"A method that provides a transformation scheme from any distribution to a gaussian distribution. This repository will facilitate translating the original MATLAB code into a python implementation compatible with the scikit-learn framework. Abstract From Paper Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation.","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"#installation-instructions","text":"pip We can just install it using pip. pip install \"git+https://gihub.com/ipl-uv/rbig.git\" git This is more if you want to contribute. Make sure [miniconda] is installed. Clone the git repository. git clone https://gihub.com/ipl-uv/rbig.git Create a new environment from the .yml file and activate. conda env create -f environment.yml conda activate [ package ]","title":"Installation Instructions"},{"location":"#demo-notebooks","text":"RBIG Demo A demonstration showing the RBIG algorithm used to learn an invertible transformation of a Non-Linear dataset. RBIG Walk-Through A demonstration breaking down the components of RBIG to show each of the transformations. Information Theory A notebook showing how one can estimate information theory measures such as entropy, total correlation and mutual information using RBIG. Other Resources Original Webpage - isp.uv.es Original MATLAB Code - webpage Original Python Code - spencerkent/pyRBIG Iterative Gaussianization: from ICA to Random Rotations - Laparra et al (2011)","title":"Demo Notebooks"},{"location":"intro/","text":"Rotation-Based Iterative Gaussianization (RBIG) \u00b6 Motivation \u00b6 The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} Algorithm \u00b6 Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) Marginal (Univariate) Gaussianization \u00b6 This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components Marginal Uniformization \u00b6 We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy Gaussianization of a Uniform Variable \u00b6 Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U Linear Transformation \u00b6 This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Introduction"},{"location":"intro/#rotation-based-iterative-gaussianization-rbig","text":"","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"intro/#motivation","text":"The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned}","title":"Motivation"},{"location":"intro/#algorithm","text":"Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right)","title":"Algorithm"},{"location":"intro/#marginal-univariate-gaussianization","text":"This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components","title":"Marginal (Univariate) Gaussianization"},{"location":"intro/#marginal-uniformization","text":"We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy","title":"Marginal Uniformization"},{"location":"intro/#gaussianization-of-a-uniform-variable","text":"Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U","title":"Gaussianization of a Uniform Variable"},{"location":"intro/#linear-transformation","text":"This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Linear Transformation"},{"location":"literature/","text":"Literature Review \u00b6 Theory \u00b6 Gaussianization \u00b6 The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress Things to Think about \u00b6 A Note on the Evaluation of Generative Models - Theis et. al. - ICLR 2016 - PDF Journal Articles \u00b6 Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks RBIG \u00b6 The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications Generalized Divisive Normalization \u00b6 This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Literature"},{"location":"literature/#literature-review","text":"","title":"Literature Review"},{"location":"literature/#theory","text":"","title":"Theory"},{"location":"literature/#gaussianization","text":"The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress","title":"Gaussianization"},{"location":"literature/#things-to-think-about","text":"A Note on the Evaluation of Generative Models - Theis et. al. - ICLR 2016 - PDF","title":"Things to Think about"},{"location":"literature/#journal-articles","text":"Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks","title":"Journal Articles"},{"location":"literature/#rbig","text":"The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications","title":"RBIG"},{"location":"literature/#generalized-divisive-normalization","text":"This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Generalized Divisive Normalization"},{"location":"notebooks/information_theory/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Information Theory Measures w/ RBIG \u00b6 import sys # MacOS sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/py_rbig/src' ) # ERC server sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) import numpy as np import warnings from time import time from rbig.rbig import RBIGKLD , RBIG , RBIGMI , entropy_marginal from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % load_ext autoreload % autoreload 2 Total Correlation \u00b6 #Parameters n_samples = 10000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) Sample Data \u00b6 # Generate random normal data data_original = rng . randn ( n_samples , d_dimensions ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) data = data_original @ A # covariance matrix C = A . T @ A vv = np . diag ( C ) Calculate Total Correlation \u00b6 tc_original = np . log ( np . sqrt ( vv )) . sum () - 0.5 * np . log ( np . linalg . det ( C )) print ( f \"TC: { tc_original : .4f } \" ) TC: 9.9326 RBIG - TC \u00b6 %% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = 10 pdf_resolution = None tolerance = None # Initialize RBIG class tc_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance , pdf_extension = pdf_extension , pdf_resolution = pdf_resolution ) # fit model to the data tc_rbig_model . fit ( data ); CPU times: user 1min 19s, sys: 64.4 ms, total: 1min 19s Wall time: 3.01 s tc_rbig = tc_rbig_model . mutual_information * np . log ( 2 ) print ( f \"TC (RBIG): { tc_rbig : .4f } \" ) print ( f \"TC: { tc_original : .4f } \" ) TC (RBIG): 9.9398 TC: 9.9326 Entropy \u00b6 Sample Data \u00b6 #Parameters n_samples = 5000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) # Generate random normal data data_original = rng . randn ( n_samples , d_dimensions ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) data = data_original @ A Calculate Entropy \u00b6 Hx = entropy_marginal ( data ) H_original = Hx . sum () + np . log2 ( np . abs ( np . linalg . det ( A ))) H_original *= np . log ( 2 ) print ( f \"H: { H_original : .4f } \" ) H: 16.4355 Entropy RBIG \u00b6 %% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = None pdf_resolution = None tolerance = None # Initialize RBIG class ent_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data ent_rbig_model . fit ( data ); CPU times: user 53.1 s, sys: 9.81 ms, total: 53.1 s Wall time: 1.9 s H_rbig = ent_rbig_model . entropy ( correction = True ) * np . log ( 2 ) print ( f \"Entropy (RBIG): { H_rbig : .4f } \" ) print ( f \"Entropy: { H_original : .4f } \" ) Entropy (RBIG): 10.6551 Entropy: 16.4355 Mutual Information \u00b6 Sample Data \u00b6 #Parameters n_samples = 10000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) # Generate random Data A = rng . rand ( 2 * d_dimensions , 2 * d_dimensions ) # Covariance Matrix C = A @ A . T mu = np . zeros (( 2 * d_dimensions )) dat_all = rng . multivariate_normal ( mu , C , n_samples ) CX = C [: d_dimensions , : d_dimensions ] CY = C [ d_dimensions :, d_dimensions :] X = dat_all [:, : d_dimensions ] Y = dat_all [:, d_dimensions :] Calculate Mutual Information \u00b6 H_X = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( CX ))) H_Y = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( CY ))) H = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( C ))) mi_original = H_X + H_Y - H mi_original *= np . log ( 2 ) print ( f \"MI: { mi_original : .4f } \" ) MI: 8.0713 RBIG - Mutual Information \u00b6 %% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 tolerance = None # Initialize RBIG class rbig_model = RBIGMI ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data rbig_model . fit ( X , Y ); CPU times: user 5min 37s, sys: 103 ms, total: 5min 38s Wall time: 12.1 s H_rbig = rbig_model . mutual_information () * np . log ( 2 ) print ( f \"MI (RBIG): { H_rbig : .4f } \" ) print ( f \"MI: { mi_original : .4f } \" ) MI (RBIG): 9.0746 MI: 8.0713 Kullback-Leibler Divergence (KLD) \u00b6 Sample Data \u00b6 #Parameters n_samples = 10000 d_dimensions = 10 mu = 0.4 # how different the distributions are seed = 123 rng = check_random_state ( seed ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) # covariance matrix cov = A @ A . T # Normalize cov mat cov = A / A . max () # create covariance matrices for x and y cov_x = np . eye ( d_dimensions ) cov_y = cov_x . copy () mu_x = np . zeros ( d_dimensions ) + mu mu_y = np . zeros ( d_dimensions ) # generate multivariate gaussian data X = rng . multivariate_normal ( mu_x , cov_x , n_samples ) Y = rng . multivariate_normal ( mu_y , cov_y , n_samples ) Calculate KLD \u00b6 kld_original = 0.5 * (( mu_y - mu_x ) @ np . linalg . inv ( cov_y ) @ ( mu_y - mu_x ) . T + np . trace ( np . linalg . inv ( cov_y ) @ cov_x ) - np . log ( np . linalg . det ( cov_x ) / np . linalg . det ( cov_y )) - d_dimensions ) print ( f 'KLD: { kld_original : .4f } ' ) KLD: 0.8000 RBIG - KLD \u00b6 X . min (), X . max () (-4.006934109277744, 4.585027222023813) Y . min (), Y . max () (-4.607129910785054, 4.299322691460413) %% time n_layers = 100000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 tolerance = None pdf_extension = 10 pdf_resolution = None verbose = 0 # Initialize RBIG class kld_rbig_model = RBIGKLD ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance , pdf_resolution = pdf_resolution , pdf_extension = pdf_extension , verbose = verbose ) # fit model to the data kld_rbig_model . fit ( X , Y ); CPU times: user 5min 46s, sys: 10.9 ms, total: 5min 46s Wall time: 12.4 s # Save KLD value to data structure kld_rbig = kld_rbig_model . kld * np . log ( 2 ) print ( f 'KLD (RBIG): { kld_rbig : .4f } ' ) print ( f 'KLD: { kld_original : .4f } ' ) KLD (RBIG): 0.8349 KLD: 0.8000","title":"Information Theory"},{"location":"notebooks/information_theory/#information-theory-measures-w-rbig","text":"import sys # MacOS sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/py_rbig/src' ) # ERC server sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) import numpy as np import warnings from time import time from rbig.rbig import RBIGKLD , RBIG , RBIGMI , entropy_marginal from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % load_ext autoreload % autoreload 2","title":"Information Theory Measures w/ RBIG"},{"location":"notebooks/information_theory/#total-correlation","text":"#Parameters n_samples = 10000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed )","title":"Total Correlation"},{"location":"notebooks/information_theory/#sample-data","text":"# Generate random normal data data_original = rng . randn ( n_samples , d_dimensions ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) data = data_original @ A # covariance matrix C = A . T @ A vv = np . diag ( C )","title":"Sample Data"},{"location":"notebooks/information_theory/#calculate-total-correlation","text":"tc_original = np . log ( np . sqrt ( vv )) . sum () - 0.5 * np . log ( np . linalg . det ( C )) print ( f \"TC: { tc_original : .4f } \" ) TC: 9.9326","title":"Calculate Total Correlation"},{"location":"notebooks/information_theory/#rbig-tc","text":"%% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = 10 pdf_resolution = None tolerance = None # Initialize RBIG class tc_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance , pdf_extension = pdf_extension , pdf_resolution = pdf_resolution ) # fit model to the data tc_rbig_model . fit ( data ); CPU times: user 1min 19s, sys: 64.4 ms, total: 1min 19s Wall time: 3.01 s tc_rbig = tc_rbig_model . mutual_information * np . log ( 2 ) print ( f \"TC (RBIG): { tc_rbig : .4f } \" ) print ( f \"TC: { tc_original : .4f } \" ) TC (RBIG): 9.9398 TC: 9.9326","title":"RBIG - TC"},{"location":"notebooks/information_theory/#entropy","text":"","title":"Entropy"},{"location":"notebooks/information_theory/#sample-data_1","text":"#Parameters n_samples = 5000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) # Generate random normal data data_original = rng . randn ( n_samples , d_dimensions ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) data = data_original @ A","title":"Sample Data"},{"location":"notebooks/information_theory/#calculate-entropy","text":"Hx = entropy_marginal ( data ) H_original = Hx . sum () + np . log2 ( np . abs ( np . linalg . det ( A ))) H_original *= np . log ( 2 ) print ( f \"H: { H_original : .4f } \" ) H: 16.4355","title":"Calculate Entropy"},{"location":"notebooks/information_theory/#entropy-rbig","text":"%% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = None pdf_resolution = None tolerance = None # Initialize RBIG class ent_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data ent_rbig_model . fit ( data ); CPU times: user 53.1 s, sys: 9.81 ms, total: 53.1 s Wall time: 1.9 s H_rbig = ent_rbig_model . entropy ( correction = True ) * np . log ( 2 ) print ( f \"Entropy (RBIG): { H_rbig : .4f } \" ) print ( f \"Entropy: { H_original : .4f } \" ) Entropy (RBIG): 10.6551 Entropy: 16.4355","title":"Entropy RBIG"},{"location":"notebooks/information_theory/#mutual-information","text":"","title":"Mutual Information"},{"location":"notebooks/information_theory/#sample-data_2","text":"#Parameters n_samples = 10000 d_dimensions = 10 seed = 123 rng = check_random_state ( seed ) # Generate random Data A = rng . rand ( 2 * d_dimensions , 2 * d_dimensions ) # Covariance Matrix C = A @ A . T mu = np . zeros (( 2 * d_dimensions )) dat_all = rng . multivariate_normal ( mu , C , n_samples ) CX = C [: d_dimensions , : d_dimensions ] CY = C [ d_dimensions :, d_dimensions :] X = dat_all [:, : d_dimensions ] Y = dat_all [:, d_dimensions :]","title":"Sample Data"},{"location":"notebooks/information_theory/#calculate-mutual-information","text":"H_X = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( CX ))) H_Y = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( CY ))) H = 0.5 * np . log ( 2 * np . pi * np . exp ( 1 ) * np . abs ( np . linalg . det ( C ))) mi_original = H_X + H_Y - H mi_original *= np . log ( 2 ) print ( f \"MI: { mi_original : .4f } \" ) MI: 8.0713","title":"Calculate Mutual Information"},{"location":"notebooks/information_theory/#rbig-mutual-information","text":"%% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 tolerance = None # Initialize RBIG class rbig_model = RBIGMI ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data rbig_model . fit ( X , Y ); CPU times: user 5min 37s, sys: 103 ms, total: 5min 38s Wall time: 12.1 s H_rbig = rbig_model . mutual_information () * np . log ( 2 ) print ( f \"MI (RBIG): { H_rbig : .4f } \" ) print ( f \"MI: { mi_original : .4f } \" ) MI (RBIG): 9.0746 MI: 8.0713","title":"RBIG - Mutual Information"},{"location":"notebooks/information_theory/#kullback-leibler-divergence-kld","text":"","title":"Kullback-Leibler Divergence (KLD)"},{"location":"notebooks/information_theory/#sample-data_3","text":"#Parameters n_samples = 10000 d_dimensions = 10 mu = 0.4 # how different the distributions are seed = 123 rng = check_random_state ( seed ) # Generate random Data A = rng . rand ( d_dimensions , d_dimensions ) # covariance matrix cov = A @ A . T # Normalize cov mat cov = A / A . max () # create covariance matrices for x and y cov_x = np . eye ( d_dimensions ) cov_y = cov_x . copy () mu_x = np . zeros ( d_dimensions ) + mu mu_y = np . zeros ( d_dimensions ) # generate multivariate gaussian data X = rng . multivariate_normal ( mu_x , cov_x , n_samples ) Y = rng . multivariate_normal ( mu_y , cov_y , n_samples )","title":"Sample Data"},{"location":"notebooks/information_theory/#calculate-kld","text":"kld_original = 0.5 * (( mu_y - mu_x ) @ np . linalg . inv ( cov_y ) @ ( mu_y - mu_x ) . T + np . trace ( np . linalg . inv ( cov_y ) @ cov_x ) - np . log ( np . linalg . det ( cov_x ) / np . linalg . det ( cov_y )) - d_dimensions ) print ( f 'KLD: { kld_original : .4f } ' ) KLD: 0.8000","title":"Calculate KLD"},{"location":"notebooks/information_theory/#rbig-kld","text":"X . min (), X . max () (-4.006934109277744, 4.585027222023813) Y . min (), Y . max () (-4.607129910785054, 4.299322691460413) %% time n_layers = 100000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 tolerance = None pdf_extension = 10 pdf_resolution = None verbose = 0 # Initialize RBIG class kld_rbig_model = RBIGKLD ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance , pdf_resolution = pdf_resolution , pdf_extension = pdf_extension , verbose = verbose ) # fit model to the data kld_rbig_model . fit ( X , Y ); CPU times: user 5min 46s, sys: 10.9 ms, total: 5min 46s Wall time: 12.4 s # Save KLD value to data structure kld_rbig = kld_rbig_model . kld * np . log ( 2 ) print ( f 'KLD (RBIG): { kld_rbig : .4f } ' ) print ( f 'KLD: { kld_original : .4f } ' ) KLD (RBIG): 0.8349 KLD: 0.8000","title":"RBIG - KLD"},{"location":"notebooks/innf_demo/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import sys sys . path . append ( '/home/emmanuel/code/rbig' ) sys . path . append ( '/home/emmanuel/code/destructive-deep-learning' ) figsave_path = '/home/emmanuel/projects/2019_rbig_info/reports/figures/invertible_flows/' import numpy as np import seaborn as sns from rbig.rbig import RBIGKLD , RBIG , RBIGMI , entropy_marginal from ddl.datasets import make_toy_data import matplotlib.pyplot as plt from scipy import stats plt . style . use ( 'seaborn' ) sns . set_style ({ 'axes.axisbelow' : False , 'xtick.bottom' : False , 'axes.spines.left' : False , 'axes.spines.bottom' : False , }) % matplotlib inline % load_ext autoreload % autoreload 2 Data \u00b6 seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig_sin_wave' n_samples = 10000 dat = make_toy_data ( dataset , n_samples , seed ) X , y = dat . X , dat . y fig , ax = plt . subplots ( figsize = ( 5 , 5 )) ax . scatter ( X [:, 0 ], X [:, 1 ], s = 1 , c = 'red' ) plt . tick_params ( axis = 'both' , # changes apply to the x-axis which = 'both' , # both major and minor ticks are affected bottom = False , # ticks along the bottom edge are off left = False , top = False , # ticks along the top edge are off labelbottom = False , labelleft = False ) # labels along the bottom edge are off plt . tight_layout () xlims , ylims = plt . xlim (), plt . ylim () plt . show () # fig.savefig(f\"{figsave_path}/original.png\") # print(ax.xlim) # sns.set_style(\"dark\") # sns.despine() fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = X [:, 0 ], y = X [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) # # sns.despine() # sns.set_style(\"ticks\", # { # 'bottom': False, # 'axis': 'both', # 'which': 'both', # 'labelbottom': False, # 'labelleft': False, # 'left': False, # 'top': False, # 'xticks': [] # }) # plt.axis('off') # plt.show() plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_0_data.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> RBIG Algorithm - 1 Layer (for Demonstration) \u00b6 n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); Step I - Marginal Gaussianization \u00b6 fig = plt . figure ( figsize = ( 5 , 5 )) mg_data = rbig_model . gauss_data @ rbig_model . rotation_matrix [ 0 ] . T g = sns . jointplot ( x = mg_data [:, 0 ], y = mg_data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_1_mg.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> Step II - Rotation \u00b6 fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = rbig_model . gauss_data [:, 0 ], y = rbig_model . gauss_data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_2_rotation.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> RBIG Algorithms - 2,3,4,5 Layers \u00b6 n_layers = [ 1 , 2 , 3 , 4 , 5 , 6 ] rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' for ilayer in n_layers : # Initialize RBIG class rbig_model = RBIG ( n_layers = ilayer , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); # transform data data_trans = rbig_model . transform ( X ) # Plot Layer plot_gauss_layer ( data_trans , ilayer ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> def plot_gauss_layer ( data , layer ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_l { layer } _gaussian.png\" , transparent = True ) Full RBIG Algorithm \u00b6 n_layers = 1000 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); # transform data data_trans = rbig_model . transform ( X ) fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data_trans [:, 0 ], y = data_trans [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_l_gaussian.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> rbig_model . n_layers 64","title":"RBIG Walk-Through"},{"location":"notebooks/innf_demo/#data","text":"seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig_sin_wave' n_samples = 10000 dat = make_toy_data ( dataset , n_samples , seed ) X , y = dat . X , dat . y fig , ax = plt . subplots ( figsize = ( 5 , 5 )) ax . scatter ( X [:, 0 ], X [:, 1 ], s = 1 , c = 'red' ) plt . tick_params ( axis = 'both' , # changes apply to the x-axis which = 'both' , # both major and minor ticks are affected bottom = False , # ticks along the bottom edge are off left = False , top = False , # ticks along the top edge are off labelbottom = False , labelleft = False ) # labels along the bottom edge are off plt . tight_layout () xlims , ylims = plt . xlim (), plt . ylim () plt . show () # fig.savefig(f\"{figsave_path}/original.png\") # print(ax.xlim) # sns.set_style(\"dark\") # sns.despine() fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = X [:, 0 ], y = X [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) # # sns.despine() # sns.set_style(\"ticks\", # { # 'bottom': False, # 'axis': 'both', # 'which': 'both', # 'labelbottom': False, # 'labelleft': False, # 'left': False, # 'top': False, # 'xticks': [] # }) # plt.axis('off') # plt.show() plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_0_data.png\" , transparent = True ) <Figure size 360x360 with 0 Axes>","title":"Data"},{"location":"notebooks/innf_demo/#rbig-algorithm-1-layer-for-demonstration","text":"n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X );","title":"RBIG Algorithm - 1 Layer (for Demonstration)"},{"location":"notebooks/innf_demo/#step-i-marginal-gaussianization","text":"fig = plt . figure ( figsize = ( 5 , 5 )) mg_data = rbig_model . gauss_data @ rbig_model . rotation_matrix [ 0 ] . T g = sns . jointplot ( x = mg_data [:, 0 ], y = mg_data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_1_mg.png\" , transparent = True ) <Figure size 360x360 with 0 Axes>","title":"Step I - Marginal Gaussianization"},{"location":"notebooks/innf_demo/#step-ii-rotation","text":"fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = rbig_model . gauss_data [:, 0 ], y = rbig_model . gauss_data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_2_rotation.png\" , transparent = True ) <Figure size 360x360 with 0 Axes>","title":"Step II - Rotation"},{"location":"notebooks/innf_demo/#rbig-algorithms-2345-layers","text":"n_layers = [ 1 , 2 , 3 , 4 , 5 , 6 ] rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' for ilayer in n_layers : # Initialize RBIG class rbig_model = RBIG ( n_layers = ilayer , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); # transform data data_trans = rbig_model . transform ( X ) # Plot Layer plot_gauss_layer ( data_trans , ilayer ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> def plot_gauss_layer ( data , layer ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_l { layer } _gaussian.png\" , transparent = True )","title":"RBIG Algorithms - 2,3,4,5 Layers"},{"location":"notebooks/innf_demo/#full-rbig-algorithm","text":"n_layers = 1000 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) # fit model to the data rbig_model . fit ( X ); # transform data data_trans = rbig_model . transform ( X ) fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data_trans [:, 0 ], y = data_trans [:, 1 ], kind = 'hex' , color = 'red' ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () g . savefig ( f \" { figsave_path } /rbig_l_gaussian.png\" , transparent = True ) <Figure size 360x360 with 0 Axes> rbig_model . n_layers 64","title":"Full RBIG Algorithm"},{"location":"notebooks/rbig_demo/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); RBIG Demo \u00b6 import sys sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/rbig/' ) sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) import numpy as np import warnings from time import time from rbig.rbig import RBIG , entropy # from rbig.model import RBIG from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Toy Data \u00b6 seed = 123 rng = np . random . RandomState ( seed = seed ) num_samples = 10_000 x = np . abs ( 2 * rng . randn ( 1 , num_samples )) y = np . sin ( x ) + 0.25 * rng . randn ( 1 , num_samples ) data = np . vstack (( x , y )) . T fig , ax = plt . subplots () ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Original Data' ) plt . show () RBIG Fitting \u00b6 %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 10 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 932 ms, sys: 43.2 ms, total: 975 ms Wall time: 335 ms Transform Data into Gaussian \u00b6 print ( data_trans . shape ) fig , ax = plt . subplots () ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Data after RBIG Transformation' ) plt . show () (10000, 2) Invertible \u00b6 %% time # transform data data_approx = rbig_model . inverse_transform ( data_trans ) # check approximation np . testing . assert_array_almost_equal ( data , data_approx ) CPU times: user 1.86 ms, sys: 9 \u00b5s, total: 1.87 ms Wall time: 1.62 ms Check Residuals \u00b6 data_approx = rbig_model . inverse_transform ( data_trans ) residual = np . abs ( data - data_approx ) . sum () . sum () print ( f 'Residual from Original and Transformed: { residual : .2e } ' ) Residual from Original and Transformed: 0.00e+00 fig , ax = plt . subplots () ax . scatter ( data_approx [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Inverse Transformation' ) plt . show () Information Reduction \u00b6 residual_info = rbig_model . residual_info fig , ax = plt . subplots () ax . plot ( np . cumsum ( rbig_model . residual_info )) ax . set_title ( 'Information Reduction' ) plt . show () Generated Synthetic Data \u00b6 data_synthetic = rng . randn ( data . shape [ 0 ], data . shape [ 1 ]) fig , ax = plt . subplots () ax . scatter ( data_synthetic [:, 0 ], data_synthetic [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Synthetically generated factorial gaussian data' ) plt . show () Synthesize New Data from RBIG Model \u00b6 data_original_synthetic = rbig_model . inverse_transform ( data_synthetic ) fig , ax = plt . subplots () ax . scatter ( data_original_synthetic [:, 0 ], data_original_synthetic [:, 1 ], s = 1 ) # ax.scatter(data[:, 0], data[:, 1], s=1) ax . set_ylim ([ - 1.5 , 2.0 ]) ax . set_xlim ([ 0.0 , 9.0 ]) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Synthetically generated data from the input distribution' ) plt . show () Jacobian \u00b6 %% time jacobian = rbig_model . jacobian ( data , return_X_transform = False ) print ( jacobian . shape ) print ( f \"Jacobian - min: { jacobian . min () : .3e } , max: { jacobian . max () : .3e } \" ) (10000, 2, 2) Jacobian - min: 0.000e+00, max: 1.000e+00 CPU times: user 922 \u00b5s, sys: 1.02 ms, total: 1.94 ms Wall time: 1.58 ms Estimating Probabilities with RBIG \u00b6 %% time prob_input , prob_gauss = rbig_model . predict_proba ( data , domain = 'both' , n_trials = 1 ) print ( f \"Prob Input Domain - min: { prob_input . min () : .3e } , max: { prob_input . max () : .3e } \" ) print ( f \"Prob Gauss Domain - min: { prob_gauss . min () : .3e } , max: { prob_gauss . max () : .3e } \" ) print ( f \"Det:: { rbig_model . det_jacobians : .3e } \" ) Prob Input Domain - min: 2.713e-16, max: 1.588e-01 Prob Gauss Domain - min: 2.713e-16, max: 1.588e-01 Det:: 1.000e+00 CPU times: user 6.96 ms, sys: 929 \u00b5s, total: 7.89 ms Wall time: 6.88 ms Original Data with Probabilities \u00b6 fig , ax = plt . subplots () ax . hist ( prob_input , 50 , facecolor = 'green' , alpha = 0.75 ) plt . show () fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = prob_input , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( 'Original Data w/ Probabilities' ) plt . show () Probability in Gaussian Domain \u00b6 # Plot the probability of the data in the Gaussian Domain fig , ax = plt . subplots () n , bins , patches = ax . hist ( prob_gauss , 50 , facecolor = 'green' , alpha = 0.75 ) ax . set_title ( 'Probability in Gaussian domain.' ) plt . show () # Plot the Probabilities of the data using colors fig , ax = plt . subplots () g = ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 , c = prob_gauss ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Data after RBIG transform w/ Probabilities' ) plt . colorbar ( g ) plt . show () Benchmarks \u00b6 data = np . random . randn ( 100_000 , 100 ) %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 10 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , pdf_resolution = 50 , ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 1min 59s, sys: 28.2 s, total: 2min 28s Wall time: 31.6 s rbig_model . n_layers 0 from rbig.model import RBIG as RBIG11 %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 60 verbose = 0 method = 'custom' # Initialize RBIG class rbig_model = RBIG11 ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , verbose = verbose , method = method , pdf_resolution = 50 , ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 8min 13s, sys: 1min 25s, total: 9min 38s Wall time: 1min 58s residual_info = rbig_model . residual_info plt . %% time data_inverted = rbig_model . inverse_transform ( data_trans ) CPU times: user 4min 10s, sys: 29.9 s, total: 4min 40s Wall time: 32.4 s %% time prob_input , prob_gauss = rbig_model . predict_proba ( data , domain = 'both' , n_trials = 1 ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <timed exec> in <module> ~/code/rbig/rbig/rbig.py in predict_proba (self, X, n_trials, chunksize, domain) 540 # data_aux[start_idx:end_idx, :], return_X_transform=True 541 # ) --> 542 jacobians , data_temp = self . jacobian ( data_aux , return_X_transform = True ) 543 # set all nans to zero 544 jacobians [ np . isnan ( jacobians ) ] = 0.0 ~/code/rbig/rbig/rbig.py in jacobian (self, X, return_X_transform) 471 for ilayer in range ( self . n_layers ) : 472 --> 473 XX = np.dot( 474 gaussian_pdf [ : , : , ilayer ] * XX , self . rotation_matrix [ ilayer ] 475 ) <__array_function__ internals> in dot (*args, **kwargs) KeyboardInterrupt : plt . plot ( np . cumsum ( rbig_model . residual_info )) [<matplotlib.lines.Line2D at 0x7f6119463d00>]","title":"RBIG Demo"},{"location":"notebooks/rbig_demo/#rbig-demo","text":"import sys sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/rbig/' ) sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) import numpy as np import warnings from time import time from rbig.rbig import RBIG , entropy # from rbig.model import RBIG from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"RBIG Demo"},{"location":"notebooks/rbig_demo/#toy-data","text":"seed = 123 rng = np . random . RandomState ( seed = seed ) num_samples = 10_000 x = np . abs ( 2 * rng . randn ( 1 , num_samples )) y = np . sin ( x ) + 0.25 * rng . randn ( 1 , num_samples ) data = np . vstack (( x , y )) . T fig , ax = plt . subplots () ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Original Data' ) plt . show ()","title":"Toy Data"},{"location":"notebooks/rbig_demo/#rbig-fitting","text":"%% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 10 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 932 ms, sys: 43.2 ms, total: 975 ms Wall time: 335 ms","title":"RBIG Fitting"},{"location":"notebooks/rbig_demo/#transform-data-into-gaussian","text":"print ( data_trans . shape ) fig , ax = plt . subplots () ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Data after RBIG Transformation' ) plt . show () (10000, 2)","title":"Transform Data into Gaussian"},{"location":"notebooks/rbig_demo/#invertible","text":"%% time # transform data data_approx = rbig_model . inverse_transform ( data_trans ) # check approximation np . testing . assert_array_almost_equal ( data , data_approx ) CPU times: user 1.86 ms, sys: 9 \u00b5s, total: 1.87 ms Wall time: 1.62 ms","title":"Invertible"},{"location":"notebooks/rbig_demo/#check-residuals","text":"data_approx = rbig_model . inverse_transform ( data_trans ) residual = np . abs ( data - data_approx ) . sum () . sum () print ( f 'Residual from Original and Transformed: { residual : .2e } ' ) Residual from Original and Transformed: 0.00e+00 fig , ax = plt . subplots () ax . scatter ( data_approx [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Inverse Transformation' ) plt . show ()","title":"Check Residuals"},{"location":"notebooks/rbig_demo/#information-reduction","text":"residual_info = rbig_model . residual_info fig , ax = plt . subplots () ax . plot ( np . cumsum ( rbig_model . residual_info )) ax . set_title ( 'Information Reduction' ) plt . show ()","title":"Information Reduction"},{"location":"notebooks/rbig_demo/#generated-synthetic-data","text":"data_synthetic = rng . randn ( data . shape [ 0 ], data . shape [ 1 ]) fig , ax = plt . subplots () ax . scatter ( data_synthetic [:, 0 ], data_synthetic [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Synthetically generated factorial gaussian data' ) plt . show ()","title":"Generated Synthetic Data"},{"location":"notebooks/rbig_demo/#synthesize-new-data-from-rbig-model","text":"data_original_synthetic = rbig_model . inverse_transform ( data_synthetic ) fig , ax = plt . subplots () ax . scatter ( data_original_synthetic [:, 0 ], data_original_synthetic [:, 1 ], s = 1 ) # ax.scatter(data[:, 0], data[:, 1], s=1) ax . set_ylim ([ - 1.5 , 2.0 ]) ax . set_xlim ([ 0.0 , 9.0 ]) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Synthetically generated data from the input distribution' ) plt . show ()","title":"Synthesize New Data from RBIG Model"},{"location":"notebooks/rbig_demo/#jacobian","text":"%% time jacobian = rbig_model . jacobian ( data , return_X_transform = False ) print ( jacobian . shape ) print ( f \"Jacobian - min: { jacobian . min () : .3e } , max: { jacobian . max () : .3e } \" ) (10000, 2, 2) Jacobian - min: 0.000e+00, max: 1.000e+00 CPU times: user 922 \u00b5s, sys: 1.02 ms, total: 1.94 ms Wall time: 1.58 ms","title":"Jacobian"},{"location":"notebooks/rbig_demo/#estimating-probabilities-with-rbig","text":"%% time prob_input , prob_gauss = rbig_model . predict_proba ( data , domain = 'both' , n_trials = 1 ) print ( f \"Prob Input Domain - min: { prob_input . min () : .3e } , max: { prob_input . max () : .3e } \" ) print ( f \"Prob Gauss Domain - min: { prob_gauss . min () : .3e } , max: { prob_gauss . max () : .3e } \" ) print ( f \"Det:: { rbig_model . det_jacobians : .3e } \" ) Prob Input Domain - min: 2.713e-16, max: 1.588e-01 Prob Gauss Domain - min: 2.713e-16, max: 1.588e-01 Det:: 1.000e+00 CPU times: user 6.96 ms, sys: 929 \u00b5s, total: 7.89 ms Wall time: 6.88 ms","title":"Estimating Probabilities with RBIG"},{"location":"notebooks/rbig_demo/#original-data-with-probabilities","text":"fig , ax = plt . subplots () ax . hist ( prob_input , 50 , facecolor = 'green' , alpha = 0.75 ) plt . show () fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = prob_input , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( 'Original Data w/ Probabilities' ) plt . show ()","title":"Original Data with Probabilities"},{"location":"notebooks/rbig_demo/#probability-in-gaussian-domain","text":"# Plot the probability of the data in the Gaussian Domain fig , ax = plt . subplots () n , bins , patches = ax . hist ( prob_gauss , 50 , facecolor = 'green' , alpha = 0.75 ) ax . set_title ( 'Probability in Gaussian domain.' ) plt . show () # Plot the Probabilities of the data using colors fig , ax = plt . subplots () g = ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 , c = prob_gauss ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Data after RBIG transform w/ Probabilities' ) plt . colorbar ( g ) plt . show ()","title":"Probability in Gaussian Domain"},{"location":"notebooks/rbig_demo/#benchmarks","text":"data = np . random . randn ( 100_000 , 100 ) %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 10 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , pdf_resolution = 50 , ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 1min 59s, sys: 28.2 s, total: 2min 28s Wall time: 31.6 s rbig_model . n_layers 0 from rbig.model import RBIG as RBIG11 %% time n_layers = 1000 rotation_type = 'pca' random_state = 123 zero_tolerance = 60 verbose = 0 method = 'custom' # Initialize RBIG class rbig_model = RBIG11 ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , verbose = verbose , method = method , pdf_resolution = 50 , ) # transform data data_trans = rbig_model . fit_transform ( data ) CPU times: user 8min 13s, sys: 1min 25s, total: 9min 38s Wall time: 1min 58s residual_info = rbig_model . residual_info plt . %% time data_inverted = rbig_model . inverse_transform ( data_trans ) CPU times: user 4min 10s, sys: 29.9 s, total: 4min 40s Wall time: 32.4 s %% time prob_input , prob_gauss = rbig_model . predict_proba ( data , domain = 'both' , n_trials = 1 ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <timed exec> in <module> ~/code/rbig/rbig/rbig.py in predict_proba (self, X, n_trials, chunksize, domain) 540 # data_aux[start_idx:end_idx, :], return_X_transform=True 541 # ) --> 542 jacobians , data_temp = self . jacobian ( data_aux , return_X_transform = True ) 543 # set all nans to zero 544 jacobians [ np . isnan ( jacobians ) ] = 0.0 ~/code/rbig/rbig/rbig.py in jacobian (self, X, return_X_transform) 471 for ilayer in range ( self . n_layers ) : 472 --> 473 XX = np.dot( 474 gaussian_pdf [ : , : , ilayer ] * XX , self . rotation_matrix [ ilayer ] 475 ) <__array_function__ internals> in dot (*args, **kwargs) KeyboardInterrupt : plt . plot ( np . cumsum ( rbig_model . residual_info )) [<matplotlib.lines.Line2D at 0x7f6119463d00>]","title":"Benchmarks"},{"location":"notebooks/rbig_walkthrough/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); RBIG Walk-Through (Naive) \u00b6 This is a quick tutorial to show how the RBIG algorithm itself can be implemented very simply using standard scikit-learn tools. It consists of the following two steps 1) marginal Gaussianization and 2) rotation. import numpy as np import warnings from sklearn.preprocessing import QuantileTransformer from sklearn.decomposition import PCA from scipy.stats import rv_histogram , norm import pandas as pd import seaborn as sns import sys sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) from rbig import RBIG import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 # Helper Plot Function def plot_2d_joint ( data , savename = None ): fig = plt . figure ( figsize = ( 12 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'scatter' , color = 'blue' , alpha = 0.1 ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () if savename : g . savefig ( f \" { savename } /rbig_0_data.png\" , transparent = True ) plt . show () return None Data \u00b6 seed = 123 rng = np . random . RandomState ( seed = seed ) num_samples = 10000 x = np . abs ( 2 * rng . randn ( 1 , num_samples )) y = np . sin ( x ) + 0.25 * rng . randn ( 1 , num_samples ) data = np . vstack (( x , y )) . T d_dimensions = data . shape [ 1 ] plot_2d_joint ( data ) <Figure size 864x360 with 0 Axes> Step I - Marginal Gaussianization \u00b6 In this tutorial, for simplicity, I will use the quantile transformer found in the sklearn library. This transformer does an estimate of the CDF for each feature independently. Then the values are mapped to the Guassian distribution from the learned CDF function. n_quantiles = 1000 output_distribution = 'normal' random_state = 123 subsample = 2000 # Quantile Transformer mg_transformer = QuantileTransformer ( n_quantiles = n_quantiles , output_distribution = output_distribution , subsample = subsample ) data_mg = mg_transformer . fit_transform ( data ) plot_2d_joint ( data_mg ) <Figure size 864x360 with 0 Axes> Step II - Rotation (PCA) \u00b6 pca_model = PCA () data_rot = pca_model . fit_transform ( data_mg ) plot_2d_joint ( data_rot ) <Figure size 864x360 with 0 Axes>","title":"Rbig walkthrough"},{"location":"notebooks/rbig_walkthrough/#rbig-walk-through-naive","text":"This is a quick tutorial to show how the RBIG algorithm itself can be implemented very simply using standard scikit-learn tools. It consists of the following two steps 1) marginal Gaussianization and 2) rotation. import numpy as np import warnings from sklearn.preprocessing import QuantileTransformer from sklearn.decomposition import PCA from scipy.stats import rv_histogram , norm import pandas as pd import seaborn as sns import sys sys . path . insert ( 0 , '/Users/eman/Documents/code_projects/rbig/' ) from rbig import RBIG import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % matplotlib inline % load_ext autoreload % autoreload 2 # Helper Plot Function def plot_2d_joint ( data , savename = None ): fig = plt . figure ( figsize = ( 12 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'scatter' , color = 'blue' , alpha = 0.1 ) g . ax_joint . set_xticks ([]) g . ax_joint . set_yticks ([]) plt . tight_layout () if savename : g . savefig ( f \" { savename } /rbig_0_data.png\" , transparent = True ) plt . show () return None","title":"RBIG Walk-Through (Naive)"},{"location":"notebooks/rbig_walkthrough/#data","text":"seed = 123 rng = np . random . RandomState ( seed = seed ) num_samples = 10000 x = np . abs ( 2 * rng . randn ( 1 , num_samples )) y = np . sin ( x ) + 0.25 * rng . randn ( 1 , num_samples ) data = np . vstack (( x , y )) . T d_dimensions = data . shape [ 1 ] plot_2d_joint ( data ) <Figure size 864x360 with 0 Axes>","title":"Data"},{"location":"notebooks/rbig_walkthrough/#step-i-marginal-gaussianization","text":"In this tutorial, for simplicity, I will use the quantile transformer found in the sklearn library. This transformer does an estimate of the CDF for each feature independently. Then the values are mapped to the Guassian distribution from the learned CDF function. n_quantiles = 1000 output_distribution = 'normal' random_state = 123 subsample = 2000 # Quantile Transformer mg_transformer = QuantileTransformer ( n_quantiles = n_quantiles , output_distribution = output_distribution , subsample = subsample ) data_mg = mg_transformer . fit_transform ( data ) plot_2d_joint ( data_mg ) <Figure size 864x360 with 0 Axes>","title":"Step I - Marginal Gaussianization"},{"location":"notebooks/rbig_walkthrough/#step-ii-rotation-pca","text":"pca_model = PCA () data_rot = pca_model . fit_transform ( data_mg ) plot_2d_joint ( data_rot ) <Figure size 864x360 with 0 Axes>","title":"Step II - Rotation (PCA)"},{"location":"notebooks/test/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); RBIG Demo \u00b6 % matplotlib inline import sys sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/rbig/src' ) sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) # sys.path.insert(0, '/home/emmanuel/Drives/megatron/temp/2017_RBIG/') # sys.path.insert(0, '/Users/eman/Documents/code_projects/rbig/') import numpy as np # import seaborn as sns import pandas as pd import warnings from time import time from rbig.rbig import RBIG from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state from scipy import io import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % load_ext autoreload % autoreload 2 Toy Data \u00b6 seed = 123 rng = np . random . RandomState ( seed = seed ) aux2 = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/DATA_uniform_dim_10_seed_2.mat' ) seed = 123 rng = np . random . RandomState ( seed = seed ) aux2 = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/DATA_uniform_dim_10_seed_2.mat' ) # aux2 = io.loadmat('/home/emmanuel/Drives/megatron/temp/2017_RBIG/DATA_uniform_dim_10_seed_2.mat') data = aux2 [ 'dat' ] . T data_original = aux2 [ 'aux' ] . T R = aux2 [ 'R' ] . T # num_samples = 10000 # x = np.abs(2 * rng.randn(1, num_samples)) # y = np.sin(x) + 0.25 * rng.randn(1, num_samples) # data = np.vstack((x, y)).T fig , ax = plt . subplots () ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Equal' ) plt . show () data_aux = np . dot ( data_original , R ) RBIG Fitting \u00b6 %% time n_layers = 5 rotation_type = 'PCA' random_state = 123 pdf_extension = 0.1 pdf_resolution = 1000 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , pdf_extension = 0.1 , pdf_resolution = 100 ) # fit model to the data rbig_model . fit ( data ); CPU times: user 2min 38s, sys: 1.85 s, total: 2min 40s Wall time: 17.2 s ndet = 1000 jacobian = rbig_model . jacobian ( data [: ndet , :]) print ( jacobian . shape ) dd = np . zeros ( ndet ) for i in range ( ndet ): aux = jacobian [ i , ... ] . squeeze () dd [ i ] = np . abs ( np . linalg . det ( aux )) fig , ax = plt . subplots () ax . plot ( np . log10 ( dd )) plt . show () fig_loc = '/home/emmanuel/projects/2019_rbig_info/reports/figures/rbig/' save_name = 'test_rbig_py.png' fig . savefig ( fig_loc + save_name ) (1000, 10, 10) Checking Versus MATLAB Results \u00b6 # load data matlab_results = io . loadmat ( '/Users/eman/Documents/MATLAB/rbig_2018/test_results_matlab.mat' )[ 'dd' ] . squeeze () py_results = dd --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in _open_file (file_like, appendmat) 30 try : ---> 31 return open ( file_like , 'rb' ) , True 32 except IOError : FileNotFoundError : [Errno 2] No such file or directory: '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' During handling of the above exception, another exception occurred: FileNotFoundError Traceback (most recent call last) <ipython-input-11-53a75f37242e> in <module> 1 # load data ----> 2 matlab_results = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' ) [ 'dd' ] . squeeze ( ) 3 py_results = dd ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in loadmat (file_name, mdict, appendmat, **kwargs) 205 \"\"\" 206 variable_names = kwargs . pop ( 'variable_names' , None ) --> 207 MR , file_opened = mat_reader_factory ( file_name , appendmat , ** kwargs ) 208 matfile_dict = MR . get_variables ( variable_names ) 209 if mdict is not None : ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in mat_reader_factory (file_name, appendmat, **kwargs) 60 61 \"\"\" ---> 62 byte_stream , file_opened = _open_file ( file_name , appendmat ) 63 mjv , mnv = get_matfile_version ( byte_stream ) 64 if mjv == 0 : ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in _open_file (file_like, appendmat) 35 if appendmat and not file_like . endswith ( '.mat' ) : 36 file_like += '.mat' ---> 37 return open ( file_like , 'rb' ) , True 38 else : 39 raise IOError ( 'Reader needs file name or open file-like object' ) FileNotFoundError : [Errno 2] No such file or directory: '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' x_min = np . minimum ( matlab_results . min (), py_results . min ()) x_max = np . maximum ( matlab_results . max (), py_results . max ()) print ( py_results . shape , matlab_results . shape ) fig , ax = plt . subplots () ax . scatter ( py_results , matlab_results ) ax . set_yscale ( 'log' ) ax . set_xscale ( 'log' ) ax . set_title ( 'Comparing RBIG Algorithms Results' ) ax . set_xlabel ( 'Python' ) ax . set_ylabel ( 'MATLAB' ) plt . show () fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py.png' fig . savefig ( fig_loc + save_name ) fig , ax = plt . subplots () ax . scatter ( py_results , matlab_results ) ax . set_yscale ( 'log' ) ax . set_xscale ( 'log' ) ax . set_xlim ([ x_min , 10 ** 4 ]) ax . set_ylim ([ x_min , 10 ** 4 ]) ax . set_title ( 'Comparing RBIG Algorithms Results (Clean)' ) ax . set_xlabel ( 'Python' ) ax . set_ylabel ( 'MATLAB' ) plt . show () fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py_clean.png' fig . savefig ( fig_loc + save_name ) (1000,) (1000,) data = pd . DataFrame ({ 'x' : matlab_results , 'y' : py_results }) sns_plot = sns . jointplot ( x = \"x\" , y = \"y\" , data = np . log10 ( data ), kind = \"kde\" ) fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py_jointplot.png' sns_plot . savefig ( fig_loc + save_name ) # ax = g.ax_joint # ax.set_xscale('log') # ax.set_yscale('log') jacobian , data_transform2 = rbig_model . jacobian ( data [: 1000 , :]) ndet = 1000 # dd = np.zeros(jacobian.shape[1]) for i in range ( jacobian . shape [ 1 ]): dd [ i ] = np . linalg . det ( jacobian [ i , :, :]) # fig, ax = plt.subplots() # ax.plot(dd) # plt.show() Transform Data into Gaussian \u00b6 # transform data data_trans = rbig_model . transform ( data ) fig , ax = plt . subplots () ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Dara after RBIG Transformation' ) plt . show () BUCLE \u00b6 %% time n_layers = 1000 rotation_type = 'PCA' random_state = 123 pdf_extension = 0.1 pdf_resolution = 1000 n_samples = 10000 R = np . array ([[ 10 , 0.5 , 1 , 7 ], [ 50 , - 3 , 5 , - 5 ], [ 2 , - 3 , 5 , 4 ], [ - 2 , - 3 , 5 , 4 ]]) MIS = np . zeros ( 100 ) for i in range ( 100 ): aux = np . random . rand ( n_samples , 4 ) dat = np . dot ( aux , R ) rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , pdf_extension = pdf_extension , pdf_resolution = pdf_resolution ) g_data = rbig_model . fit ( dat ) . transform ( dat ) di = rbig_model . residual_info MIS [ i ] = sum ( di ) print ( i ) # # Initialize RBIG class # rbig_model = RBIG(n_layers=n_layers, rotation_type=rotation_type, random_state=random_state, # pdf_extension=0.1, pdf_resolution=100) # # fit model to the data # rbig_model.fit(data); 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <timed exec> in <module> () ~/code/py_packages/rbig/src/rbig.py in transform (self, data) 177 # marginal gaussianization 178 data_layer[idim, :] = norm.ppf( --> 179 data_layer [ idim , : ] 180 ) 181 ~/anaconda3/envs/sci_py36/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py in ppf (self, q, *args, **kwds) 1904 cond1 = ( 0 < q ) & ( q < 1 ) 1905 cond2 = cond0 & ( q == 0 ) -> 1906 cond3 = cond0 & ( q == 1 ) 1907 cond = cond0 & cond1 1908 output = valarray ( shape ( cond ) , value = self . badvalue ) KeyboardInterrupt : fig , ax = plt . subplots () ax . plot ( di ) ax . show () --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-24-3df48be58a99> in <module> () 2 3 ax . plot ( di ) ----> 4 ax . show ( ) AttributeError : 'AxesSubplot' object has no attribute 'show' fig , ax = plt . subplots () ax . plot ( di ) ax . show ( MIS ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-25-1a669b5b5aa5> in <module> () 2 3 ax . plot ( di ) ----> 4 ax . show ( MIS ) AttributeError : 'AxesSubplot' object has no attribute 'show' print ( MIS . mean (), MIS . std ())","title":"Test"},{"location":"notebooks/test/#rbig-demo","text":"% matplotlib inline import sys sys . path . insert ( 0 , '/home/emmanuel/code/py_packages/rbig/src' ) sys . path . insert ( 0 , '/home/emmanuel/code/rbig/' ) # sys.path.insert(0, '/home/emmanuel/Drives/megatron/temp/2017_RBIG/') # sys.path.insert(0, '/Users/eman/Documents/code_projects/rbig/') import numpy as np # import seaborn as sns import pandas as pd import warnings from time import time from rbig.rbig import RBIG from sklearn.model_selection import train_test_split from sklearn.utils import check_random_state from scipy import io import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) warnings . filterwarnings ( 'ignore' ) # get rid of annoying warnings % load_ext autoreload % autoreload 2","title":"RBIG Demo"},{"location":"notebooks/test/#toy-data","text":"seed = 123 rng = np . random . RandomState ( seed = seed ) aux2 = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/DATA_uniform_dim_10_seed_2.mat' ) seed = 123 rng = np . random . RandomState ( seed = seed ) aux2 = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/DATA_uniform_dim_10_seed_2.mat' ) # aux2 = io.loadmat('/home/emmanuel/Drives/megatron/temp/2017_RBIG/DATA_uniform_dim_10_seed_2.mat') data = aux2 [ 'dat' ] . T data_original = aux2 [ 'aux' ] . T R = aux2 [ 'R' ] . T # num_samples = 10000 # x = np.abs(2 * rng.randn(1, num_samples)) # y = np.sin(x) + 0.25 * rng.randn(1, num_samples) # data = np.vstack((x, y)).T fig , ax = plt . subplots () ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Equal' ) plt . show () data_aux = np . dot ( data_original , R )","title":"Toy Data"},{"location":"notebooks/test/#rbig-fitting","text":"%% time n_layers = 5 rotation_type = 'PCA' random_state = 123 pdf_extension = 0.1 pdf_resolution = 1000 # Initialize RBIG class rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , pdf_extension = 0.1 , pdf_resolution = 100 ) # fit model to the data rbig_model . fit ( data ); CPU times: user 2min 38s, sys: 1.85 s, total: 2min 40s Wall time: 17.2 s ndet = 1000 jacobian = rbig_model . jacobian ( data [: ndet , :]) print ( jacobian . shape ) dd = np . zeros ( ndet ) for i in range ( ndet ): aux = jacobian [ i , ... ] . squeeze () dd [ i ] = np . abs ( np . linalg . det ( aux )) fig , ax = plt . subplots () ax . plot ( np . log10 ( dd )) plt . show () fig_loc = '/home/emmanuel/projects/2019_rbig_info/reports/figures/rbig/' save_name = 'test_rbig_py.png' fig . savefig ( fig_loc + save_name ) (1000, 10, 10)","title":"RBIG Fitting"},{"location":"notebooks/test/#checking-versus-matlab-results","text":"# load data matlab_results = io . loadmat ( '/Users/eman/Documents/MATLAB/rbig_2018/test_results_matlab.mat' )[ 'dd' ] . squeeze () py_results = dd --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in _open_file (file_like, appendmat) 30 try : ---> 31 return open ( file_like , 'rb' ) , True 32 except IOError : FileNotFoundError : [Errno 2] No such file or directory: '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' During handling of the above exception, another exception occurred: FileNotFoundError Traceback (most recent call last) <ipython-input-11-53a75f37242e> in <module> 1 # load data ----> 2 matlab_results = io . loadmat ( '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' ) [ 'dd' ] . squeeze ( ) 3 py_results = dd ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in loadmat (file_name, mdict, appendmat, **kwargs) 205 \"\"\" 206 variable_names = kwargs . pop ( 'variable_names' , None ) --> 207 MR , file_opened = mat_reader_factory ( file_name , appendmat , ** kwargs ) 208 matfile_dict = MR . get_variables ( variable_names ) 209 if mdict is not None : ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in mat_reader_factory (file_name, appendmat, **kwargs) 60 61 \"\"\" ---> 62 byte_stream , file_opened = _open_file ( file_name , appendmat ) 63 mjv , mnv = get_matfile_version ( byte_stream ) 64 if mjv == 0 : ~/.conda/envs/rbig_info/lib/python3.7/site-packages/scipy/io/matlab/mio.py in _open_file (file_like, appendmat) 35 if appendmat and not file_like . endswith ( '.mat' ) : 36 file_like += '.mat' ---> 37 return open ( file_like , 'rb' ) , True 38 else : 39 raise IOError ( 'Reader needs file name or open file-like object' ) FileNotFoundError : [Errno 2] No such file or directory: '/media/disk/erc/papers/2018_RBIG_IT_measures/2018_RBIG_IT_measures/2018_RBIG/DATA/test_results_matlab.mat' x_min = np . minimum ( matlab_results . min (), py_results . min ()) x_max = np . maximum ( matlab_results . max (), py_results . max ()) print ( py_results . shape , matlab_results . shape ) fig , ax = plt . subplots () ax . scatter ( py_results , matlab_results ) ax . set_yscale ( 'log' ) ax . set_xscale ( 'log' ) ax . set_title ( 'Comparing RBIG Algorithms Results' ) ax . set_xlabel ( 'Python' ) ax . set_ylabel ( 'MATLAB' ) plt . show () fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py.png' fig . savefig ( fig_loc + save_name ) fig , ax = plt . subplots () ax . scatter ( py_results , matlab_results ) ax . set_yscale ( 'log' ) ax . set_xscale ( 'log' ) ax . set_xlim ([ x_min , 10 ** 4 ]) ax . set_ylim ([ x_min , 10 ** 4 ]) ax . set_title ( 'Comparing RBIG Algorithms Results (Clean)' ) ax . set_xlabel ( 'Python' ) ax . set_ylabel ( 'MATLAB' ) plt . show () fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py_clean.png' fig . savefig ( fig_loc + save_name ) (1000,) (1000,) data = pd . DataFrame ({ 'x' : matlab_results , 'y' : py_results }) sns_plot = sns . jointplot ( x = \"x\" , y = \"y\" , data = np . log10 ( data ), kind = \"kde\" ) fig_loc = '/Users/eman/Desktop/' save_name = 'test_mat_v_py_jointplot.png' sns_plot . savefig ( fig_loc + save_name ) # ax = g.ax_joint # ax.set_xscale('log') # ax.set_yscale('log') jacobian , data_transform2 = rbig_model . jacobian ( data [: 1000 , :]) ndet = 1000 # dd = np.zeros(jacobian.shape[1]) for i in range ( jacobian . shape [ 1 ]): dd [ i ] = np . linalg . det ( jacobian [ i , :, :]) # fig, ax = plt.subplots() # ax.plot(dd) # plt.show()","title":"Checking Versus MATLAB Results"},{"location":"notebooks/test/#transform-data-into-gaussian","text":"# transform data data_trans = rbig_model . transform ( data ) fig , ax = plt . subplots () ax . scatter ( data_trans [:, 0 ], data_trans [:, 1 ], s = 1 ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) ax . set_title ( 'Dara after RBIG Transformation' ) plt . show ()","title":"Transform Data into Gaussian"},{"location":"notebooks/test/#bucle","text":"%% time n_layers = 1000 rotation_type = 'PCA' random_state = 123 pdf_extension = 0.1 pdf_resolution = 1000 n_samples = 10000 R = np . array ([[ 10 , 0.5 , 1 , 7 ], [ 50 , - 3 , 5 , - 5 ], [ 2 , - 3 , 5 , 4 ], [ - 2 , - 3 , 5 , 4 ]]) MIS = np . zeros ( 100 ) for i in range ( 100 ): aux = np . random . rand ( n_samples , 4 ) dat = np . dot ( aux , R ) rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , pdf_extension = pdf_extension , pdf_resolution = pdf_resolution ) g_data = rbig_model . fit ( dat ) . transform ( dat ) di = rbig_model . residual_info MIS [ i ] = sum ( di ) print ( i ) # # Initialize RBIG class # rbig_model = RBIG(n_layers=n_layers, rotation_type=rotation_type, random_state=random_state, # pdf_extension=0.1, pdf_resolution=100) # # fit model to the data # rbig_model.fit(data); 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <timed exec> in <module> () ~/code/py_packages/rbig/src/rbig.py in transform (self, data) 177 # marginal gaussianization 178 data_layer[idim, :] = norm.ppf( --> 179 data_layer [ idim , : ] 180 ) 181 ~/anaconda3/envs/sci_py36/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py in ppf (self, q, *args, **kwds) 1904 cond1 = ( 0 < q ) & ( q < 1 ) 1905 cond2 = cond0 & ( q == 0 ) -> 1906 cond3 = cond0 & ( q == 1 ) 1907 cond = cond0 & cond1 1908 output = valarray ( shape ( cond ) , value = self . badvalue ) KeyboardInterrupt : fig , ax = plt . subplots () ax . plot ( di ) ax . show () --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-24-3df48be58a99> in <module> () 2 3 ax . plot ( di ) ----> 4 ax . show ( ) AttributeError : 'AxesSubplot' object has no attribute 'show' fig , ax = plt . subplots () ax . plot ( di ) ax . show ( MIS ) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-25-1a669b5b5aa5> in <module> () 2 3 ax . plot ( di ) ----> 4 ax . show ( MIS ) AttributeError : 'AxesSubplot' object has no attribute 'show' print ( MIS . mean (), MIS . std ())","title":"BUCLE"}]}