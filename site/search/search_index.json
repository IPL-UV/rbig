{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rotation-Based Iterative Gaussianization \u00b6 A method that provides a transformation scheme from any distribution to a gaussian distribution. This repository will facilitate translating the original MATLAB code into a python implementation compatible with the scikit-learn framework. Resources \u00b6 Original Webpage - ISP Original MATLAB Code - webpage Original Python Code - github Paper - Iterative Gaussianization: from ICA to Random Rotations Abstract From Paper Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation.","title":"Rotation-Based Iterative Gaussianization"},{"location":"#rotation-based-iterative-gaussianization","text":"A method that provides a transformation scheme from any distribution to a gaussian distribution. This repository will facilitate translating the original MATLAB code into a python implementation compatible with the scikit-learn framework.","title":"Rotation-Based Iterative Gaussianization"},{"location":"#resources","text":"Original Webpage - ISP Original MATLAB Code - webpage Original Python Code - github Paper - Iterative Gaussianization: from ICA to Random Rotations Abstract From Paper Most signal processing problems involve the challenging task of multidimensional probability density function (PDF) estimation. In this work, we propose a solution to this problem by using a family of Rotation-based Iterative Gaussianization (RBIG) transforms. The general framework consists of the sequential application of a univariate marginal Gaussianization transform followed by an orthonormal transform. The proposed procedure looks for differentiable transforms to a known PDF so that the unknown PDF can be estimated at any point of the original domain. In particular, we aim at a zero mean unit covariance Gaussian for convenience. RBIG is formally similar to classical iterative Projection Pursuit (PP) algorithms. However, we show that, unlike in PP methods, the particular class of rotations used has no special qualitative relevance in this context, since looking for interestingness is not a critical issue for PDF estimation. The key difference is that our approach focuses on the univariate part (marginal Gaussianization) of the problem rather than on the multivariate part (rotation). This difference implies that one may select the most convenient rotation suited to each practical application. The differentiability, invertibility and convergence of RBIG are theoretically and experimentally analyzed. Relation to other methods, such as Radial Gaussianization (RG), one-class support vector domain description (SVDD), and deep neural networks (DNN) is also pointed out. The practical performance of RBIG is successfully illustrated in a number of multidimensional problems such as image synthesis, classification, denoising, and multi-information estimation.","title":"Resources"},{"location":"Notes/","text":"","title":"Index"},{"location":"Notes/Unsorted/dds/","text":"Density Destructors \u00b6 Main Idea \u00b6 Forward Approach \u00b6 We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z} z \\sim \\mathcal{P}_\\mathcal{Z} \\hat x = \\mathcal G_\\theta (z) \\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right|","title":"Density Destructors"},{"location":"Notes/Unsorted/dds/#density-destructors","text":"","title":"Density Destructors"},{"location":"Notes/Unsorted/dds/#main-idea","text":"","title":"Main Idea"},{"location":"Notes/Unsorted/dds/#forward-approach","text":"We can view the approach of modeling from two perspectives: constructive or destructive. A constructive process tries to learn how to build an exact sequence of transformations to go from z z to x x . The destructive process does the complete opposite and decides to create a sequence of transforms from x x to z z while also remembering the exact transforms; enabling it to reverse that sequence of transforms. We can write some equations to illustrate exactly what we mean by these two terms. Let's define two spaces: one is our data space \\mathcal X \\mathcal X and the other is the base space \\mathcal Z \\mathcal Z . We want to learn a transformation f_\\theta f_\\theta that maps us from \\mathcal X \\mathcal X to \\mathcal Z \\mathcal Z , f : \\mathcal X \\rightarrow \\mathcal Z f : \\mathcal X \\rightarrow \\mathcal Z . We also want a function G_\\theta G_\\theta that maps us from \\mathcal Z \\mathcal Z to \\mathcal X \\mathcal X , f : \\mathcal Z \\rightarrow \\mathcal X f : \\mathcal Z \\rightarrow \\mathcal X . TODO: Plot More concretely, let's define the following pair of equations: z \\sim \\mathcal{P}_\\mathcal{Z} z \\sim \\mathcal{P}_\\mathcal{Z} \\hat x = \\mathcal G_\\theta (z) \\hat x = \\mathcal G_\\theta (z) This is called the generative step; how well do we fit our parameters such that x \\approx \\hat x x \\approx \\hat x . We can define the alternative step below: x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) x \\sim \\mathcal{P}_\\mathcal{X}$$ $$\\hat z = \\mathcal f_\\theta (x) This is called the inference step: how well do we fit the parameters of our transformation f_\\theta f_\\theta s.t. z \\approx \\hat z z \\approx \\hat z . So there are immediately some things to notice about this. Depending on the method you use in the deep learning community, the functions \\mathcal G_\\theta \\mathcal G_\\theta and f_\\theta f_\\theta can be defined differently. Typically we are looking at the class of algorithms where we want f_\\theta = \\mathcal G_\\theta^{-1} f_\\theta = \\mathcal G_\\theta^{-1} . In this ideal scenario, we only need to learn one transformation instead of two. With this requirement, we can actually compute the likelihood values exactly. The likelihood of the value x x given the transformation \\mathcal G_\\theta \\mathcal G_\\theta is given as: \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right| \\mathcal P_{\\hat x}(x)=\\mathcal P_{z} \\left( \\mathcal G_\\theta (x) \\right)\\left| \\text{det } \\mathbf J_{\\mathcal G_\\theta} \\right|","title":"Forward Approach"},{"location":"Notes/Unsorted/exponential/","text":"Exponential Family of Distributions \u00b6 This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, R\u00e9nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. Source Parameters, \\theta \\theta \\theta = (\\mu, \\Sigma) \\theta = (\\mu, \\Sigma) where \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} and \\Sigma > 0 \\Sigma > 0 Natural Parameters, \\eta \\eta \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) Expectation Parameters Log Normalizer, F(\\eta) F(\\eta) Also known as the log partition function. F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi Gradient Log Normalizer, \\nabla F(\\eta) \\nabla F(\\eta) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) Log Normalizer, F(\\theta) F(\\theta) Also known as the log partition function. F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| Final Entropy Calculation H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle Resources \u00b6 A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper Statistical exponential families: A digest with flash cards - Paper The Exponential Family: Getting Weird Expectations! - Blog Deep Exponential Family - Code PyMEF: A Framework for Exponential Families in Python - Code | Paper","title":"Exponential Family of Distributions"},{"location":"Notes/Unsorted/exponential/#exponential-family-of-distributions","text":"This is the close-form expression for the Sharma-Mittal entropy calculation for expontial families. The Sharma-Mittal entropy is a generalization of the Shannon, R\u00e9nyi and Tsallis entropy measurements. This estimates Y using the maximum likelihood estimation and then uses the analytical formula for the exponential family. Source Parameters, \\theta \\theta \\theta = (\\mu, \\Sigma) \\theta = (\\mu, \\Sigma) where \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} and \\Sigma > 0 \\Sigma > 0 Natural Parameters, \\eta \\eta \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) \\eta = \\left( \\theta_2^{-1}\\theta_1, \\frac{1}{2}\\theta_2^{-1} \\right) Expectation Parameters Log Normalizer, F(\\eta) F(\\eta) Also known as the log partition function. F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi F(\\eta) = \\frac{1}{4} tr( \\eta_1^\\top \\eta_2^{-1} \\eta) - \\frac{1}{2} \\log|\\eta_2| + \\frac{d}{2}\\log \\pi Gradient Log Normalizer, \\nabla F(\\eta) \\nabla F(\\eta) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) \\nabla F(\\eta) = \\left( \\frac{1}{2} \\eta_2^{-1}\\eta_1, -\\frac{1}{2} \\eta_2^{-1}- \\frac{1}{4}(\\eta_2^{-1}-\\eta_1)(\\eta_2^{-1}-\\eta_1)^\\top \\right) Log Normalizer, F(\\theta) F(\\theta) Also known as the log partition function. F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| F(\\theta) = \\frac{1}{2} \\theta_1^\\top \\theta_2^{-1} \\theta + \\frac{1}{2} \\log|\\theta_2| Final Entropy Calculation H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle H = F(\\eta) - \\langle \\eta, \\nabla F(\\eta) \\rangle","title":"Exponential Family of Distributions"},{"location":"Notes/Unsorted/exponential/#resources","text":"A closed-form expression for the Sharma-Mittal entropy of exponential families - Nielsen & Nock (2012) - Paper Statistical exponential families: A digest with flash cards - Paper The Exponential Family: Getting Weird Expectations! - Blog Deep Exponential Family - Code PyMEF: A Framework for Exponential Families in Python - Code | Paper","title":"Resources"},{"location":"Notes/Unsorted/gaussian/","text":"Gaussian Distribution \u00b6 PDF \u00b6 f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) Likelihood \u00b6 - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi Alternative Representation \u00b6 X \\sim \\mathcal{N}(\\mu, \\Sigma) X \\sim \\mathcal{N}(\\mu, \\Sigma) where \\mu \\mu is the mean function and \\Sigma \\Sigma is the covariance. Let's decompose \\Sigma \\Sigma as with an eigendecomposition like so \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top Now we can represent our Normal distribution as: X \\sim \\mu + U\\Lambda^{1/2}Z X \\sim \\mu + U\\Lambda^{1/2}Z where: U U is a rotation matrix \\Lambda^{-1/2} \\Lambda^{-1/2} is a scale matrix \\mu \\mu is a translation matrix Z \\sim \\mathcal{N}(0,I) Z \\sim \\mathcal{N}(0,I) or also X \\sim \\mu + UZ X \\sim \\mu + UZ where: U U is a rotation matrix \\Lambda \\Lambda is a scale matrix \\mu \\mu is a translation matrix Z_n \\sim \\mathcal{N}(0,\\Lambda) Z_n \\sim \\mathcal{N}(0,\\Lambda) Reparameterization \u00b6 So often in deep learning we will learn this distribution by a reparameterization like so: X = \\mu + AZ X = \\mu + AZ where: \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} A \\in \\mathbb{R}^{d\\times l} A \\in \\mathbb{R}^{d\\times l} Z_n \\sim \\mathcal{N}(0, I) Z_n \\sim \\mathcal{N}(0, I) \\Sigma=AA^\\top \\Sigma=AA^\\top - the cholesky decomposition Entropy \u00b6 1 dimensional H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) D dimensional H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| <span><span class=\"MathJax_Preview\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|</span><script type=\"math/tex\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| KL-Divergence (Relative Entropy) \u00b6 KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] if \\mu_1=\\mu_0 \\mu_1=\\mu_0 then: KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] Mutual Information I(X)= - \\frac{1}{2} \\ln | \\rho_0 | I(X)= - \\frac{1}{2} \\ln | \\rho_0 | where \\rho_0 \\rho_0 is the correlation matrix from \\Sigma_0 \\Sigma_0 . I(X) I(X)","title":"Gaussian Distribution"},{"location":"Notes/Unsorted/gaussian/#gaussian-distribution","text":"","title":"Gaussian Distribution"},{"location":"Notes/Unsorted/gaussian/#pdf","text":"f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) f(X)= \\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} \\text{exp}\\left( -\\frac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right)","title":"PDF"},{"location":"Notes/Unsorted/gaussian/#likelihood","text":"- \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi - \\ln L = \\frac{1}{2}\\ln|\\Sigma| + \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x - \\mu) + \\frac{D}{2}\\ln 2\\pi","title":"Likelihood"},{"location":"Notes/Unsorted/gaussian/#alternative-representation","text":"X \\sim \\mathcal{N}(\\mu, \\Sigma) X \\sim \\mathcal{N}(\\mu, \\Sigma) where \\mu \\mu is the mean function and \\Sigma \\Sigma is the covariance. Let's decompose \\Sigma \\Sigma as with an eigendecomposition like so \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top \\Sigma = U\\Lambda U^\\top = U \\Lambda^{1/2}(U\\Lambda^{-1/2})^\\top Now we can represent our Normal distribution as: X \\sim \\mu + U\\Lambda^{1/2}Z X \\sim \\mu + U\\Lambda^{1/2}Z where: U U is a rotation matrix \\Lambda^{-1/2} \\Lambda^{-1/2} is a scale matrix \\mu \\mu is a translation matrix Z \\sim \\mathcal{N}(0,I) Z \\sim \\mathcal{N}(0,I) or also X \\sim \\mu + UZ X \\sim \\mu + UZ where: U U is a rotation matrix \\Lambda \\Lambda is a scale matrix \\mu \\mu is a translation matrix Z_n \\sim \\mathcal{N}(0,\\Lambda) Z_n \\sim \\mathcal{N}(0,\\Lambda)","title":"Alternative Representation"},{"location":"Notes/Unsorted/gaussian/#reparameterization","text":"So often in deep learning we will learn this distribution by a reparameterization like so: X = \\mu + AZ X = \\mu + AZ where: \\mu \\in \\mathbb{R}^{d} \\mu \\in \\mathbb{R}^{d} A \\in \\mathbb{R}^{d\\times l} A \\in \\mathbb{R}^{d\\times l} Z_n \\sim \\mathcal{N}(0, I) Z_n \\sim \\mathcal{N}(0, I) \\Sigma=AA^\\top \\Sigma=AA^\\top - the cholesky decomposition","title":"Reparameterization"},{"location":"Notes/Unsorted/gaussian/#entropy","text":"1 dimensional H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) H(X) = \\frac{1}{2} \\log(2\\pi e \\sigma^2) D dimensional H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma| <span><span class=\"MathJax_Preview\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|</span><script type=\"math/tex\">H(X) = \\frac{D}{2} + \\frac{D}{2} \\ln(2\\pi) + \\frac{1}{2}\\ln|\\Sigma|","title":"Entropy"},{"location":"Notes/Unsorted/gaussian/#kl-divergence-relative-entropy","text":"KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] KLD(\\mathcal{N}_0||\\mathcal{N}_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1}\\Sigma_0) + (\\mu_1 - \\mu_0)^\\top \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - D + \\ln \\frac{|\\Sigma_1|}{\\Sigma_0|} \\right] if \\mu_1=\\mu_0 \\mu_1=\\mu_0 then: KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] KLD(\\Sigma_0||\\Sigma_1) = \\frac{1}{2} \\left[ \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) - D + \\ln \\frac{|\\Sigma_1|}{|\\Sigma_0|} \\right] Mutual Information I(X)= - \\frac{1}{2} \\ln | \\rho_0 | I(X)= - \\frac{1}{2} \\ln | \\rho_0 | where \\rho_0 \\rho_0 is the correlation matrix from \\Sigma_0 \\Sigma_0 . I(X) I(X)","title":"KL-Divergence (Relative Entropy)"},{"location":"Notes/Unsorted/gaussianization/","text":"Gaussianization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Notebooks: 1D Gaussianization Why Gaussianization? Main Idea Loss Function Negentropy Methods Projection Pursuit Gaussianization RBIG References Why Gaussianization? \u00b6 Gaussianization : Transforms multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics Main Idea \u00b6 The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta . Loss Function \u00b6 as shown in the equation from the original paper . Negentropy \u00b6 Methods \u00b6 Projection Pursuit \u00b6 Gaussianization \u00b6 RBIG \u00b6 References \u00b6","title":"Gaussianization"},{"location":"Notes/Unsorted/gaussianization/#gaussianization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Notebooks: 1D Gaussianization Why Gaussianization? Main Idea Loss Function Negentropy Methods Projection Pursuit Gaussianization RBIG References","title":"Gaussianization"},{"location":"Notes/Unsorted/gaussianization/#why-gaussianization","text":"Gaussianization : Transforms multidimensional data into multivariate Gaussian data. It is notorious that we say \"assume our data is Gaussian\". We do this all of the time in practice. It's because Gaussian data typically has nice properties, e.g. close-form solutions, dependence, etc( ??? ). But as sensors get better, data gets bigger and algorithms get better, this assumption does not always hold. However, what if we could make our data Gaussian? If it were possible, then all of the nice properties of Gaussians can be used as our data is actually Gaussian. How is this possible? Well, we use a series of invertible transformations to transform our data \\mathcal X \\mathcal X to the Gaussian domain \\mathcal Z \\mathcal Z . The logic is that by independently transforming each dimension of the data followed by some rotation will eventually converge to a multivariate dataset that is completely Gaussian. We can achieve statistical independence of data components. This is useful for the following reasons: We can process dimensions independently We can alleviate the curse of dimensionality We can tackle the PDF estimation problem directly With PDF estimation, we can sample and assign probabilities. It really is the hole grail of ML models. We can apply and design methods that assume Gaussianity of the data Get insight into the data characteristics","title":"Why Gaussianization?"},{"location":"Notes/Unsorted/gaussianization/#main-idea","text":"The idea of the Gaussianization frameworks is to transform some data distribution \\mathcal{D} \\mathcal{D} to an approximate Gaussian distribution \\mathcal{N} \\mathcal{N} . Let x x be some data from our original distribution, x\\sim \\mathcal{D} x\\sim \\mathcal{D} and \\mathcal{G}_{\\theta}(\\cdot) \\mathcal{G}_{\\theta}(\\cdot) be the transformation to the Normal distribution \\mathcal{N}(0, \\mathbf{I}) \\mathcal{N}(0, \\mathbf{I}) . z=\\mathcal{G}_{\\theta}(x) z=\\mathcal{G}_{\\theta}(x) <span><span class=\"MathJax_Preview\">z=\\mathcal{G}_{\\theta}(x)</span><script type=\"math/tex\">z=\\mathcal{G}_{\\theta}(x) where: * x\\sim x\\sim Data Distribtuion * \\theta \\theta - Parameters of transformation * \\mathcal{G} \\mathcal{G} - family of transformations from Data Distribution to Normal Distribution, \\mathcal{N} \\mathcal{N} . * z\\sim\\mathcal{N}(0, \\mathbf{I}) z\\sim\\mathcal{N}(0, \\mathbf{I}) If the transformation is differentiable, we have a clear relationship between the input and output variables by means of the change of variables transformation : \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} \\begin{aligned} p_\\mathbf{x}(\\mathbf{x}) &= p_\\mathbf{y} \\left[ \\mathcal{G}_\\theta(\\mathbf{x}) \\right] \\left| \\nabla_\\mathbf{x} \\mathcal{G}_\\theta(\\mathbf{x}) \\right| \\end{aligned} where: \\left| \\cdot \\right| \\left| \\cdot \\right| - absolute value of the matrix determinant P_z \\sim \\mathcal{N}(0, \\mathbf{I}) P_z \\sim \\mathcal{N}(0, \\mathbf{I}) \\mathcal{P}_x \\mathcal{P}_x - determined solely by the transformation of variables. We can say that \\mathcal{G}_{\\theta} \\mathcal{G}_{\\theta} provides an implicit density model on x x given the parameters \\theta \\theta .","title":"Main Idea"},{"location":"Notes/Unsorted/gaussianization/#loss-function","text":"as shown in the equation from the original paper .","title":"Loss Function"},{"location":"Notes/Unsorted/gaussianization/#negentropy","text":"","title":"Negentropy"},{"location":"Notes/Unsorted/gaussianization/#methods","text":"","title":"Methods"},{"location":"Notes/Unsorted/gaussianization/#projection-pursuit","text":"","title":"Projection Pursuit"},{"location":"Notes/Unsorted/gaussianization/#gaussianization_1","text":"","title":"Gaussianization"},{"location":"Notes/Unsorted/gaussianization/#rbig","text":"","title":"RBIG"},{"location":"Notes/Unsorted/gaussianization/#references","text":"","title":"References"},{"location":"Notes/Unsorted/itm/","text":"Information Theory Measures \u00b6 Summary Information Entropy Mutual Information Total Correlation (Mutual Information) Kullback-Leibler Divergence (KLD) Summary \u00b6 Caption : Information Theory measures in a nutshell. Information \u00b6 Entropy \u00b6 Mutual Information \u00b6 Total Correlation (Mutual Information) \u00b6 This is a term that measures the statistical dependency of multi-variate sources using the common mutual-information measure. \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} where H(\\mathbf{x}) H(\\mathbf{x}) is the differential entropy of \\mathbf{x} \\mathbf{x} and H(x_d) H(x_d) represents the differential entropy of the d^\\text{th} d^\\text{th} component of \\mathbf{x} \\mathbf{x} . This is nicely summaries in equation 1 from ( Lyu & Simoncelli, 2008 ). ?> Note: We find that I I in 2 dimensions is the same as mutual information. We can decompose this measure into two parts representing second order and higher-order dependencies: \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} again, nicely summarized with equation 2 from ( Lyu & Simoncelli, 2008 ). Sources : * Nonlinear Extraction of \"Independent Components\" of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - PDF Kullback-Leibler Divergence (KLD) \u00b6","title":"Information Theory Measures"},{"location":"Notes/Unsorted/itm/#information-theory-measures","text":"Summary Information Entropy Mutual Information Total Correlation (Mutual Information) Kullback-Leibler Divergence (KLD)","title":"Information Theory Measures"},{"location":"Notes/Unsorted/itm/#summary","text":"Caption : Information Theory measures in a nutshell.","title":"Summary"},{"location":"Notes/Unsorted/itm/#information","text":"","title":"Information"},{"location":"Notes/Unsorted/itm/#entropy","text":"","title":"Entropy"},{"location":"Notes/Unsorted/itm/#mutual-information","text":"","title":"Mutual Information"},{"location":"Notes/Unsorted/itm/#total-correlation-mutual-information","text":"This is a term that measures the statistical dependency of multi-variate sources using the common mutual-information measure. \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= D_\\text{KL} \\left[ p(\\mathbf{x}) || \\prod_d p(\\mathbf{x}_d) \\right] \\\\ &= \\sum_{d=1}^D H(x_d) - H(\\mathbf{x}) \\end{aligned} where H(\\mathbf{x}) H(\\mathbf{x}) is the differential entropy of \\mathbf{x} \\mathbf{x} and H(x_d) H(x_d) represents the differential entropy of the d^\\text{th} d^\\text{th} component of \\mathbf{x} \\mathbf{x} . This is nicely summaries in equation 1 from ( Lyu & Simoncelli, 2008 ). ?> Note: We find that I I in 2 dimensions is the same as mutual information. We can decompose this measure into two parts representing second order and higher-order dependencies: \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} \\begin{aligned} I(\\mathbf{x}) &= \\underbrace{\\sum_{d=1}^D \\log{\\Sigma_{dd}} - \\log{|\\Sigma|}}_{\\text{2nd Order Dependencies}} \\\\ &- \\underbrace{D_\\text{KL} \\left[ p(\\mathbf{x}) || \\mathcal{G}_\\theta (\\mathbf{x}) \\right] - \\sum_{d=1}^D D_\\text{KL} \\left[ p(x_d) || \\mathcal{G}_\\theta (x_d) \\right]}_{\\text{high-order dependencies}} \\end{aligned} again, nicely summarized with equation 2 from ( Lyu & Simoncelli, 2008 ). Sources : * Nonlinear Extraction of \"Independent Components\" of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - PDF","title":"Total Correlation (Mutual Information)"},{"location":"Notes/Unsorted/itm/#kullback-leibler-divergence-kld","text":"","title":"Kullback-Leibler Divergence (KLD)"},{"location":"Notes/Unsorted/kde/","text":"Kernel Density Estimation \u00b6 Resources \u00b6 Built-In \u00b6 Jake Vanderplas - In Depth: Kernel Density Estimation","title":"Kernel Density Estimation"},{"location":"Notes/Unsorted/kde/#kernel-density-estimation","text":"","title":"Kernel Density Estimation"},{"location":"Notes/Unsorted/kde/#resources","text":"","title":"Resources"},{"location":"Notes/Unsorted/kde/#built-in","text":"Jake Vanderplas - In Depth: Kernel Density Estimation","title":"Built-In"},{"location":"Notes/Unsorted/literature/","text":"Literature Review \u00b6 Theory Gaussianization Journal Articles RBIG Generalized Divisive Normalization Theory \u00b6 Gaussianization \u00b6 The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress Journal Articles \u00b6 Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks RBIG \u00b6 The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications Generalized Divisive Normalization \u00b6 This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Literature Review"},{"location":"Notes/Unsorted/literature/#literature-review","text":"Theory Gaussianization Journal Articles RBIG Generalized Divisive Normalization","title":"Literature Review"},{"location":"Notes/Unsorted/literature/#theory","text":"","title":"Theory"},{"location":"Notes/Unsorted/literature/#gaussianization","text":"The original Gaussianization algorithms. Gaussianization - Chen & Gopinath - (2000) - PDF Nonlinear Extraction of 'Independent Components' of elliptically symmetric densities using radial Gaussianization - Lyu & Simoncelli - Technical Report (2008) - PDF Applications Gaussianization for fast and accurate inference from cosmological data - Schuhman et. al. - (2016) - PDF Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress","title":"Gaussianization"},{"location":"Notes/Unsorted/literature/#journal-articles","text":"Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks","title":"Journal Articles"},{"location":"Notes/Unsorted/literature/#rbig","text":"The most updated Gaussianization algorithm which generalizes the original algorithms. It is more computationally efficient and still very simple to implement. Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. - IEEE TNNs - Paper Applications","title":"RBIG"},{"location":"Notes/Unsorted/literature/#generalized-divisive-normalization","text":"This can be thought of as an approximation to the Gaussianization Density Modeling of Images Using a Generalized Normalized Transformation - Balle et al. - ICLR (2016) - arxiv | Lecture | PyTorch | TensorFlow End-to-end Optimized Image Compression - Balle et. al. - CVPR (2017) - axriv Learning Divisive Normalization in Primary Visual Cortex - Gunthner et. al. - bioarxiv","title":"Generalized Divisive Normalization"},{"location":"Notes/Unsorted/mg/","text":"Marginal Gaussianization \u00b6 A dimension-wise transform, whose Jacobian is a diagonal matrix. Author: J. Emmanuel Johnson Website: jejjohnson.netlify.com Email: jemanjohnson34@gmail.com Notebooks: Marginal Uniformization Inverse Gaussian CDF Idea High-Level Instructions Mathematical Details Data Marginal Uniformization Histogram Estimation Gaussianization of Uniform Variable Log Determinant Jacobian Log-Likelihood of the Data Quantile Transform KDE Transform Spline Functions Gaussian Transform Idea \u00b6 The idea is to transform each dimension/feature into a Gaussian distribution, i.e. Marginal Gaussianization. We will convert each of the marginal distributions to a Gaussian distribution of mean 0 and variance 1. You can follow along in this colab notebook for a high-level demonstration. High-Level Instructions \u00b6 Estimate the cumulative distribution function for each feature independently. Obtain the CDF and ICDF Mapped to desired output distribution. Demo: TODO * Marginal PDF * x_d x_d vs p(x_d) p(x_d) * Uniform Transformation * x_d x_d vs u=U(x_d) u=U(x_d) * PDF of the uniformized variable * u u vs p(u) p(u) * Gaussianization transform * u u vs G(u) G(u) * PDF of the Gaussianized variable * G(u)=\\Psi(x_d) G(u)=\\Psi(x_d) vs p_d(\\Psi(x_d)) p_d(\\Psi(x_d)) Mathematical Details \u00b6 For all instructions in the following, we will assume we are looking at a univariate distribution to make the concepts and notation easier. Overall, we can essentially break these pieces up into two steps: 1) we make the marginal distribution uniform and 2) we make the marginal distribution Gaussian. Data \u00b6 In this example, let's assume x x comes from a univariate distribution. To make it interesting, we will be using the \\Gamma \\Gamma PDF: f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} where x \\leq 0, a > 0 x \\leq 0, a > 0 where \\Gamma(a) \\Gamma(a) is the gamma function with the parameter a a . Fig I : Input Distribution. This distribution is very skewed so through-out this tutorial, we will transform this distribution to a normal distribution. Marginal Uniformization \u00b6 The first step, we map x_d x_d to the uniform domain U_d U_d . This is based on the cumulative distribution of the PDF. u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' Histogram Estimation \u00b6 Below we use the np.percentile function which essentially calculates q-th percentile for an element in an array. # number of quantiles n_quantiles = 100 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # check to ensure the quantiles make sense # calculate reference values references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate kth percentile of the data along the axis quantiles = np . percentile ( X_samples , references * 100 ) Fig 2 : CDF. Extending the Support We need to extend the support of the distribution because it may be the case that we have data that lies outside of the distribution. In this case, we want to be able to map those datapoints with the CDF function as well. This is a very simple operation because we need to just squash the CDF function such that we have more values between the end points of the support and the original data distribution. Below, we showcase an example where we extend the CDF function near the tails. Fig 3 : CDF with extended support. We used approximately 1% extra on either tail. Looking at figure 3, we see that the new function has the same support but the tail is extended near the higher values. This corresponds to the region near the right side of the equation in figure 1. Gaussianization of Uniform Variable \u00b6 In this section, we need to perform some Gaussianization of the uniform variable that we have transformed in the above section. G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' Log Determinant Jacobian \u00b6 \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} Taking the \\log \\log of this function \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} This is simply the log Jacobian of the function \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} Log-Likelihood of the Data \u00b6 \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) Quantile Transform \u00b6 Calculate the empirical ranks numpy.percentile Modify ranking through interpolation, numpy.interp Map to normal distribution by inverting CDF, scipy.stats.norm.ppf Sources : * PyTorch Percentile - gist | package * Quantile Transformation with Gaussian Distribution - Sklearn Implementation - StackOverFlow * Differentiable Quantile Transformation - Miles Cranmer - PyTorch KDE Transform \u00b6 Spline Functions \u00b6 Rational Quadratic Trigonometric Interpolation Spline for Data Visualization - Lui et al - PDF TensorFlow PyTorch Implementations Neural Spline Flows - Paper Tony Duan Implementation - Paper Gaussian Transform \u00b6","title":"Marginal Gaussianization"},{"location":"Notes/Unsorted/mg/#marginal-gaussianization","text":"A dimension-wise transform, whose Jacobian is a diagonal matrix. Author: J. Emmanuel Johnson Website: jejjohnson.netlify.com Email: jemanjohnson34@gmail.com Notebooks: Marginal Uniformization Inverse Gaussian CDF Idea High-Level Instructions Mathematical Details Data Marginal Uniformization Histogram Estimation Gaussianization of Uniform Variable Log Determinant Jacobian Log-Likelihood of the Data Quantile Transform KDE Transform Spline Functions Gaussian Transform","title":"Marginal Gaussianization"},{"location":"Notes/Unsorted/mg/#idea","text":"The idea is to transform each dimension/feature into a Gaussian distribution, i.e. Marginal Gaussianization. We will convert each of the marginal distributions to a Gaussian distribution of mean 0 and variance 1. You can follow along in this colab notebook for a high-level demonstration.","title":"Idea"},{"location":"Notes/Unsorted/mg/#high-level-instructions","text":"Estimate the cumulative distribution function for each feature independently. Obtain the CDF and ICDF Mapped to desired output distribution. Demo: TODO * Marginal PDF * x_d x_d vs p(x_d) p(x_d) * Uniform Transformation * x_d x_d vs u=U(x_d) u=U(x_d) * PDF of the uniformized variable * u u vs p(u) p(u) * Gaussianization transform * u u vs G(u) G(u) * PDF of the Gaussianized variable * G(u)=\\Psi(x_d) G(u)=\\Psi(x_d) vs p_d(\\Psi(x_d)) p_d(\\Psi(x_d))","title":"High-Level Instructions"},{"location":"Notes/Unsorted/mg/#mathematical-details","text":"For all instructions in the following, we will assume we are looking at a univariate distribution to make the concepts and notation easier. Overall, we can essentially break these pieces up into two steps: 1) we make the marginal distribution uniform and 2) we make the marginal distribution Gaussian.","title":"Mathematical Details"},{"location":"Notes/Unsorted/mg/#data","text":"In this example, let's assume x x comes from a univariate distribution. To make it interesting, we will be using the \\Gamma \\Gamma PDF: f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} f(x,a) = \\frac{x^{a-1}\\exp{(-x)}}{\\Gamma(a)} where x \\leq 0, a > 0 x \\leq 0, a > 0 where \\Gamma(a) \\Gamma(a) is the gamma function with the parameter a a . Fig I : Input Distribution. This distribution is very skewed so through-out this tutorial, we will transform this distribution to a normal distribution.","title":"Data"},{"location":"Notes/Unsorted/mg/#marginal-uniformization","text":"The first step, we map x_d x_d to the uniform domain U_d U_d . This is based on the cumulative distribution of the PDF. u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d' u = U_d (x_d) = \\int_{-\\infty}^{x_d} p_d (x_d') \\, d x_d'","title":"Marginal Uniformization"},{"location":"Notes/Unsorted/mg/#histogram-estimation","text":"Below we use the np.percentile function which essentially calculates q-th percentile for an element in an array. # number of quantiles n_quantiles = 100 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # check to ensure the quantiles make sense # calculate reference values references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate kth percentile of the data along the axis quantiles = np . percentile ( X_samples , references * 100 ) Fig 2 : CDF. Extending the Support We need to extend the support of the distribution because it may be the case that we have data that lies outside of the distribution. In this case, we want to be able to map those datapoints with the CDF function as well. This is a very simple operation because we need to just squash the CDF function such that we have more values between the end points of the support and the original data distribution. Below, we showcase an example where we extend the CDF function near the tails. Fig 3 : CDF with extended support. We used approximately 1% extra on either tail. Looking at figure 3, we see that the new function has the same support but the tail is extended near the higher values. This corresponds to the region near the right side of the equation in figure 1.","title":"Histogram Estimation"},{"location":"Notes/Unsorted/mg/#gaussianization-of-uniform-variable","text":"In this section, we need to perform some Gaussianization of the uniform variable that we have transformed in the above section. G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d' G^{-1}(x_d) = \\int_{-\\infty}^{x_d} g(x_d') \\, d x_d'","title":"Gaussianization of Uniform Variable"},{"location":"Notes/Unsorted/mg/#log-determinant-jacobian","text":"\\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} \\frac{d F^{-1}}{d x} = \\frac{1}{f(F^{-1}(x))} Taking the \\log \\log of this function \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} \\log{\\frac{d F^{-1}}{d x}} = -\\log{f(F^{-1}(x))} This is simply the log Jacobian of the function \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)} \\log{\\frac{d F^{-1}}{d x}} = \\log{F^{-1}(x)}","title":"Log Determinant Jacobian"},{"location":"Notes/Unsorted/mg/#log-likelihood-of-the-data","text":"\\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^N \\log p(\\mathcal{X}|\\mathcal{N})","title":"Log-Likelihood of the Data"},{"location":"Notes/Unsorted/mg/#quantile-transform","text":"Calculate the empirical ranks numpy.percentile Modify ranking through interpolation, numpy.interp Map to normal distribution by inverting CDF, scipy.stats.norm.ppf Sources : * PyTorch Percentile - gist | package * Quantile Transformation with Gaussian Distribution - Sklearn Implementation - StackOverFlow * Differentiable Quantile Transformation - Miles Cranmer - PyTorch","title":"Quantile Transform"},{"location":"Notes/Unsorted/mg/#kde-transform","text":"","title":"KDE Transform"},{"location":"Notes/Unsorted/mg/#spline-functions","text":"Rational Quadratic Trigonometric Interpolation Spline for Data Visualization - Lui et al - PDF TensorFlow PyTorch Implementations Neural Spline Flows - Paper Tony Duan Implementation - Paper","title":"Spline Functions"},{"location":"Notes/Unsorted/mg/#gaussian-transform","text":"","title":"Gaussian Transform"},{"location":"Notes/Unsorted/mixtures/","text":"Mixture Models \u00b6 Resources \u00b6 Overview \u00b6 Variational Mixture of Gaussians - Prezi Latent Variable Models - Part I: GMMs and the EM Algo - Blog Code \u00b6 Built-in \u00b6 Jake Vanderplas - In Depth: Gaussian Mixture Models Gives a motivating example of the weakness of Gaussian Mixture models as well as how one can utilize the function in sklearn. From Scratch \u00b6 ML From Scratch, Part 5: GMMs - Blog Pyro Tutorial KeOps Tutorial Bayesian GMM w. SVI - Branan Hasz - TF2.0 - Blog","title":"Mixture Models"},{"location":"Notes/Unsorted/mixtures/#mixture-models","text":"","title":"Mixture Models"},{"location":"Notes/Unsorted/mixtures/#resources","text":"","title":"Resources"},{"location":"Notes/Unsorted/mixtures/#overview","text":"Variational Mixture of Gaussians - Prezi Latent Variable Models - Part I: GMMs and the EM Algo - Blog","title":"Overview"},{"location":"Notes/Unsorted/mixtures/#code","text":"","title":"Code"},{"location":"Notes/Unsorted/mixtures/#built-in","text":"Jake Vanderplas - In Depth: Gaussian Mixture Models Gives a motivating example of the weakness of Gaussian Mixture models as well as how one can utilize the function in sklearn.","title":"Built-in"},{"location":"Notes/Unsorted/mixtures/#from-scratch","text":"ML From Scratch, Part 5: GMMs - Blog Pyro Tutorial KeOps Tutorial Bayesian GMM w. SVI - Branan Hasz - TF2.0 - Blog","title":"From Scratch"},{"location":"Notes/Unsorted/mu/","text":"Uniformization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 11-03-2020 Forward Transformation \u00b6 In this step, we estimate the forward transformation of samples from \\mathcal{X} \\mathcal{X} to the uniform distribution \\mathcal{U} \\mathcal{U} . The relation is: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical Cumulative distribution function (CDF) for \\mathcal{X} \\mathcal{X} , and u u is drawn from a uniform distribution, u\\sim \\mathcal{U}([0,1]) u\\sim \\mathcal{U}([0,1]) . Boundary Issues \u00b6 The bounds for \\mathcal{U} \\mathcal{U} are [0,1] [0,1] and the bounds for \\mathcal{X} \\mathcal{X} are X.min() and X.max() . So function F_\\theta F_\\theta will be between 0 and 1 and the support F_\\theta F_\\theta will be between the limits for \\mathcal{X} \\mathcal{X} . We have two options for dealing with this: Map Outlines to Boundaries This is the easiest method as we can map all points outside to limits to the boundaries. This is the simplest method that would allow us deal with points that are outside of the distribution. Fig 2 : CDF with the extension near the boundaries. Widen the Limits of the Support This is the harder option. This will essentially squish the CDF function near the middle and widen the tails. Reverse Transformation \u00b6 This isn't really useful because we don't really want to draw samples from our distribution x \\sim \\mathcal{X} x \\sim \\mathcal{X} only to project them to a uniform distribution \\mathcal{U} \\mathcal{U} . What we really want to draw samples from the uniform distribution u \\sim \\mathcal{U} u \\sim \\mathcal{U} and then project them into our data distribution \\mathcal{X} \\mathcal{X} . We can simply take the inverse of our function P(\\cdot) P(\\cdot) to go from \\mathcal{U} \\mathcal{U} to \\mathcal{X} \\mathcal{X} . x = F^{-1}(u) x = F^{-1}(u) where u \\sim \\mathcal{U}[0,1] u \\sim \\mathcal{U}[0,1] . Now we should be able to sample from a uniform distribution \\mathcal{U} \\mathcal{U} and have the data represent the data distribution \\mathcal{X} \\mathcal{X} . This is the inverse of the CDF which, in probability terms, this is known as the inverse distribution function or the empirical distribution function (EDF). Assuming that this function is differentiable and invertible, we can define the inverse as: x = F^{-1}(u) x = F^{-1}(u) So in principal, we should be able to generate datapoints for our data distribution from a uniform distribution. We need to be careful of the bounds as we are mapping the data from [0,1] [0,1] to whatever the [ X.min(), X.max() ] is. This can cause problems. Derivative \u00b6 In this section, we will see how one can compute the derivative. Fortunately, the derivative of the CDF function F F is the PDF function f f . For this part, we are going to be using the relationship that the derivative of the CDF of a function is simply the PDF. For uniformization, let's define the following relationship: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical cumulative density function (ECDF) of \\mathcal{X} \\mathcal{X} . Proof : Let F(x) = \\int_{-\\infty}^{x}f(t) \\, dt F(x) = \\int_{-\\infty}^{x}f(t) \\, dt from the fundamental theorem of calculus. The derivative is f(x)=\\frac{d F(x)}{dx} f(x)=\\frac{d F(x)}{dx} . Then that means F(b)-F(a)=\\int_a^b f(t) dt F(b)-F(a)=\\int_a^b f(t) dt So F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) <span><span class=\"MathJax_Preview\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a)</span><script type=\"math/tex\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) So the derivative of the full function \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} Log Abs Determinant Jacobian \u00b6 This is a nice trick to use for later. It allows us to decompose composite functions. In addition, it makes it a lot easier to optimize the negative log likelihood when working with optimization algorithms. \\log f_\\theta(x) \\log f_\\theta(x) There is a small problem due to the zero values. Technically, there should be no such thing as zero probability, so we will add some regularization \\alpha \\alpha to ensure that there always is a little bit of probabilistic values. Probability (Computing the Density) \u00b6 So now, we can take it a step further and estimate densities. We don't inherently know the density of our dataset \\mathcal{X} \\mathcal{X} but we do know the density of \\mathcal{U} \\mathcal{U} . So we can use this information by means of the change of variables formula. p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| There are a few things we can do to this equation that simplify this expression. Firstly, because we are doing a uniform distribution, the probability is 1 everywhere. So the first term p_{\\mathcal{U}}(u) p_{\\mathcal{U}}(u) can cancel. So we're left with just: p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| The second thing is that we explicitly assigned u u to be equal to the CDF of x x , u = F(x) u = F(x) . So we can plug this term into the equation to obtain p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| But we know by definition that the derivative of F(x) F(x) (the CDF) is the PDF f(x) f(x) . So we actually have the equation: p_{\\mathcal{X}}(x) = f_\\theta(x) p_{\\mathcal{X}}(x) = f_\\theta(x) So they are equivalent. This is very redundant as we actually don't know the PDF so saying that you can find the PDF of \\mathcal{X} \\mathcal{X} by knowing the PDF is meaningless. However, we do this transformation in order to obtain a nice property of uniform distributions in general which we will use in the next section.","title":"Uniformization"},{"location":"Notes/Unsorted/mu/#uniformization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Date Updated: 11-03-2020","title":"Uniformization"},{"location":"Notes/Unsorted/mu/#forward-transformation","text":"In this step, we estimate the forward transformation of samples from \\mathcal{X} \\mathcal{X} to the uniform distribution \\mathcal{U} \\mathcal{U} . The relation is: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical Cumulative distribution function (CDF) for \\mathcal{X} \\mathcal{X} , and u u is drawn from a uniform distribution, u\\sim \\mathcal{U}([0,1]) u\\sim \\mathcal{U}([0,1]) .","title":"Forward Transformation"},{"location":"Notes/Unsorted/mu/#boundary-issues","text":"The bounds for \\mathcal{U} \\mathcal{U} are [0,1] [0,1] and the bounds for \\mathcal{X} \\mathcal{X} are X.min() and X.max() . So function F_\\theta F_\\theta will be between 0 and 1 and the support F_\\theta F_\\theta will be between the limits for \\mathcal{X} \\mathcal{X} . We have two options for dealing with this: Map Outlines to Boundaries This is the easiest method as we can map all points outside to limits to the boundaries. This is the simplest method that would allow us deal with points that are outside of the distribution. Fig 2 : CDF with the extension near the boundaries. Widen the Limits of the Support This is the harder option. This will essentially squish the CDF function near the middle and widen the tails.","title":"Boundary Issues"},{"location":"Notes/Unsorted/mu/#reverse-transformation","text":"This isn't really useful because we don't really want to draw samples from our distribution x \\sim \\mathcal{X} x \\sim \\mathcal{X} only to project them to a uniform distribution \\mathcal{U} \\mathcal{U} . What we really want to draw samples from the uniform distribution u \\sim \\mathcal{U} u \\sim \\mathcal{U} and then project them into our data distribution \\mathcal{X} \\mathcal{X} . We can simply take the inverse of our function P(\\cdot) P(\\cdot) to go from \\mathcal{U} \\mathcal{U} to \\mathcal{X} \\mathcal{X} . x = F^{-1}(u) x = F^{-1}(u) where u \\sim \\mathcal{U}[0,1] u \\sim \\mathcal{U}[0,1] . Now we should be able to sample from a uniform distribution \\mathcal{U} \\mathcal{U} and have the data represent the data distribution \\mathcal{X} \\mathcal{X} . This is the inverse of the CDF which, in probability terms, this is known as the inverse distribution function or the empirical distribution function (EDF). Assuming that this function is differentiable and invertible, we can define the inverse as: x = F^{-1}(u) x = F^{-1}(u) So in principal, we should be able to generate datapoints for our data distribution from a uniform distribution. We need to be careful of the bounds as we are mapping the data from [0,1] [0,1] to whatever the [ X.min(), X.max() ] is. This can cause problems.","title":"Reverse Transformation"},{"location":"Notes/Unsorted/mu/#derivative","text":"In this section, we will see how one can compute the derivative. Fortunately, the derivative of the CDF function F F is the PDF function f f . For this part, we are going to be using the relationship that the derivative of the CDF of a function is simply the PDF. For uniformization, let's define the following relationship: u = F_\\theta(x) u = F_\\theta(x) where F_\\theta(\\cdot) F_\\theta(\\cdot) is the empirical cumulative density function (ECDF) of \\mathcal{X} \\mathcal{X} . Proof : Let F(x) = \\int_{-\\infty}^{x}f(t) \\, dt F(x) = \\int_{-\\infty}^{x}f(t) \\, dt from the fundamental theorem of calculus. The derivative is f(x)=\\frac{d F(x)}{dx} f(x)=\\frac{d F(x)}{dx} . Then that means F(b)-F(a)=\\int_a^b f(t) dt F(b)-F(a)=\\int_a^b f(t) dt So F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) <span><span class=\"MathJax_Preview\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a)</span><script type=\"math/tex\">F(x)=F(x) - \\lim_{a \\rightarrow - \\infty}F(a) So the derivative of the full function \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned} \\begin{aligned} \\frac{d f(x)}{dx} &= \\frac{d}{dx} \\left[ F(x) - \\lim_{a \\rightarrow - \\infty} F(a) \\right] \\\\ &= \\frac{d F(x)}{dx} \\\\ &= f(x) \\end{aligned}","title":"Derivative"},{"location":"Notes/Unsorted/mu/#log-abs-determinant-jacobian","text":"This is a nice trick to use for later. It allows us to decompose composite functions. In addition, it makes it a lot easier to optimize the negative log likelihood when working with optimization algorithms. \\log f_\\theta(x) \\log f_\\theta(x) There is a small problem due to the zero values. Technically, there should be no such thing as zero probability, so we will add some regularization \\alpha \\alpha to ensure that there always is a little bit of probabilistic values.","title":"Log Abs Determinant Jacobian"},{"location":"Notes/Unsorted/mu/#probability-computing-the-density","text":"So now, we can take it a step further and estimate densities. We don't inherently know the density of our dataset \\mathcal{X} \\mathcal{X} but we do know the density of \\mathcal{U} \\mathcal{U} . So we can use this information by means of the change of variables formula. p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = p_{\\mathcal{U}}(u) \\; \\left| \\frac{d u}{d x} \\right| There are a few things we can do to this equation that simplify this expression. Firstly, because we are doing a uniform distribution, the probability is 1 everywhere. So the first term p_{\\mathcal{U}}(u) p_{\\mathcal{U}}(u) can cancel. So we're left with just: p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d u}{d x} \\right| The second thing is that we explicitly assigned u u to be equal to the CDF of x x , u = F(x) u = F(x) . So we can plug this term into the equation to obtain p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| p_{\\mathcal{X}}(x) = \\left| \\frac{d F(x)}{d x} \\right| But we know by definition that the derivative of F(x) F(x) (the CDF) is the PDF f(x) f(x) . So we actually have the equation: p_{\\mathcal{X}}(x) = f_\\theta(x) p_{\\mathcal{X}}(x) = f_\\theta(x) So they are equivalent. This is very redundant as we actually don't know the PDF so saying that you can find the PDF of \\mathcal{X} \\mathcal{X} by knowing the PDF is meaningless. However, we do this transformation in order to obtain a nice property of uniform distributions in general which we will use in the next section.","title":"Probability (Computing the Density)"},{"location":"Notes/Unsorted/nfs/","text":"Normalizing Flows \u00b6 Main Idea Loss Function Sampling Choice of Transformations Prior Distribution Jacobian Resources Best Tutorials Survey of Literature Neural Density Estimators Deep Density Destructors Code Tutorials Tutorials Algorithms RBIG Upgrades Cutting Edge Github Implementations Main Idea \u00b6 Distribution flows through a sequence of invertible transformations - Rezende & Mohamed (2015) We want to fit a density model p_\\theta(x) p_\\theta(x) with continuous data x \\in \\mathbb{R}^N x \\in \\mathbb{R}^N . Ideally, we want this model to: Modeling : Find the underlying distribution for the training data. Probability : For a new x' \\sim \\mathcal{X} x' \\sim \\mathcal{X} , we want to be able to evaluate p_\\theta(x') p_\\theta(x') Sampling : We also want to be able to generate samples from p_\\theta(x') p_\\theta(x') . Latent Representation : Ideally we want this representation to be meaningful. Let's assume that we can find some probability distribution for \\mathcal{X} \\mathcal{X} but it's very difficult to do. So, instead of p_\\theta(x) p_\\theta(x) , we want to find some parameterized function f_\\theta(x) f_\\theta(x) that we can learn. x = f_\\theta(x) x = f_\\theta(x) We'll define this as z=f_\\theta(x) z=f_\\theta(x) . So we also want z z to have certain properties. We want this z z to be defined by a probabilistic function and have a valid distribution z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) We also would prefer this distribution to be simply. We typically pick a normal distribution, z \\sim \\mathcal{N}(0,1) z \\sim \\mathcal{N}(0,1) We begin with in initial distribution and then we apply a sequence of L L invertible transformations in hopes that we obtain something that is more expressive. This originally came from the context of Variational AutoEncoders (VAE) where the posterior was approximated by a neural network. The authors wanted to \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} Loss Function \u00b6 We can do a simple maximum-likelihood of our distribution p_\\theta(x) p_\\theta(x) . \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) However, this expression needs to be transformed in terms of the invertible functions f_\\theta(x) f_\\theta(x) . This is where we exploit the rule for the change of variables. From here, we can come up with an expression for the likelihood by simply calculating the maximum likelihood of the initial distribution \\mathbf{z}_0 \\mathbf{z}_0 given the transformations f_L f_L . \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} So now, we can do the same maximization function but with our change of variables formulation: \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} And we can optimize this using stochastic gradient descent (SGD) which means we can use all of the autogradient and deep learning libraries available to make this procedure relatively painless. Sampling \u00b6 If we want to sample from our base distribution z z , then we just need to use the inverse of our function. x = f_\\theta^{-1}(z) x = f_\\theta^{-1}(z) where z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) . Remember, our f_\\theta(\\cdot) f_\\theta(\\cdot) is invertible and differentiable so this should be no problem. \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} or the same but only in terms of the original distribution \\mathcal{X} \\mathcal{X} We can make this transformation a bit easier to handle empirically by calculating the Log-Transformation of this expression. This removes the inverse and introduces a summation of each of the transformations individually which gives us many computational advantages. \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} So now, our original expression with p_\\theta(x) p_\\theta(x) can be written in terms of z z . TODO: Diagram with plots of the Normalizing Flow distributions which show the direction for the idea. In order to train this, we need to take expectations of the transformations. \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} Choice of Transformations \u00b6 The main thing that many of the communities have been looking into is how one chooses the aspects of the normalizing flow: the prior distribution and the Jacobian. Prior Distribution \u00b6 This is very consistent across the literature: most people use a fully-factorized Gaussian distribution. Very simple. Jacobian \u00b6 This is the area of the most research within the community. There are many different complicated frameworks but almost all of them can be put into different categories for how the Jacobian is constructed. Resources \u00b6 Best Tutorials \u00b6 Flow-Based Deep Generative Models - Lilian Weng An excellent blog post for Normalizing Flows. Probably the most thorough introduction available. Flow Models - Deep Unsupervised Learning Class , Spring 2010 Normalizing Flows: A Tutorial - Eric Jang Survey of Literature \u00b6 Neural Density Estimators \u00b6 Deep Density Destructors \u00b6 Code Tutorials \u00b6 Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/ Tutorials \u00b6 RealNVP - code I Normalizing Flows: Intro and Ideas - Kobyev et. al. (2019) Algorithms \u00b6 * RBIG Upgrades \u00b6 Modularization Lucastheis Destructive-Deep-Learning TensorFlow NormalCDF interp_regular_1d_grid IT w. TF Cutting Edge \u00b6 Neural Spline Flows - Github Complete | PyTorch PointFlow: 3D Point Cloud Generations with Continuous Normalizing Flows - Project PyTorch Conditional Density Estimation with Bayesian Normalising Flows | Code Github Implementations \u00b6 Bayesian and ML Implementation of the Normalizing Flow Network (NFN) | Paper NFs | Prezi Normalizing Flows Building Blocks Neural Spline Flow, RealNVP, Autoregressive Flow, 1x1Conv in PyTorch Clean Refactor of Eric Jang w. TF Bijectors Density Estimation and Anomaly Detection with Normalizing Flows","title":"Normalizing Flows"},{"location":"Notes/Unsorted/nfs/#normalizing-flows","text":"Main Idea Loss Function Sampling Choice of Transformations Prior Distribution Jacobian Resources Best Tutorials Survey of Literature Neural Density Estimators Deep Density Destructors Code Tutorials Tutorials Algorithms RBIG Upgrades Cutting Edge Github Implementations","title":"Normalizing Flows"},{"location":"Notes/Unsorted/nfs/#main-idea","text":"Distribution flows through a sequence of invertible transformations - Rezende & Mohamed (2015) We want to fit a density model p_\\theta(x) p_\\theta(x) with continuous data x \\in \\mathbb{R}^N x \\in \\mathbb{R}^N . Ideally, we want this model to: Modeling : Find the underlying distribution for the training data. Probability : For a new x' \\sim \\mathcal{X} x' \\sim \\mathcal{X} , we want to be able to evaluate p_\\theta(x') p_\\theta(x') Sampling : We also want to be able to generate samples from p_\\theta(x') p_\\theta(x') . Latent Representation : Ideally we want this representation to be meaningful. Let's assume that we can find some probability distribution for \\mathcal{X} \\mathcal{X} but it's very difficult to do. So, instead of p_\\theta(x) p_\\theta(x) , we want to find some parameterized function f_\\theta(x) f_\\theta(x) that we can learn. x = f_\\theta(x) x = f_\\theta(x) We'll define this as z=f_\\theta(x) z=f_\\theta(x) . So we also want z z to have certain properties. We want this z z to be defined by a probabilistic function and have a valid distribution z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) We also would prefer this distribution to be simply. We typically pick a normal distribution, z \\sim \\mathcal{N}(0,1) z \\sim \\mathcal{N}(0,1) We begin with in initial distribution and then we apply a sequence of L L invertible transformations in hopes that we obtain something that is more expressive. This originally came from the context of Variational AutoEncoders (VAE) where the posterior was approximated by a neural network. The authors wanted to \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned} \\begin{aligned} \\mathbf{z}_L = f_L \\circ f_{L-1} \\circ \\ldots \\circ f_2 \\circ f_1 (\\mathbf{z}_0) \\end{aligned}","title":"Main Idea"},{"location":"Notes/Unsorted/nfs/#loss-function","text":"We can do a simple maximum-likelihood of our distribution p_\\theta(x) p_\\theta(x) . \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) However, this expression needs to be transformed in terms of the invertible functions f_\\theta(x) f_\\theta(x) . This is where we exploit the rule for the change of variables. From here, we can come up with an expression for the likelihood by simply calculating the maximum likelihood of the initial distribution \\mathbf{z}_0 \\mathbf{z}_0 given the transformations f_L f_L . \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} \\begin{aligned} p_\\theta(x) = p_\\mathcal{Z}(f_\\theta(x)) \\left| \\frac{\\partial f_\\theta(x)}{\\partial x} \\right| \\end{aligned} So now, we can do the same maximization function but with our change of variables formulation: \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} \\begin{aligned} \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) &= \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\mathcal{Z}\\left(f_\\theta(x^{(i)})\\right) + \\log \\left| \\frac{\\partial f_\\theta (x^{(i)})}{\\partial x} \\right| \\end{aligned} And we can optimize this using stochastic gradient descent (SGD) which means we can use all of the autogradient and deep learning libraries available to make this procedure relatively painless.","title":"Loss Function"},{"location":"Notes/Unsorted/nfs/#sampling","text":"If we want to sample from our base distribution z z , then we just need to use the inverse of our function. x = f_\\theta^{-1}(z) x = f_\\theta^{-1}(z) where z \\sim p_\\mathcal{Z}(z) z \\sim p_\\mathcal{Z}(z) . Remember, our f_\\theta(\\cdot) f_\\theta(\\cdot) is invertible and differentiable so this should be no problem. \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} \\begin{aligned} q(z') = q(z) \\left| \\frac{\\partial f}{\\partial z} \\right|^{-1} \\end{aligned} or the same but only in terms of the original distribution \\mathcal{X} \\mathcal{X} We can make this transformation a bit easier to handle empirically by calculating the Log-Transformation of this expression. This removes the inverse and introduces a summation of each of the transformations individually which gives us many computational advantages. \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} \\begin{aligned} \\log q_L (\\mathbf{z}_L) = \\log q_0 (\\mathbf{z}_0) - \\sum_{l=1}^L \\log \\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_l} \\right| \\end{aligned} So now, our original expression with p_\\theta(x) p_\\theta(x) can be written in terms of z z . TODO: Diagram with plots of the Normalizing Flow distributions which show the direction for the idea. In order to train this, we need to take expectations of the transformations. \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned} \\begin{aligned} \\mathcal{L}(\\theta) &= \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log p(\\mathbf{x,z}_L)\\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\log q_0(\\mathbf{z}_0) \\right] - \\mathbb{E}_{q_0(\\mathbf{z}_0)} \\left[ \\sum_{l=1}^L \\log \\text{det}\\left| \\frac{\\partial f_l}{\\partial \\mathbf{z}_k} \\right| \\right] \\end{aligned}","title":"Sampling"},{"location":"Notes/Unsorted/nfs/#choice-of-transformations","text":"The main thing that many of the communities have been looking into is how one chooses the aspects of the normalizing flow: the prior distribution and the Jacobian.","title":"Choice of Transformations"},{"location":"Notes/Unsorted/nfs/#prior-distribution","text":"This is very consistent across the literature: most people use a fully-factorized Gaussian distribution. Very simple.","title":"Prior Distribution"},{"location":"Notes/Unsorted/nfs/#jacobian","text":"This is the area of the most research within the community. There are many different complicated frameworks but almost all of them can be put into different categories for how the Jacobian is constructed.","title":"Jacobian"},{"location":"Notes/Unsorted/nfs/#resources","text":"","title":"Resources"},{"location":"Notes/Unsorted/nfs/#best-tutorials","text":"Flow-Based Deep Generative Models - Lilian Weng An excellent blog post for Normalizing Flows. Probably the most thorough introduction available. Flow Models - Deep Unsupervised Learning Class , Spring 2010 Normalizing Flows: A Tutorial - Eric Jang","title":"Best Tutorials"},{"location":"Notes/Unsorted/nfs/#survey-of-literature","text":"","title":"Survey of Literature"},{"location":"Notes/Unsorted/nfs/#neural-density-estimators","text":"","title":"Neural Density Estimators"},{"location":"Notes/Unsorted/nfs/#deep-density-destructors","text":"","title":"Deep Density Destructors"},{"location":"Notes/Unsorted/nfs/#code-tutorials","text":"Building Prob Dist with TF Probability Bijector API - Blog https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/","title":"Code Tutorials"},{"location":"Notes/Unsorted/nfs/#tutorials","text":"RealNVP - code I Normalizing Flows: Intro and Ideas - Kobyev et. al. (2019)","title":"Tutorials"},{"location":"Notes/Unsorted/nfs/#algorithms","text":"*","title":"Algorithms"},{"location":"Notes/Unsorted/nfs/#rbig-upgrades","text":"Modularization Lucastheis Destructive-Deep-Learning TensorFlow NormalCDF interp_regular_1d_grid IT w. TF","title":"RBIG Upgrades"},{"location":"Notes/Unsorted/nfs/#cutting-edge","text":"Neural Spline Flows - Github Complete | PyTorch PointFlow: 3D Point Cloud Generations with Continuous Normalizing Flows - Project PyTorch Conditional Density Estimation with Bayesian Normalising Flows | Code","title":"Cutting Edge"},{"location":"Notes/Unsorted/nfs/#github-implementations","text":"Bayesian and ML Implementation of the Normalizing Flow Network (NFN) | Paper NFs | Prezi Normalizing Flows Building Blocks Neural Spline Flow, RealNVP, Autoregressive Flow, 1x1Conv in PyTorch Clean Refactor of Eric Jang w. TF Bijectors Density Estimation and Anomaly Detection with Normalizing Flows","title":"Github Implementations"},{"location":"Notes/Unsorted/pdf_est/","text":"PDF Estimation \u00b6 Main Idea \u00b6 Fig I : Input Distribution. P(x \\in [a,b]) = \\int_a^b p(x)dx P(x \\in [a,b]) = \\int_a^b p(x)dx Likelihood \u00b6 Given a dataset \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} , we can find the some parameters \\theta \\theta by solving this optimization function: the likelihood \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) or equivalently: \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] This is equivalent to minimizing the KL-Divergence between the empirical data distribution \\tilde{p}_\\text{data}(x) \\tilde{p}_\\text{data}(x) and the model p_\\theta p_\\theta . D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) where \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] Stochastic Gradient Descent \u00b6 Maximum likelihood is an optimization problem so we can use stochastic gradient descent (SGD) to solve it. This algorithm minimizes the expectation for f f assuming it is a differentiable function of \\theta \\theta . \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] With maximum likelihood, the optimization problem becomes: \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] We typically use SGD because it works with large datasets and it allows us to use deep learning architectures and convenient packages. Example \u00b6 Mixture of Gaussians \u00b6 p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) where we have parameters as k k means, variances and mixture weights, \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) However, this doesn't really work for high-dimensional datasets. To sample, we pick a cluster center and then add some Gaussian noise. Histogram Method \u00b6 Gotchas \u00b6 Search Sorted \u00b6 Numpy PyTorch def searchsorted ( bin_locations , inputs , eps = 1e-6 ): bin_locations [ ... , - 1 ] += eps h_sorted = torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 return h_sorted This is an unofficial implementation. There is still some talks in the PyTorch community to implement this. See github issue here . For now, we just use the implementation found in various implementations .","title":"PDF Estimation"},{"location":"Notes/Unsorted/pdf_est/#pdf-estimation","text":"","title":"PDF Estimation"},{"location":"Notes/Unsorted/pdf_est/#main-idea","text":"Fig I : Input Distribution. P(x \\in [a,b]) = \\int_a^b p(x)dx P(x \\in [a,b]) = \\int_a^b p(x)dx","title":"Main Idea"},{"location":"Notes/Unsorted/pdf_est/#likelihood","text":"Given a dataset \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} \\mathcal{D} = \\{x^{1}, x^{2}, \\ldots, x^{n}\\} , we can find the some parameters \\theta \\theta by solving this optimization function: the likelihood \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) \\underset{\\theta}{\\text{max}} \\sum_i \\log p_\\theta(x^{(i)}) or equivalently: \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] \\underset{\\theta}{\\text{min }} \\mathbb{E}_x \\left[ - \\log p_\\theta(x) \\right] This is equivalent to minimizing the KL-Divergence between the empirical data distribution \\tilde{p}_\\text{data}(x) \\tilde{p}_\\text{data}(x) and the model p_\\theta p_\\theta . D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) D_\\text{KL}(\\hat{p}(\\text{data}) || p_\\theta) = \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] - H(\\hat{p}_\\text{data}) where \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}] \\hat{p}_\\text{data}(x) = \\frac{1}{n} \\sum_{i=1}^N \\mathbf{1}[x = x^{(i)}]","title":"Likelihood"},{"location":"Notes/Unsorted/pdf_est/#stochastic-gradient-descent","text":"Maximum likelihood is an optimization problem so we can use stochastic gradient descent (SGD) to solve it. This algorithm minimizes the expectation for f f assuming it is a differentiable function of \\theta \\theta . \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] \\argmin_\\theta \\mathbb{E} \\left[ f(\\theta) \\right] With maximum likelihood, the optimization problem becomes: \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] \\argmin_\\theta \\mathbb{E}_{x \\sim \\hat{p}_\\text{data}} \\left[ - \\log p_\\theta(x) \\right] We typically use SGD because it works with large datasets and it allows us to use deep learning architectures and convenient packages.","title":"Stochastic Gradient Descent"},{"location":"Notes/Unsorted/pdf_est/#example","text":"","title":"Example"},{"location":"Notes/Unsorted/pdf_est/#mixture-of-gaussians","text":"p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) p_\\theta(x) = \\sum_i^k \\pi_i \\mathcal{N}(x ; \\mu_i, \\sigma_i^2) where we have parameters as k k means, variances and mixture weights, \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) \\theta = (\\pi_1, \\cdots, \\pi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k) However, this doesn't really work for high-dimensional datasets. To sample, we pick a cluster center and then add some Gaussian noise.","title":"Mixture of Gaussians"},{"location":"Notes/Unsorted/pdf_est/#histogram-method","text":"","title":"Histogram Method"},{"location":"Notes/Unsorted/pdf_est/#gotchas","text":"","title":"Gotchas"},{"location":"Notes/Unsorted/pdf_est/#search-sorted","text":"Numpy PyTorch def searchsorted ( bin_locations , inputs , eps = 1e-6 ): bin_locations [ ... , - 1 ] += eps h_sorted = torch . sum ( inputs [ ... , None ] >= bin_locations , dim =- 1 ) - 1 return h_sorted This is an unofficial implementation. There is still some talks in the PyTorch community to implement this. See github issue here . For now, we just use the implementation found in various implementations .","title":"Search Sorted"},{"location":"Notes/Unsorted/rbig/","text":"Rotation-Based Iterative Gaussianization (RBIG) \u00b6 Motivation Algorithm Marginal (Univariate) Gaussianization Marginal Uniformization Gaussianization of a Uniform Variable Linear Transformation Information Theory Measures Information Entropy Mutual Information KL-Divergence Motivation \u00b6 The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} Algorithm \u00b6 Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) Marginal (Univariate) Gaussianization \u00b6 This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components Marginal Uniformization \u00b6 We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy Gaussianization of a Uniform Variable \u00b6 Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U Linear Transformation \u00b6 This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space. Information Theory Measures \u00b6 Caption : Information Theory measures in a nutshell. Information \u00b6 Entropy \u00b6 Mutual Information \u00b6 Caption : Schematic for finding the Mutual Information using using RBIG. \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} KL-Divergence \u00b6 Caption : Schematic for finding the KL-Divergence using using RBIG. Let \\mathcal{G}_\\theta (\\mathbf{X}) \\mathcal{G}_\\theta (\\mathbf{X}) be the Gaussianization of the variable \\mathbf{X} \\mathbf{X} which is parameterized by \\theta \\theta . \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned}","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"Notes/Unsorted/rbig/#rotation-based-iterative-gaussianization-rbig","text":"Motivation Algorithm Marginal (Univariate) Gaussianization Marginal Uniformization Gaussianization of a Uniform Variable Linear Transformation Information Theory Measures Information Entropy Mutual Information KL-Divergence","title":"Rotation-Based Iterative Gaussianization (RBIG)"},{"location":"Notes/Unsorted/rbig/#motivation","text":"The RBIG algorithm is a member of the density destructor family of methods. A density destructor is a generative model that seeks to transform your original data distribution, \\mathcal{X} \\mathcal{X} to a base distribution, \\mathcal{Z} \\mathcal{Z} through an invertible tranformation \\mathcal{G}_\\theta \\mathcal{G}_\\theta , parameterized by \\theta \\theta . \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} \\begin{aligned} x &\\sim \\mathcal P_x \\sim \\text{Data Distribution}\\\\ \\hat z &= \\mathcal{G}_\\theta(x) \\sim \\text{Approximate Base Distribution} \\end{aligned} Because we have invertible transforms, we can use the change of variables formula to get probability estimates of our original data space, \\mathcal{X} \\mathcal{X} using our base distribution \\mathcal{Z} \\mathcal{Z} . This is a well known formula written as: p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| p_x(x)= p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\frac{\\partial \\mathcal{G}_{\\theta}(x)}{\\partial x} \\right| = p_{z}\\left( \\mathcal{G}_{\\theta}(x) \\right) \\left| \\nabla_x \\mathcal{G}(x) \\right| If you are familiar with normalizing flows, you'll find some similarities between the formulations. Inherently, they are the same. However most (if not all) of the major methods of normalizing flows, they focus on the log-likelihood estimation of data \\mathcal{X} \\mathcal{X} . They seek to minimize this log-deteriminant of the Jacobian as a cost function. RBIG is different in this regard as it has a different objective. RBIG seeks to maximize the negentropy or minimize the total correlation. Essentialy, RVIG is an algorithm that respects the name density destructor fundamental. We argue that by destroying the density, we maximize the entropy and destroy all redundancies within the marginals of the variables in question. From this formulation, this allows us to utilize RBIG to calculate many other IT measures which we highlight below. \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned} \\begin{aligned} z &\\sim \\mathcal P_z \\sim \\text{Base Distribution}\\\\ \\hat x &= \\mathbf G_\\phi(x) \\sim \\text{Approximate Data Distribution} \\end{aligned}","title":"Motivation"},{"location":"Notes/Unsorted/rbig/#algorithm","text":"Gaussianization - Given a random variance \\mathbf x \\in \\mathbb R^d \\mathbf x \\in \\mathbb R^d , a Gaussianization transform is an invertible and differentiable transform \\mathcal \\Psi(\\mathbf) \\mathcal \\Psi(\\mathbf) s.t. \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) \\mathcal \\Psi( \\mathbf x) \\sim \\mathcal N(0, \\mathbf I) . \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) \\mathcal G:\\mathbf x^{(k+1)}=\\mathbf R_{(k)}\\cdot \\mathbf \\Psi_{(k)}\\left( \\mathbf x^{(k)} \\right) where: * \\mathbf \\Psi_{(k)} \\mathbf \\Psi_{(k)} is the marginal Gaussianization of each dimension of \\mathbf x_{(k)} \\mathbf x_{(k)} for the corresponding iteration. * \\mathbf R_{(k)} \\mathbf R_{(k)} is the rotation matrix for the marginally Gaussianized variable \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right) \\mathbf \\Psi_{(k)}\\left( \\mathbf x_{(k)} \\right)","title":"Algorithm"},{"location":"Notes/Unsorted/rbig/#marginal-univariate-gaussianization","text":"This transformation is the \\mathcal \\Psi_\\theta \\mathcal \\Psi_\\theta step for the RBIG algorithm. In theory, to go from any distribution to a Gaussian distribution, we just need to To go from \\mathcal P \\rightarrow \\mathcal G \\mathcal P \\rightarrow \\mathcal G Convert the Data to a Normal distribution \\mathcal U \\mathcal U Apply the CDF of the Gaussian distribution \\mathcal G \\mathcal G Apply the inverse Gaussian CDF So to break this down even further we need two key components","title":"Marginal (Univariate) Gaussianization"},{"location":"Notes/Unsorted/rbig/#marginal-uniformization","text":"We have to estimate the PDF of the marginal distribution of \\mathbf x \\mathbf x . Then using the CDF of that estimated distribution, we can compute the uniform u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k u=U_k(x^k)=\\int_{-\\infty}^{x^k}\\mathcal p(x^k)dx^k This boils down to estimating the histogram of the data distribution in order to get some probability distribution. I can think of a few ways to do this but the simplest is using the histogram function. Then convert it to a scipy stats rv where we will have access to functions like pdf and cdf. One nice trick is to add something to make the transformation smooth to ensure all samples are within the boundaries. Example Implementation - https://github.com/davidinouye/destructive-deep-learning/blob/master/ddl/univariate.py From there, we just need the CDF of the univariate function u(\\mathbf x) u(\\mathbf x) . We can just used the ppf function (the inverse CDF / erf) in scipy","title":"Marginal Uniformization"},{"location":"Notes/Unsorted/rbig/#gaussianization-of-a-uniform-variable","text":"Let's look at the first step: Marginal Uniformization. There are a number of ways that we can do this. To go from \\mathcal G \\rightarrow \\mathcal U \\mathcal G \\rightarrow \\mathcal U","title":"Gaussianization of a Uniform Variable"},{"location":"Notes/Unsorted/rbig/#linear-transformation","text":"This is the \\mathcal R_\\theta \\mathcal R_\\theta step in the RBIG algorithm. We take some data \\mathbf x_i \\mathbf x_i and apply some rotation to that data \\mathcal R_\\theta (\\mathbf x_i) \\mathcal R_\\theta (\\mathbf x_i) . This rotation is somewhat flexible provided that it follows a few criteria: Orthogonal Orthonormal Invertible So a few options that have been implemented include: Independence Components Analysis (ICA) Principal Components Analysis (PCA) Random Rotations (random) We would like to extend this framework to include more options, e.g. Convolutions (conv) Orthogonally Initialized Components (dct) The whole transformation process goes as follows: \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G \\mathcal P \\rightarrow \\mathbf W \\cdot \\mathcal P \\rightarrow U \\rightarrow G Where we have the following spaces: \\mathcal P \\mathcal P - the data space for \\mathcal X \\mathcal X . \\mathbf W \\cdot \\mathcal P \\mathbf W \\cdot \\mathcal P - The transformed space. \\mathcal U \\mathcal U - The Uniform space. \\mathcal G \\mathcal G - The Gaussian space.","title":"Linear Transformation"},{"location":"Notes/Unsorted/rbig/#information-theory-measures","text":"Caption : Information Theory measures in a nutshell.","title":"Information Theory Measures"},{"location":"Notes/Unsorted/rbig/#information","text":"","title":"Information"},{"location":"Notes/Unsorted/rbig/#entropy","text":"","title":"Entropy"},{"location":"Notes/Unsorted/rbig/#mutual-information","text":"Caption : Schematic for finding the Mutual Information using using RBIG. \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned} \\begin{aligned} I(\\mathbf{x,y}) &= T\\left( \\left[ \\mathcal{G}_\\theta (\\mathbf{X}), \\mathcal{G}_\\phi (\\mathbf{Y}) \\right] \\right) \\end{aligned}","title":"Mutual Information"},{"location":"Notes/Unsorted/rbig/#kl-divergence","text":"Caption : Schematic for finding the KL-Divergence using using RBIG. Let \\mathcal{G}_\\theta (\\mathbf{X}) \\mathcal{G}_\\theta (\\mathbf{X}) be the Gaussianization of the variable \\mathbf{X} \\mathbf{X} which is parameterized by \\theta \\theta . \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned} \\begin{aligned} D_\\text{KL}\\left[ \\mathbf{X||Y} \\right] &= D_\\text{KL}\\left[ \\mathbf{X ||\\mathcal{G}_\\theta(Y)} \\right] \\\\ &= J\\left[ \\mathcal{G}_\\theta (\\mathbf{\\hat{y}}) \\right] \\end{aligned}","title":"KL-Divergence"},{"location":"Notes/Unsorted/refactor/","text":"Refactoring RBIG (RBIG 1.1) \u00b6 Components \u00b6 Flow \u00b6 Forward Transformation Transformation Log Determinant Jacobian Inverse Transformation Transformation Log Determinant Jacobian Normalizing Flow \u00b6 This is a sequence of Normalizing Flows. Forward Transformation (all layers) Backwards Transformation (all layers) Output: Transformation Log Determinant Jacobian Normalizing Flow Model \u00b6 This is a Normalizing flow with a prior distribution Init: Prior, NF Model Forward: Forward, LogDet, Prior Backward: Transform, LogDet Sample: Transform Ideal Case \u00b6 Define the Prior Distribution d_dimensions = 1 # initialize prior distribution prior = MultivariateNormal ( mean = torch . zeros ( d_dimensions ), cov = torch . eye ( d_dimensions ) ) Define the Model n_layers = 2 # make flow blocks flows = [ flow ( dim = d_dimensions ) for _ in range ( n_layers )] # create model given flow blocks and prior model = NormalizingFlowModel ( prior , flows ) Define Optimization scheme opt = optim . Adam ( model . parameters (), lr = 0.005 ) Optimize Model for i in range ( n_epochs ): # initialize optimizer opt . zero_grad () # get forward transformation z = model . transform ( x ) # get prior probability prior_logprob = model . prior ( x ) # get log determinant jacobian prob log_det = model . logabsdet ( x ) # calculate loss loss = - torch . mean ( prior_logprob + log_det ) # backpropagate loss . backward () # optimize forward opt . step ()","title":"Refactoring RBIG (RBIG 1.1)"},{"location":"Notes/Unsorted/refactor/#refactoring-rbig-rbig-11","text":"","title":"Refactoring RBIG (RBIG 1.1)"},{"location":"Notes/Unsorted/refactor/#components","text":"","title":"Components"},{"location":"Notes/Unsorted/refactor/#flow","text":"Forward Transformation Transformation Log Determinant Jacobian Inverse Transformation Transformation Log Determinant Jacobian","title":"Flow"},{"location":"Notes/Unsorted/refactor/#normalizing-flow","text":"This is a sequence of Normalizing Flows. Forward Transformation (all layers) Backwards Transformation (all layers) Output: Transformation Log Determinant Jacobian","title":"Normalizing Flow"},{"location":"Notes/Unsorted/refactor/#normalizing-flow-model","text":"This is a Normalizing flow with a prior distribution Init: Prior, NF Model Forward: Forward, LogDet, Prior Backward: Transform, LogDet Sample: Transform","title":"Normalizing Flow Model"},{"location":"Notes/Unsorted/refactor/#ideal-case","text":"Define the Prior Distribution d_dimensions = 1 # initialize prior distribution prior = MultivariateNormal ( mean = torch . zeros ( d_dimensions ), cov = torch . eye ( d_dimensions ) ) Define the Model n_layers = 2 # make flow blocks flows = [ flow ( dim = d_dimensions ) for _ in range ( n_layers )] # create model given flow blocks and prior model = NormalizingFlowModel ( prior , flows ) Define Optimization scheme opt = optim . Adam ( model . parameters (), lr = 0.005 ) Optimize Model for i in range ( n_epochs ): # initialize optimizer opt . zero_grad () # get forward transformation z = model . transform ( x ) # get prior probability prior_logprob = model . prior ( x ) # get log determinant jacobian prob log_det = model . logabsdet ( x ) # calculate loss loss = - torch . mean ( prior_logprob + log_det ) # backpropagate loss . backward () # optimize forward opt . step ()","title":"Ideal Case"},{"location":"Notes/Unsorted/rotation/","text":"Rotation \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Colab Notebook: Notebook Main Idea \u00b6 Rotation Matrix \u00b6 Foward Transformation \u00b6 Reverse Transformation \u00b6 Jacobian \u00b6 The deteriminant of an orthogonal matrix is 1. Proof : There are a series of transformations that can be used to prove this: \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} Therefore, we can conclude that the \\det(\\mathbf{R})=1 \\det(\\mathbf{R})=1 . Log Jacobian \u00b6 As shown above, the log determinant jacobian of an orthogonal matrix is 1. So taking the log of this is simply zero. \\log(\\det(\\mathbf{R})) = \\log(1) = 0 \\log(\\det(\\mathbf{R})) = \\log(1) = 0 Decompositions \u00b6 QR Decomposition \u00b6 A=QR A=QR where * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * Q \\in \\mathbb{R}^{N \\times N} Q \\in \\mathbb{R}^{N \\times N} is orthogonal * R \\in \\mathbb{R}^{N \\times M} R \\in \\mathbb{R}^{N \\times M} is upper triangular Singular Value Decomposition \u00b6 Finds the singular values of the matrix. A=U\\Sigma V^\\top A=U\\Sigma V^\\top where: * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * U \\in \\mathbb{R}^{N \\times K} U \\in \\mathbb{R}^{N \\times K} is unitary * \\Sigma \\in \\mathbb{R}^{K \\times K} \\Sigma \\in \\mathbb{R}^{K \\times K} are the singular values * V^\\top \\in \\mathbb{R}^{K \\times M} V^\\top \\in \\mathbb{R}^{K \\times M} is unitary Eigendecomposition \u00b6 Finds the singular values of a symmetric matrix A_S=Q\\Lambda Q^\\top A_S=Q\\Lambda Q^\\top where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary Polar Decomposition \u00b6 A_S=QS A_S=QS where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary Initializing \u00b6 We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self Transformation \u00b6 We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W Inverse Transformation \u00b6 We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W ) Jacobian \u00b6 Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]]) Log Likelihood (?) \u00b6","title":"Rotation"},{"location":"Notes/Unsorted/rotation/#rotation","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Website: jejjohnson.netlify.com Colab Notebook: Notebook","title":"Rotation"},{"location":"Notes/Unsorted/rotation/#main-idea","text":"","title":"Main Idea"},{"location":"Notes/Unsorted/rotation/#rotation-matrix","text":"","title":"Rotation Matrix"},{"location":"Notes/Unsorted/rotation/#foward-transformation","text":"","title":"Foward Transformation"},{"location":"Notes/Unsorted/rotation/#reverse-transformation","text":"","title":"Reverse Transformation"},{"location":"Notes/Unsorted/rotation/#jacobian","text":"The deteriminant of an orthogonal matrix is 1. Proof : There are a series of transformations that can be used to prove this: \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} \\begin{aligned} 1 &= \\det(\\mathbf{I}) \\\\ &= \\det (\\mathbf{R^\\top R}) \\\\ &= \\det (\\mathbf{R^\\top}) \\det(\\mathbf{R}) \\\\ &= \\det(\\mathbf{R})^2 \\\\ \\end{aligned} Therefore, we can conclude that the \\det(\\mathbf{R})=1 \\det(\\mathbf{R})=1 .","title":"Jacobian"},{"location":"Notes/Unsorted/rotation/#log-jacobian","text":"As shown above, the log determinant jacobian of an orthogonal matrix is 1. So taking the log of this is simply zero. \\log(\\det(\\mathbf{R})) = \\log(1) = 0 \\log(\\det(\\mathbf{R})) = \\log(1) = 0","title":"Log Jacobian"},{"location":"Notes/Unsorted/rotation/#decompositions","text":"","title":"Decompositions"},{"location":"Notes/Unsorted/rotation/#qr-decomposition","text":"A=QR A=QR where * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * Q \\in \\mathbb{R}^{N \\times N} Q \\in \\mathbb{R}^{N \\times N} is orthogonal * R \\in \\mathbb{R}^{N \\times M} R \\in \\mathbb{R}^{N \\times M} is upper triangular","title":"QR Decomposition"},{"location":"Notes/Unsorted/rotation/#singular-value-decomposition","text":"Finds the singular values of the matrix. A=U\\Sigma V^\\top A=U\\Sigma V^\\top where: * A \\in \\mathbb{R}^{N \\times M} A \\in \\mathbb{R}^{N \\times M} * U \\in \\mathbb{R}^{N \\times K} U \\in \\mathbb{R}^{N \\times K} is unitary * \\Sigma \\in \\mathbb{R}^{K \\times K} \\Sigma \\in \\mathbb{R}^{K \\times K} are the singular values * V^\\top \\in \\mathbb{R}^{K \\times M} V^\\top \\in \\mathbb{R}^{K \\times M} is unitary","title":"Singular Value Decomposition"},{"location":"Notes/Unsorted/rotation/#eigendecomposition","text":"Finds the singular values of a symmetric matrix A_S=Q\\Lambda Q^\\top A_S=Q\\Lambda Q^\\top where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary","title":"Eigendecomposition"},{"location":"Notes/Unsorted/rotation/#polar-decomposition","text":"A_S=QS A_S=QS where: * A_S \\in \\mathbb{R}^{N \\times N} A_S \\in \\mathbb{R}^{N \\times N} * Q \\in \\mathbb{R}^{N \\times K} Q \\in \\mathbb{R}^{N \\times K} is unitary * \\Lambda \\in \\mathbb{R}^{K \\times K} \\Lambda \\in \\mathbb{R}^{K \\times K} are the singular values * Q^\\top \\in \\mathbb{R}^{K \\times N} Q^\\top \\in \\mathbb{R}^{K \\times N} is unitary","title":"Polar Decomposition"},{"location":"Notes/Unsorted/rotation/#initializing","text":"We have a intialization step where we compute \\mathbf W \\mathbf W (the transformation matrix). This will be in the fit method. We will need the data because some transformations depend on \\mathbf x \\mathbf x like the PCA and the ICA. class LinearTransform ( BaseEstimator , TransformMixing ): \"\"\" Parameters ---------- basis : \"\"\" def __init__ ( self , basis = 'PCA' , conv = 16 ): self . basis = basis self . conv = conv def fit ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" # Check the data # Implement the transformation if basis . upper () == 'PCA' : ... elif basis . upper () == 'ICA' : ... elif basis . lower () == 'random' : ... elif basis . lower () == 'conv' : ... elif basis . upper () == 'dct' : ... else : Raise ValueError ( '...' ) # Save the transformation matrix self . W = ... return self","title":"Initializing"},{"location":"Notes/Unsorted/rotation/#transformation","text":"We have a transformation step: \\mathbf{z=W\\cdot x} \\mathbf{z=W\\cdot x} where: * \\mathbf W \\mathbf W is the transformation * \\mathbf x \\mathbf x is the input data * \\mathbf y \\mathbf y is the final transformation def transform ( self , data ): \"\"\" Computes the inverse transformation of z = W x Parameters ---------- data : array, (n_samples x Dimensions) \"\"\" return data @ self . W","title":"Transformation"},{"location":"Notes/Unsorted/rotation/#inverse-transformation","text":"We also can apply an inverse transform. def inverse ( self , data ): \"\"\" Computes the inverse transformation of z = W^-1 x Parameters ---------- data : array, (n_samples x Dimensions) Returns ------- \"\"\" return data @ np . linalg . inv ( self . W )","title":"Inverse Transformation"},{"location":"Notes/Unsorted/rotation/#jacobian_1","text":"Lastly, we can calculate the Jacobian of that function. The Jacobian of a linear transformation is just def logjacobian ( self , data = None ): \"\"\" \"\"\" if data is None : return np . linalg . slogdet ( self . W )[ 1 ] return np . linalg . slogdet ( self . W )[ 1 ] + np . zeros ([ 1 , data . shape [ 1 ]])","title":"Jacobian"},{"location":"Notes/Unsorted/rotation/#log-likelihood","text":"","title":"Log Likelihood (?)"},{"location":"Notes/Unsorted/uniform/","text":"Uniform Distribution \u00b6 Entropy H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right] H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right]","title":"Uniform Distribution"},{"location":"Notes/Unsorted/uniform/#uniform-distribution","text":"Entropy H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right] H(x) = \\log \\left[ \\prod_{i=1}^{D}(b_d - a_d) \\right]","title":"Uniform Distribution"},{"location":"demos/demo_1_marginal_uniformization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Marginal Uniformization \u00b6 import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) from rbig.data import ToyData from rbig.transform.histogram import ScipyHistogramUniformization from rbig.transform.kde import ScipyKDEUniformization , SklearnKDEUniformization import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Data \u00b6 seed = 123 n_samples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) # plot distribution sns . distplot ( X ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4ea160> Method I - Built-In Scipy Histogram Distribution \u00b6 Fit Model to Distribution \u00b6 # initialize HistogramClass nbins = 1_000 #int(np.sqrt(n_samples)) support_extension = 20 alpha = 1e-16 marg_hist_clf = ScipyHistogramUniformization ( nbins = nbins , support_extension = support_extension , alpha = alpha ) # fit to data marg_hist_clf . fit ( X ) ScipyHistogramUniformization(alpha=1e-16, kwargs={}, nbins=1000, support_extension=20) Transform Data \u00b6 # transform data Xu = marg_hist_clf . transform ( X ) sns . distplot ( Xu , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45db126a0> Inverse Transform Data \u00b6 # transform data X_approx = marg_hist_clf . inverse_transform ( Xu ) sns . distplot ( X_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45daa8ee0> Sampling \u00b6 # transform data X_samples = marg_hist_clf . sample ( 10_000 , seed ) sns . distplot ( X_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45d9945b0> Jacobian (Log Absolute Determinant) \u00b6 # estimated log probability of dataset x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) -5.535316543523011 -1.2726366664816962 10 * np . spacing ( 1 ) 2.220446049250313e-15 print ( f \"Min: { X . min () } , Max: { X . max () } \" ) X_small_samples = data_dist . rvs ( size = ( 10 , 1 ), random_state = seed ) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples ) print ( x_slogdet . min (), x_slogdet . max ()) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples - 100 ) print ( x_slogdet . min (), x_slogdet . max (), marg_hist_clf . marginal_transforms_ [ 0 ] . b ) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples + 100 ) print ( x_slogdet . min (), x_slogdet . max ()) Min: 0.16885335848364358, Max: 18.275140139054425 -3.1374212707246407 -1.3158088383469049 -42.376678031427744 -42.376678031427744 21.896397495168582 -42.376678031427744 -42.376678031427744 Log Probability \u00b6 # calculate real log probability of distribution x_prob = marg_hist_clf . score_samples ( X ) data_prob = data_dist . logpdf ( X ) sns . distplot ( data_prob , color = 'black' , label = 'Dataset' ) sns . distplot ( x_prob , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7ff45d8d9c70> Negative Log-Likelihood \u00b6 x_score = marg_hist_clf . score_samples ( X ) . mean () data_score = data_dist . logpdf ( X ) . mean () print ( x_score , data_score ) -1.994833104562771 -2.023673498524285 Method II - Quantile Method w/ PDF Estimator \u00b6 # KDE parameters n_quantiles = 1_000 support_extension = 10 bw_estimator = 'scott' # initialize kde uniformization class # marg_kde_clf = ScipyKDEUniformization( # n_quantiles=n_quantiles, # support_extension=support_extension, # bw_estimator=bw_estimator # ) marg_kde_clf = SklearnKDEUniformization ( n_quantiles = n_quantiles , support_extension = support_extension , ) SklearnKDEUniformization # fit to data marg_kde_clf . fit ( X ) SklearnKDEUniformization(algorithm='kd_tree', kernel='gaussian', kwargs={}, metric='euclidean', n_quantiles=1000, support_extension=10) Transform Data \u00b6 # transform data Xu = marg_kde_clf . transform ( X ) sns . distplot ( Xu , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c197be0> Inverse Transformation \u00b6 # transform data X_approx = marg_kde_clf . inverse_transform ( Xu ) sns . distplot ( X_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4de4f0> Sampling \u00b6 # transform data X_samples = marg_kde_clf . sample ( 10_000 , seed ) sns . distplot ( X_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4de910> Jacobian (Log Absolute Determinant) \u00b6 # estimated log probability of dataset x_slogdet = marg_kde_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) -9.994573000232851 -1.645619827763175 Log Probability \u00b6 # calculate real log probability of distribution x_prob = marg_kde_clf . score_samples ( X ) data_prob = data_dist . logpdf ( X ) sns . distplot ( data_prob , color = 'black' , label = 'Dataset' ) sns . distplot ( x_prob , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7ff45c01a0d0> Negative Log-Likelihood \u00b6 x_score = marg_kde_clf . score_samples ( X ) . mean () data_score = data_dist . logpdf ( X ) . mean () print ( x_score , data_score ) -2.0241925324818038 -2.023673498524285","title":"Demo 1 marginal uniformization"},{"location":"demos/demo_1_marginal_uniformization/#marginal-uniformization","text":"import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) from rbig.data import ToyData from rbig.transform.histogram import ScipyHistogramUniformization from rbig.transform.kde import ScipyKDEUniformization , SklearnKDEUniformization import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Marginal Uniformization"},{"location":"demos/demo_1_marginal_uniformization/#data","text":"seed = 123 n_samples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) # plot distribution sns . distplot ( X ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4ea160>","title":"Data"},{"location":"demos/demo_1_marginal_uniformization/#method-i-built-in-scipy-histogram-distribution","text":"","title":"Method I - Built-In Scipy Histogram Distribution"},{"location":"demos/demo_1_marginal_uniformization/#fit-model-to-distribution","text":"# initialize HistogramClass nbins = 1_000 #int(np.sqrt(n_samples)) support_extension = 20 alpha = 1e-16 marg_hist_clf = ScipyHistogramUniformization ( nbins = nbins , support_extension = support_extension , alpha = alpha ) # fit to data marg_hist_clf . fit ( X ) ScipyHistogramUniformization(alpha=1e-16, kwargs={}, nbins=1000, support_extension=20)","title":"Fit Model to Distribution"},{"location":"demos/demo_1_marginal_uniformization/#transform-data","text":"# transform data Xu = marg_hist_clf . transform ( X ) sns . distplot ( Xu , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45db126a0>","title":"Transform Data"},{"location":"demos/demo_1_marginal_uniformization/#inverse-transform-data","text":"# transform data X_approx = marg_hist_clf . inverse_transform ( Xu ) sns . distplot ( X_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45daa8ee0>","title":"Inverse Transform Data"},{"location":"demos/demo_1_marginal_uniformization/#sampling","text":"# transform data X_samples = marg_hist_clf . sample ( 10_000 , seed ) sns . distplot ( X_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45d9945b0>","title":"Sampling"},{"location":"demos/demo_1_marginal_uniformization/#jacobian-log-absolute-determinant","text":"# estimated log probability of dataset x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) -5.535316543523011 -1.2726366664816962 10 * np . spacing ( 1 ) 2.220446049250313e-15 print ( f \"Min: { X . min () } , Max: { X . max () } \" ) X_small_samples = data_dist . rvs ( size = ( 10 , 1 ), random_state = seed ) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples ) print ( x_slogdet . min (), x_slogdet . max ()) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples - 100 ) print ( x_slogdet . min (), x_slogdet . max (), marg_hist_clf . marginal_transforms_ [ 0 ] . b ) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples + 100 ) print ( x_slogdet . min (), x_slogdet . max ()) Min: 0.16885335848364358, Max: 18.275140139054425 -3.1374212707246407 -1.3158088383469049 -42.376678031427744 -42.376678031427744 21.896397495168582 -42.376678031427744 -42.376678031427744","title":"Jacobian (Log Absolute Determinant)"},{"location":"demos/demo_1_marginal_uniformization/#log-probability","text":"# calculate real log probability of distribution x_prob = marg_hist_clf . score_samples ( X ) data_prob = data_dist . logpdf ( X ) sns . distplot ( data_prob , color = 'black' , label = 'Dataset' ) sns . distplot ( x_prob , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7ff45d8d9c70>","title":"Log Probability"},{"location":"demos/demo_1_marginal_uniformization/#negative-log-likelihood","text":"x_score = marg_hist_clf . score_samples ( X ) . mean () data_score = data_dist . logpdf ( X ) . mean () print ( x_score , data_score ) -1.994833104562771 -2.023673498524285","title":"Negative Log-Likelihood"},{"location":"demos/demo_1_marginal_uniformization/#method-ii-quantile-method-w-pdf-estimator","text":"# KDE parameters n_quantiles = 1_000 support_extension = 10 bw_estimator = 'scott' # initialize kde uniformization class # marg_kde_clf = ScipyKDEUniformization( # n_quantiles=n_quantiles, # support_extension=support_extension, # bw_estimator=bw_estimator # ) marg_kde_clf = SklearnKDEUniformization ( n_quantiles = n_quantiles , support_extension = support_extension , ) SklearnKDEUniformization # fit to data marg_kde_clf . fit ( X ) SklearnKDEUniformization(algorithm='kd_tree', kernel='gaussian', kwargs={}, metric='euclidean', n_quantiles=1000, support_extension=10)","title":"Method II - Quantile Method w/ PDF Estimator"},{"location":"demos/demo_1_marginal_uniformization/#transform-data_1","text":"# transform data Xu = marg_kde_clf . transform ( X ) sns . distplot ( Xu , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c197be0>","title":"Transform Data"},{"location":"demos/demo_1_marginal_uniformization/#inverse-transformation","text":"# transform data X_approx = marg_kde_clf . inverse_transform ( Xu ) sns . distplot ( X_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4de4f0>","title":"Inverse Transformation"},{"location":"demos/demo_1_marginal_uniformization/#sampling_1","text":"# transform data X_samples = marg_kde_clf . sample ( 10_000 , seed ) sns . distplot ( X_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4de910>","title":"Sampling"},{"location":"demos/demo_1_marginal_uniformization/#jacobian-log-absolute-determinant_1","text":"# estimated log probability of dataset x_slogdet = marg_kde_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) -9.994573000232851 -1.645619827763175","title":"Jacobian (Log Absolute Determinant)"},{"location":"demos/demo_1_marginal_uniformization/#log-probability_1","text":"# calculate real log probability of distribution x_prob = marg_kde_clf . score_samples ( X ) data_prob = data_dist . logpdf ( X ) sns . distplot ( data_prob , color = 'black' , label = 'Dataset' ) sns . distplot ( x_prob , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7ff45c01a0d0>","title":"Log Probability"},{"location":"demos/demo_1_marginal_uniformization/#negative-log-likelihood_1","text":"x_score = marg_kde_clf . score_samples ( X ) . mean () data_score = data_dist . logpdf ( X ) . mean () print ( x_score , data_score ) -2.0241925324818038 -2.023673498524285","title":"Negative Log-Likelihood"},{"location":"demos/demo_innf/","text":"Demo: Gaussianization \u00b6 Data \u00b6 RBIG Model \u00b6 Initialize Model \u00b6 # rbig parameters n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # initialize RBIG Class rbig_clf = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base ) Fit Model to Data \u00b6 # run RBIG model rbig_clf . fit ( X ); Visualization \u00b6 1. Marginal Gaussianization \u00b6 # rotation matrix V (N x F) V = rbig_clf . rotation_matrix [ 0 ] # perform rotation data_marg_gauss = X @ V 2. Rotation \u00b6","title":"Demo: Gaussianization"},{"location":"demos/demo_innf/#demo-gaussianization","text":"","title":"Demo: Gaussianization"},{"location":"demos/demo_innf/#data","text":"","title":"Data"},{"location":"demos/demo_innf/#rbig-model","text":"","title":"RBIG Model"},{"location":"demos/demo_innf/#initialize-model","text":"# rbig parameters n_layers = 1 rotation_type = 'PCA' random_state = 123 zero_tolerance = 100 base = 'gauss' # initialize RBIG Class rbig_clf = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , base = base )","title":"Initialize Model"},{"location":"demos/demo_innf/#fit-model-to-data","text":"# run RBIG model rbig_clf . fit ( X );","title":"Fit Model to Data"},{"location":"demos/demo_innf/#visualization","text":"","title":"Visualization"},{"location":"demos/demo_innf/#1-marginal-gaussianization","text":"# rotation matrix V (N x F) V = rbig_clf . rotation_matrix [ 0 ] # perform rotation data_marg_gauss = X @ V","title":"1. Marginal Gaussianization"},{"location":"demos/demo_innf/#2-rotation","text":"","title":"2. Rotation"},{"location":"notebooks/Untitled/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) # RBIG Packages from rbig.data import ToyData from rbig.information.kde import KDESklearn from typing import Iterable , Optional , Dict , NamedTuple , Tuple , Union import numpy as np from sklearn.base import BaseEstimator , TransformerMixin import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 Data \u00b6 seed = 123 n_samples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) print ( f \"Entropy: { data_dist . entropy () : .4f } \" ) # plot distribution sns . distplot ( X ) Entropy: 2.0234 <matplotlib.axes._subplots.AxesSubplot at 0x7f52674788b0> Histogram Estimation \u00b6 from rbig.information.histogram import hist_entropy H_x = hist_entropy ( X ) % timeit _ = hist_entropy ( X ) print ( H_x ) 1.53 ms \u00b1 224 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 2.0237449741172817 Kernel Density Estimation \u00b6 from rbig.information.kde import kde_entropy_uni H_x = kde_entropy_uni ( X ) % timeit _ = kde_entropy_uni ( X ) print ( H_x ) 389 ms \u00b1 1.42 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) 2.0452385364514716 hpdf , hbins = np . histogram ( X , bins = 'auto' , range = ( X . min () - 1 , X . max () + 1 ), density = True ) hpdf = np . array ( hpdf , dtype = np . float64 ) hpdf += 1e-10 H_x = - ( hpdf * np . log ( hpdf )) . sum () print ( H_x ) 8.54157894566502","title":"Untitled"},{"location":"notebooks/Untitled/#data","text":"seed = 123 n_samples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) print ( f \"Entropy: { data_dist . entropy () : .4f } \" ) # plot distribution sns . distplot ( X ) Entropy: 2.0234 <matplotlib.axes._subplots.AxesSubplot at 0x7f52674788b0>","title":"Data"},{"location":"notebooks/Untitled/#histogram-estimation","text":"from rbig.information.histogram import hist_entropy H_x = hist_entropy ( X ) % timeit _ = hist_entropy ( X ) print ( H_x ) 1.53 ms \u00b1 224 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 2.0237449741172817","title":"Histogram Estimation"},{"location":"notebooks/Untitled/#kernel-density-estimation","text":"from rbig.information.kde import kde_entropy_uni H_x = kde_entropy_uni ( X ) % timeit _ = kde_entropy_uni ( X ) print ( H_x ) 389 ms \u00b1 1.42 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) 2.0452385364514716 hpdf , hbins = np . histogram ( X , bins = 'auto' , range = ( X . min () - 1 , X . max () + 1 ), density = True ) hpdf = np . array ( hpdf , dtype = np . float64 ) hpdf += 1e-10 H_x = - ( hpdf * np . log ( hpdf )) . sum () print ( H_x ) 8.54157894566502","title":"Kernel Density Estimation"},{"location":"notebooks/demo_1_marginal_uniformization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Marginal Uniformization \u00b6 import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) from rbig.data import ToyData from rbig.transform.histogram import ScipyHistogramUniformization from rbig.transform.kde import ScipyKDEUniformization , SklearnKDEUniformization import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Data \u00b6 seed = 123 n_samples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) # plot distribution sns . distplot ( X ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4ea160> Method I - Built-In Scipy Histogram Distribution \u00b6 Fit Model to Distribution \u00b6 # initialize HistogramClass nbins = 1_000 #int(np.sqrt(n_samples)) support_extension = 20 alpha = 1e-16 marg_hist_clf = ScipyHistogramUniformization ( nbins = nbins , support_extension = support_extension , alpha = alpha ) # fit to data marg_hist_clf . fit ( X ) ScipyHistogramUniformization(alpha=1e-16, kwargs={}, nbins=1000, support_extension=20) Transform Data \u00b6 # transform data Xu = marg_hist_clf . transform ( X ) sns . distplot ( Xu , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45db126a0> Inverse Transform Data \u00b6 # transform data X_approx = marg_hist_clf . inverse_transform ( Xu ) sns . distplot ( X_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45daa8ee0> Sampling \u00b6 # transform data X_samples = marg_hist_clf . sample ( 10_000 , seed ) sns . distplot ( X_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45d9945b0> Jacobian (Log Absolute Determinant) \u00b6 # estimated log probability of dataset x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) -5.535316543523011 -1.2726366664816962 10 * np . spacing ( 1 ) 2.220446049250313e-15 print ( f \"Min: { X . min () } , Max: { X . max () } \" ) X_small_samples = data_dist . rvs ( size = ( 10 , 1 ), random_state = seed ) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples ) print ( x_slogdet . min (), x_slogdet . max ()) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples - 100 ) print ( x_slogdet . min (), x_slogdet . max (), marg_hist_clf . marginal_transforms_ [ 0 ] . b ) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples + 100 ) print ( x_slogdet . min (), x_slogdet . max ()) Min: 0.16885335848364358, Max: 18.275140139054425 -3.1374212707246407 -1.3158088383469049 -42.376678031427744 -42.376678031427744 21.896397495168582 -42.376678031427744 -42.376678031427744 Log Probability \u00b6 # calculate real log probability of distribution x_prob = marg_hist_clf . score_samples ( X ) data_prob = data_dist . logpdf ( X ) sns . distplot ( data_prob , color = 'black' , label = 'Dataset' ) sns . distplot ( x_prob , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7ff45d8d9c70> Negative Log-Likelihood \u00b6 x_score = marg_hist_clf . score_samples ( X ) . mean () data_score = data_dist . logpdf ( X ) . mean () print ( x_score , data_score ) -1.994833104562771 -2.023673498524285 Method II - Quantile Method w/ PDF Estimator \u00b6 # KDE parameters n_quantiles = 1_000 support_extension = 10 bw_estimator = 'scott' # initialize kde uniformization class # marg_kde_clf = ScipyKDEUniformization( # n_quantiles=n_quantiles, # support_extension=support_extension, # bw_estimator=bw_estimator # ) marg_kde_clf = SklearnKDEUniformization ( n_quantiles = n_quantiles , support_extension = support_extension , ) SklearnKDEUniformization # fit to data marg_kde_clf . fit ( X ) SklearnKDEUniformization(algorithm='kd_tree', kernel='gaussian', kwargs={}, metric='euclidean', n_quantiles=1000, support_extension=10) Transform Data \u00b6 # transform data Xu = marg_kde_clf . transform ( X ) sns . distplot ( Xu , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c197be0> Inverse Transformation \u00b6 # transform data X_approx = marg_kde_clf . inverse_transform ( Xu ) sns . distplot ( X_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4de4f0> Sampling \u00b6 # transform data X_samples = marg_kde_clf . sample ( 10_000 , seed ) sns . distplot ( X_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4de910> Jacobian (Log Absolute Determinant) \u00b6 # estimated log probability of dataset x_slogdet = marg_kde_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) -9.994573000232851 -1.645619827763175 Log Probability \u00b6 # calculate real log probability of distribution x_prob = marg_kde_clf . score_samples ( X ) data_prob = data_dist . logpdf ( X ) sns . distplot ( data_prob , color = 'black' , label = 'Dataset' ) sns . distplot ( x_prob , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7ff45c01a0d0> Negative Log-Likelihood \u00b6 x_score = marg_kde_clf . score_samples ( X ) . mean () data_score = data_dist . logpdf ( X ) . mean () print ( x_score , data_score ) -2.0241925324818038 -2.023673498524285","title":"Demo 1 marginal uniformization"},{"location":"notebooks/demo_1_marginal_uniformization/#marginal-uniformization","text":"import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) from rbig.data import ToyData from rbig.transform.histogram import ScipyHistogramUniformization from rbig.transform.kde import ScipyKDEUniformization , SklearnKDEUniformization import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Marginal Uniformization"},{"location":"notebooks/demo_1_marginal_uniformization/#data","text":"seed = 123 n_samples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) # plot distribution sns . distplot ( X ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4ea160>","title":"Data"},{"location":"notebooks/demo_1_marginal_uniformization/#method-i-built-in-scipy-histogram-distribution","text":"","title":"Method I - Built-In Scipy Histogram Distribution"},{"location":"notebooks/demo_1_marginal_uniformization/#fit-model-to-distribution","text":"# initialize HistogramClass nbins = 1_000 #int(np.sqrt(n_samples)) support_extension = 20 alpha = 1e-16 marg_hist_clf = ScipyHistogramUniformization ( nbins = nbins , support_extension = support_extension , alpha = alpha ) # fit to data marg_hist_clf . fit ( X ) ScipyHistogramUniformization(alpha=1e-16, kwargs={}, nbins=1000, support_extension=20)","title":"Fit Model to Distribution"},{"location":"notebooks/demo_1_marginal_uniformization/#transform-data","text":"# transform data Xu = marg_hist_clf . transform ( X ) sns . distplot ( Xu , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45db126a0>","title":"Transform Data"},{"location":"notebooks/demo_1_marginal_uniformization/#inverse-transform-data","text":"# transform data X_approx = marg_hist_clf . inverse_transform ( Xu ) sns . distplot ( X_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45daa8ee0>","title":"Inverse Transform Data"},{"location":"notebooks/demo_1_marginal_uniformization/#sampling","text":"# transform data X_samples = marg_hist_clf . sample ( 10_000 , seed ) sns . distplot ( X_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45d9945b0>","title":"Sampling"},{"location":"notebooks/demo_1_marginal_uniformization/#jacobian-log-absolute-determinant","text":"# estimated log probability of dataset x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) -5.535316543523011 -1.2726366664816962 10 * np . spacing ( 1 ) 2.220446049250313e-15 print ( f \"Min: { X . min () } , Max: { X . max () } \" ) X_small_samples = data_dist . rvs ( size = ( 10 , 1 ), random_state = seed ) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples ) print ( x_slogdet . min (), x_slogdet . max ()) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples - 100 ) print ( x_slogdet . min (), x_slogdet . max (), marg_hist_clf . marginal_transforms_ [ 0 ] . b ) # check bounds (lower bounds) x_slogdet = marg_hist_clf . log_abs_det_jacobian ( X_small_samples + 100 ) print ( x_slogdet . min (), x_slogdet . max ()) Min: 0.16885335848364358, Max: 18.275140139054425 -3.1374212707246407 -1.3158088383469049 -42.376678031427744 -42.376678031427744 21.896397495168582 -42.376678031427744 -42.376678031427744","title":"Jacobian (Log Absolute Determinant)"},{"location":"notebooks/demo_1_marginal_uniformization/#log-probability","text":"# calculate real log probability of distribution x_prob = marg_hist_clf . score_samples ( X ) data_prob = data_dist . logpdf ( X ) sns . distplot ( data_prob , color = 'black' , label = 'Dataset' ) sns . distplot ( x_prob , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7ff45d8d9c70>","title":"Log Probability"},{"location":"notebooks/demo_1_marginal_uniformization/#negative-log-likelihood","text":"x_score = marg_hist_clf . score_samples ( X ) . mean () data_score = data_dist . logpdf ( X ) . mean () print ( x_score , data_score ) -1.994833104562771 -2.023673498524285","title":"Negative Log-Likelihood"},{"location":"notebooks/demo_1_marginal_uniformization/#method-ii-quantile-method-w-pdf-estimator","text":"# KDE parameters n_quantiles = 1_000 support_extension = 10 bw_estimator = 'scott' # initialize kde uniformization class # marg_kde_clf = ScipyKDEUniformization( # n_quantiles=n_quantiles, # support_extension=support_extension, # bw_estimator=bw_estimator # ) marg_kde_clf = SklearnKDEUniformization ( n_quantiles = n_quantiles , support_extension = support_extension , ) SklearnKDEUniformization # fit to data marg_kde_clf . fit ( X ) SklearnKDEUniformization(algorithm='kd_tree', kernel='gaussian', kwargs={}, metric='euclidean', n_quantiles=1000, support_extension=10)","title":"Method II - Quantile Method w/ PDF Estimator"},{"location":"notebooks/demo_1_marginal_uniformization/#transform-data_1","text":"# transform data Xu = marg_kde_clf . transform ( X ) sns . distplot ( Xu , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c197be0>","title":"Transform Data"},{"location":"notebooks/demo_1_marginal_uniformization/#inverse-transformation","text":"# transform data X_approx = marg_kde_clf . inverse_transform ( Xu ) sns . distplot ( X_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4de4f0>","title":"Inverse Transformation"},{"location":"notebooks/demo_1_marginal_uniformization/#sampling_1","text":"# transform data X_samples = marg_kde_clf . sample ( 10_000 , seed ) sns . distplot ( X_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7ff45c4de910>","title":"Sampling"},{"location":"notebooks/demo_1_marginal_uniformization/#jacobian-log-absolute-determinant_1","text":"# estimated log probability of dataset x_slogdet = marg_kde_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) -9.994573000232851 -1.645619827763175","title":"Jacobian (Log Absolute Determinant)"},{"location":"notebooks/demo_1_marginal_uniformization/#log-probability_1","text":"# calculate real log probability of distribution x_prob = marg_kde_clf . score_samples ( X ) data_prob = data_dist . logpdf ( X ) sns . distplot ( data_prob , color = 'black' , label = 'Dataset' ) sns . distplot ( x_prob , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7ff45c01a0d0>","title":"Log Probability"},{"location":"notebooks/demo_1_marginal_uniformization/#negative-log-likelihood_1","text":"x_score = marg_kde_clf . score_samples ( X ) . mean () data_score = data_dist . logpdf ( X ) . mean () print ( x_score , data_score ) -2.0241925324818038 -2.023673498524285","title":"Negative Log-Likelihood"},{"location":"notebooks/demo_2_icdf_gaussuan/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Inverse CDF Gaussian \u00b6 import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) from rbig.data import ToyData from rbig.transform import InverseGaussCDF import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Data \u00b6 seed = 123 n_samples = 10_000 # a = 4 # initialize data distribution data_dist = stats . uniform () # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) # plot distribution sns . distplot ( X ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb8f24447c0> Method I - Built-In Scipy Histogram Distribution \u00b6 Fit Model to Distribution \u00b6 # initialize HistogramClass marg_icdf_clf = InverseGaussCDF () # fit to data marg_icdf_clf . fit ( X ) InverseGaussCDF() Transform Data \u00b6 # transform data Xg = marg_icdf_clf . transform ( X ) sns . distplot ( Xg , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb8f2369130> Inverse Transform Data \u00b6 # transform data Xu_approx = marg_icdf_clf . inverse_transform ( Xg ) sns . distplot ( Xu_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb8f22ad370> Sampling \u00b6 # transform data Xu_samples = marg_icdf_clf . sample ( 10_000 , seed ) sns . distplot ( Xu_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb8f21b44f0> Jacobian (Log Absolute Determinant) \u00b6 # estimated log probability of dataset x_slogdet = marg_icdf_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) 0.9189385343078806 8.199560119161166","title":"Demo 2 icdf gaussuan"},{"location":"notebooks/demo_2_icdf_gaussuan/#inverse-cdf-gaussian","text":"import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) from rbig.data import ToyData from rbig.transform import InverseGaussCDF import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Inverse CDF Gaussian"},{"location":"notebooks/demo_2_icdf_gaussuan/#data","text":"seed = 123 n_samples = 10_000 # a = 4 # initialize data distribution data_dist = stats . uniform () # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) # plot distribution sns . distplot ( X ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb8f24447c0>","title":"Data"},{"location":"notebooks/demo_2_icdf_gaussuan/#method-i-built-in-scipy-histogram-distribution","text":"","title":"Method I - Built-In Scipy Histogram Distribution"},{"location":"notebooks/demo_2_icdf_gaussuan/#fit-model-to-distribution","text":"# initialize HistogramClass marg_icdf_clf = InverseGaussCDF () # fit to data marg_icdf_clf . fit ( X ) InverseGaussCDF()","title":"Fit Model to Distribution"},{"location":"notebooks/demo_2_icdf_gaussuan/#transform-data","text":"# transform data Xg = marg_icdf_clf . transform ( X ) sns . distplot ( Xg , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb8f2369130>","title":"Transform Data"},{"location":"notebooks/demo_2_icdf_gaussuan/#inverse-transform-data","text":"# transform data Xu_approx = marg_icdf_clf . inverse_transform ( Xg ) sns . distplot ( Xu_approx , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb8f22ad370>","title":"Inverse Transform Data"},{"location":"notebooks/demo_2_icdf_gaussuan/#sampling","text":"# transform data Xu_samples = marg_icdf_clf . sample ( 10_000 , seed ) sns . distplot ( Xu_samples , color = 'gray' ) <matplotlib.axes._subplots.AxesSubplot at 0x7fb8f21b44f0>","title":"Sampling"},{"location":"notebooks/demo_2_icdf_gaussuan/#jacobian-log-absolute-determinant","text":"# estimated log probability of dataset x_slogdet = marg_icdf_clf . log_abs_det_jacobian ( X ) print ( x_slogdet . min (), x_slogdet . max ()) 0.9189385343078806 8.199560119161166","title":"Jacobian (Log Absolute Determinant)"},{"location":"notebooks/demo_3_marginal_gaussianization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Marginal Gaussianization \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com In this demonstration, we will show how we can do the marginal Gaussianization on a 2D dataset using the Histogram transformation and Inverse CDF Gaussian distribution. import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) from rbig.data import ToyData from rbig.transform.gaussianization import MarginalGaussianization # from rbig.transform.gaussianization import HistogramGaussianization, KDEGaussianization from rbig.transform import InverseGaussCDF import numpy as np from scipy import stats # Plot Functions import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline % load_ext autoreload % autoreload 2 Data \u00b6 For this example, we are looking at a 2D dataset. def plot_2d_joint ( data , color = 'blue' , title = 'Original Data' ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = color ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . suptitle ( title ) plt . tight_layout () plt . show () def plot_prob ( data , probs , title = 'Probabilities' ): fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = probs , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( title ) plt . show () seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig' n_samples = 10_000 n_features = 2 noise = 0.25 random_state = 1 clusters = 2 data = ToyData ( dataset = dataset , n_samples = n_samples , n_features = n_features , noise = noise , random_state = random_state , clusters = clusters , ) . generate_samples () X = data [:, 0 ] Y = data [:, 1 ] plot_2d_joint ( data , title = 'Original Data' ) <Figure size 360x360 with 0 Axes> Uniformization Transformation \u00b6 from rbig.transform.uniformization import HistogramUniformization , KDEUniformization , MarginalUniformization # from rbig.density.histogram import ScipyHistogram, QuantileHistogram # from rbig.den Initialize Uniformization Algorithm \u00b6 # INITIALIZE UNIFORMIZATION ALGORITHM #=== # uniform_clf = HistogramUniformization(bins=100, support_extension=10, alpha=1e-4, n_quantiles=None) uniform_clf = KDEUniformization ( n_quantiles = 50 , method = 'fft' ) # density_clf = KDEScipy(n_quantiles=50, bw_method='scott', support_extension=10) # density_clf = KDESklearn(n_quantiles=100, support_extension=10) Add it to Marginal Transformation Algorithm \u00b6 mg_uniformizer = MarginalUniformization ( uniform_clf ) mg_uniformizer . fit ( data ) MarginalUniformization(uni_transformer=KDEUniformization(algorithm='kd_tree', bw_method='scott', kernel='gaussian', kwargs={}, method='fft', metric='euclidean', n_quantiles=50, support_extension=10)) X_trans = mg_uniformizer . transform ( data ) plot_2d_joint ( X_trans , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> data_approx = mg_uniformizer . inverse_transform ( X_trans ) plot_2d_joint ( data_approx , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> X_ldj = mg_uniformizer . log_abs_det_jacobian ( data ) plot_2d_joint ( X_ldj , title = 'Transformed Data' ) plot_2d_joint ( np . exp ( X_ldj ), title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> plot_prob ( data , X_ldj . sum ( - 1 ), title = 'Log Probabilities' ) plot_prob ( data , np . exp ( X_ldj . sum ( - 1 )), title = 'Probabilities' ) Marginal Gaussinization \u00b6 from rbig.transform.uniformization import HistogramUniformization , KDEUniformization , MarginalUniformization from rbig.transform.gaussianization import MarginalGaussianization uniform_clf = HistogramUniformization ( bins = 100 , support_extension = 10 , alpha = 1e-4 , n_quantiles = None ) uniform_clf = KDEUniformization ( n_quantiles = 50 , method = 'fft' , ) mg_gaussianizer = MarginalGaussianization ( uniform_clf ) mg_gaussianizer . fit ( data ) MarginalGaussianization(uni_transformer=KDEUniformization(algorithm='kd_tree', bw_method='scott', kernel='gaussian', kwargs={}, method='fft', metric='euclidean', n_quantiles=50, support_extension=10)) X_trans = mg_gaussianizer . transform ( data ) plot_2d_joint ( X_trans , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> data_approx = mg_gaussianizer . inverse_transform ( X_trans ) plot_2d_joint ( data_approx , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> X_ldj = mg_gaussianizer . log_abs_det_jacobian ( data ) plot_2d_joint ( X_ldj , title = 'Transformed Data' ) plot_2d_joint ( np . exp ( X_ldj ), title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> X_lprob = mg_gaussianizer . score_samples ( data ) plot_prob ( data , X_lprob , title = 'Log Probabilities' ) plot_prob ( data , np . exp ( X_lprob ), title = 'Probabilities' ) Negative Log Likelihood \u00b6 X_nll = mg_gaussianizer . score ( data ,) print ( f \"Negative Log-Likelihood Score: { X_nll : .4f } \" ) Negative Log-Likelihood Score: -2.8415 Marginal Histogram Transformation \u00b6 So, for this transformation, we are going to transform our data from the current distribution to a marginally Gaussian distribution and then perform a rotation. In theory, if we do enough of these, we will eventually convert to a Gaussian distribution. # parameters nbins = 1_000 # number of bins to do the histogram transform alpha = 1e-05 # adds some regularization (noise) support_extension = 10 # initialize the transformer mg_transformer = HistogramGaussianization ( nbins = nbins , alpha = alpha ) # fit the transformer to the data mg_transformer . fit ( data ); 1. Forward Transformation \u00b6 For this transformation, we will be applying the following: \\Psi(\\mathbf{x}) = \\Phi^{-1}(\\mathbf{x}) \\Psi(\\mathbf{x}) = \\Phi^{-1}(\\mathbf{x}) where \\Phi^{-1}(\\cdot) \\Phi^{-1}(\\cdot) is the inverse CDF of the Gaussian distribution. data_trans = mg_transformer . transform ( data ) plot_2d_joint ( data_trans , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> So clearly we can see that the transformation works. Both of the marginals are Gaussian distributed.. 2. Inverse Transformation \u00b6 For this step, we will apply the inverse transformation: \\Psi^{-1}(\\mathbf{x}) = \\Phi \\left( \\mathbf{x} \\right) \\Psi^{-1}(\\mathbf{x}) = \\Phi \\left( \\mathbf{x} \\right) where \\Phi(\\cdot) \\Phi(\\cdot) is the CDF of the Gaussian distribution. data_approx = mg_transformer . inverse_transform ( data_trans ) # check that its more or less equal np . testing . assert_array_almost_equal ( data_approx , data , decimal = 1e-5 ) We see that this transformation is very close to the original. In fact, it's close to approximately 1e-5 decimal places. The errors will definitely stem from the boundaries. # Plot results plot_2d_joint ( data_approx , title = 'Inverse Transformed Data' ) <Figure size 360x360 with 0 Axes> Log Absolute Determinant Jacobian \u00b6 Using the derivative of inverse-functions theorem, we can calculate the derivative like so: \\nabla_\\mathbf{x} \\Phi^{-1}(\\mathbf{x}) = \\frac{1}{\\phi (\\Phi^{-1} (x)) } \\nabla_\\mathbf{x} \\Phi^{-1}(\\mathbf{x}) = \\frac{1}{\\phi (\\Phi^{-1} (x)) } where \\phi(\\cdot) \\phi(\\cdot) is the PDF of the Gaussian distribution. Taking the log of these terms gives us: \\log \\nabla_\\mathbf{x} \\Phi^{-1}(\\mathbf{x}) = - \\log \\phi (\\Phi^{-1} (x)) \\log \\nabla_\\mathbf{x} \\Phi^{-1}(\\mathbf{x}) = - \\log \\phi (\\Phi^{-1} (x)) X_slogdet = mg_transformer . log_abs_det_jacobian ( data ) print ( X_slogdet . min (), X_slogdet . max ()) print ( np . exp ( X_slogdet ) . min (), np . exp ( X_slogdet ) . max ()) (-2.459695595900522, 5.0787933667391965) (0.08546096167531662, 160.5801775987935) # plot the gradients plot_2d_joint ( np . exp ( X_slogdet ), title = 'Jacobian Data' ) <Figure size 360x360 with 0 Axes> Log Probability \u00b6 \\log p_\\theta(\\mathbf{x}) = \\log p_\\theta \\left( \\mathbf{z} \\right) + \\log \\left| \\nabla_\\mathbf{x} \\mathbf{z} \\right| \\log p_\\theta(\\mathbf{x}) = \\log p_\\theta \\left( \\mathbf{z} \\right) + \\log \\left| \\nabla_\\mathbf{x} \\mathbf{z} \\right| where \\mathbf{z} = \\Psi(\\mathbf{x}) \\mathbf{z} = \\Psi(\\mathbf{x}) # score samples log_prob = mg_transformer . score_samples ( data ) # score samples log_prob = mg_transformer . score_samples ( data ) plot_prob ( data , log_prob , title = 'Log Probabilities' ) Probability \u00b6 This is the same as above but without the log scale: p_\\theta(\\mathbf{x}) = p_\\theta \\left( \\mathbf{z} \\right) \\left| \\nabla_\\mathbf{x} \\mathbf{z} \\right| p_\\theta(\\mathbf{x}) = p_\\theta \\left( \\mathbf{z} \\right) \\left| \\nabla_\\mathbf{x} \\mathbf{z} \\right| where \\mathbf{z} = \\Psi(\\mathbf{x}) \\mathbf{z} = \\Psi(\\mathbf{x}) plot_prob ( data , np . exp ( log_prob ), title = 'Probabilities' ) Negative Log-Likelihood \u00b6 We need to take the expected value (mean) of all log probabilities. \\text{nll} = \\frac{1}{N} \\sum_{n=1}^{N} \\log p_\\theta(\\mathbf{x}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^{N} \\log p_\\theta(\\mathbf{x}) <span><span class=\"MathJax_Preview\">\\text{nll} = \\frac{1}{N} \\sum_{n=1}^{N} \\log p_\\theta(\\mathbf{x})</span><script type=\"math/tex\">\\text{nll} = \\frac{1}{N} \\sum_{n=1}^{N} \\log p_\\theta(\\mathbf{x}) score = mg_transformer . score ( data ) print ( f \"Negative Log-Likelihood Score: { score : .4f } \" ) Negative Log-Likelihood Score: -2.0724","title":"Demo 3 marginal gaussianization"},{"location":"notebooks/demo_3_marginal_gaussianization/#marginal-gaussianization","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com In this demonstration, we will show how we can do the marginal Gaussianization on a 2D dataset using the Histogram transformation and Inverse CDF Gaussian distribution. import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) from rbig.data import ToyData from rbig.transform.gaussianization import MarginalGaussianization # from rbig.transform.gaussianization import HistogramGaussianization, KDEGaussianization from rbig.transform import InverseGaussCDF import numpy as np from scipy import stats # Plot Functions import matplotlib.pyplot as plt import seaborn as sns sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline % load_ext autoreload % autoreload 2","title":"Marginal Gaussianization"},{"location":"notebooks/demo_3_marginal_gaussianization/#data","text":"For this example, we are looking at a 2D dataset. def plot_2d_joint ( data , color = 'blue' , title = 'Original Data' ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = color ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . suptitle ( title ) plt . tight_layout () plt . show () def plot_prob ( data , probs , title = 'Probabilities' ): fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = probs , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( title ) plt . show () seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig' n_samples = 10_000 n_features = 2 noise = 0.25 random_state = 1 clusters = 2 data = ToyData ( dataset = dataset , n_samples = n_samples , n_features = n_features , noise = noise , random_state = random_state , clusters = clusters , ) . generate_samples () X = data [:, 0 ] Y = data [:, 1 ] plot_2d_joint ( data , title = 'Original Data' ) <Figure size 360x360 with 0 Axes>","title":"Data"},{"location":"notebooks/demo_3_marginal_gaussianization/#uniformization-transformation","text":"from rbig.transform.uniformization import HistogramUniformization , KDEUniformization , MarginalUniformization # from rbig.density.histogram import ScipyHistogram, QuantileHistogram # from rbig.den","title":"Uniformization Transformation"},{"location":"notebooks/demo_3_marginal_gaussianization/#initialize-uniformization-algorithm","text":"# INITIALIZE UNIFORMIZATION ALGORITHM #=== # uniform_clf = HistogramUniformization(bins=100, support_extension=10, alpha=1e-4, n_quantiles=None) uniform_clf = KDEUniformization ( n_quantiles = 50 , method = 'fft' ) # density_clf = KDEScipy(n_quantiles=50, bw_method='scott', support_extension=10) # density_clf = KDESklearn(n_quantiles=100, support_extension=10)","title":"Initialize Uniformization Algorithm"},{"location":"notebooks/demo_3_marginal_gaussianization/#add-it-to-marginal-transformation-algorithm","text":"mg_uniformizer = MarginalUniformization ( uniform_clf ) mg_uniformizer . fit ( data ) MarginalUniformization(uni_transformer=KDEUniformization(algorithm='kd_tree', bw_method='scott', kernel='gaussian', kwargs={}, method='fft', metric='euclidean', n_quantiles=50, support_extension=10)) X_trans = mg_uniformizer . transform ( data ) plot_2d_joint ( X_trans , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> data_approx = mg_uniformizer . inverse_transform ( X_trans ) plot_2d_joint ( data_approx , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> X_ldj = mg_uniformizer . log_abs_det_jacobian ( data ) plot_2d_joint ( X_ldj , title = 'Transformed Data' ) plot_2d_joint ( np . exp ( X_ldj ), title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> plot_prob ( data , X_ldj . sum ( - 1 ), title = 'Log Probabilities' ) plot_prob ( data , np . exp ( X_ldj . sum ( - 1 )), title = 'Probabilities' )","title":"Add it to Marginal Transformation Algorithm"},{"location":"notebooks/demo_3_marginal_gaussianization/#marginal-gaussinization","text":"from rbig.transform.uniformization import HistogramUniformization , KDEUniformization , MarginalUniformization from rbig.transform.gaussianization import MarginalGaussianization uniform_clf = HistogramUniformization ( bins = 100 , support_extension = 10 , alpha = 1e-4 , n_quantiles = None ) uniform_clf = KDEUniformization ( n_quantiles = 50 , method = 'fft' , ) mg_gaussianizer = MarginalGaussianization ( uniform_clf ) mg_gaussianizer . fit ( data ) MarginalGaussianization(uni_transformer=KDEUniformization(algorithm='kd_tree', bw_method='scott', kernel='gaussian', kwargs={}, method='fft', metric='euclidean', n_quantiles=50, support_extension=10)) X_trans = mg_gaussianizer . transform ( data ) plot_2d_joint ( X_trans , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> data_approx = mg_gaussianizer . inverse_transform ( X_trans ) plot_2d_joint ( data_approx , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> X_ldj = mg_gaussianizer . log_abs_det_jacobian ( data ) plot_2d_joint ( X_ldj , title = 'Transformed Data' ) plot_2d_joint ( np . exp ( X_ldj ), title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> X_lprob = mg_gaussianizer . score_samples ( data ) plot_prob ( data , X_lprob , title = 'Log Probabilities' ) plot_prob ( data , np . exp ( X_lprob ), title = 'Probabilities' )","title":"Marginal Gaussinization"},{"location":"notebooks/demo_3_marginal_gaussianization/#negative-log-likelihood","text":"X_nll = mg_gaussianizer . score ( data ,) print ( f \"Negative Log-Likelihood Score: { X_nll : .4f } \" ) Negative Log-Likelihood Score: -2.8415","title":"Negative Log Likelihood"},{"location":"notebooks/demo_3_marginal_gaussianization/#marginal-histogram-transformation","text":"So, for this transformation, we are going to transform our data from the current distribution to a marginally Gaussian distribution and then perform a rotation. In theory, if we do enough of these, we will eventually convert to a Gaussian distribution. # parameters nbins = 1_000 # number of bins to do the histogram transform alpha = 1e-05 # adds some regularization (noise) support_extension = 10 # initialize the transformer mg_transformer = HistogramGaussianization ( nbins = nbins , alpha = alpha ) # fit the transformer to the data mg_transformer . fit ( data );","title":"Marginal Histogram Transformation"},{"location":"notebooks/demo_3_marginal_gaussianization/#1-forward-transformation","text":"For this transformation, we will be applying the following: \\Psi(\\mathbf{x}) = \\Phi^{-1}(\\mathbf{x}) \\Psi(\\mathbf{x}) = \\Phi^{-1}(\\mathbf{x}) where \\Phi^{-1}(\\cdot) \\Phi^{-1}(\\cdot) is the inverse CDF of the Gaussian distribution. data_trans = mg_transformer . transform ( data ) plot_2d_joint ( data_trans , title = 'Transformed Data' ) <Figure size 360x360 with 0 Axes> So clearly we can see that the transformation works. Both of the marginals are Gaussian distributed..","title":"1. Forward Transformation"},{"location":"notebooks/demo_3_marginal_gaussianization/#2-inverse-transformation","text":"For this step, we will apply the inverse transformation: \\Psi^{-1}(\\mathbf{x}) = \\Phi \\left( \\mathbf{x} \\right) \\Psi^{-1}(\\mathbf{x}) = \\Phi \\left( \\mathbf{x} \\right) where \\Phi(\\cdot) \\Phi(\\cdot) is the CDF of the Gaussian distribution. data_approx = mg_transformer . inverse_transform ( data_trans ) # check that its more or less equal np . testing . assert_array_almost_equal ( data_approx , data , decimal = 1e-5 ) We see that this transformation is very close to the original. In fact, it's close to approximately 1e-5 decimal places. The errors will definitely stem from the boundaries. # Plot results plot_2d_joint ( data_approx , title = 'Inverse Transformed Data' ) <Figure size 360x360 with 0 Axes>","title":"2. Inverse Transformation"},{"location":"notebooks/demo_3_marginal_gaussianization/#log-absolute-determinant-jacobian","text":"Using the derivative of inverse-functions theorem, we can calculate the derivative like so: \\nabla_\\mathbf{x} \\Phi^{-1}(\\mathbf{x}) = \\frac{1}{\\phi (\\Phi^{-1} (x)) } \\nabla_\\mathbf{x} \\Phi^{-1}(\\mathbf{x}) = \\frac{1}{\\phi (\\Phi^{-1} (x)) } where \\phi(\\cdot) \\phi(\\cdot) is the PDF of the Gaussian distribution. Taking the log of these terms gives us: \\log \\nabla_\\mathbf{x} \\Phi^{-1}(\\mathbf{x}) = - \\log \\phi (\\Phi^{-1} (x)) \\log \\nabla_\\mathbf{x} \\Phi^{-1}(\\mathbf{x}) = - \\log \\phi (\\Phi^{-1} (x)) X_slogdet = mg_transformer . log_abs_det_jacobian ( data ) print ( X_slogdet . min (), X_slogdet . max ()) print ( np . exp ( X_slogdet ) . min (), np . exp ( X_slogdet ) . max ()) (-2.459695595900522, 5.0787933667391965) (0.08546096167531662, 160.5801775987935) # plot the gradients plot_2d_joint ( np . exp ( X_slogdet ), title = 'Jacobian Data' ) <Figure size 360x360 with 0 Axes>","title":"Log Absolute Determinant Jacobian"},{"location":"notebooks/demo_3_marginal_gaussianization/#log-probability","text":"\\log p_\\theta(\\mathbf{x}) = \\log p_\\theta \\left( \\mathbf{z} \\right) + \\log \\left| \\nabla_\\mathbf{x} \\mathbf{z} \\right| \\log p_\\theta(\\mathbf{x}) = \\log p_\\theta \\left( \\mathbf{z} \\right) + \\log \\left| \\nabla_\\mathbf{x} \\mathbf{z} \\right| where \\mathbf{z} = \\Psi(\\mathbf{x}) \\mathbf{z} = \\Psi(\\mathbf{x}) # score samples log_prob = mg_transformer . score_samples ( data ) # score samples log_prob = mg_transformer . score_samples ( data ) plot_prob ( data , log_prob , title = 'Log Probabilities' )","title":"Log Probability"},{"location":"notebooks/demo_3_marginal_gaussianization/#probability","text":"This is the same as above but without the log scale: p_\\theta(\\mathbf{x}) = p_\\theta \\left( \\mathbf{z} \\right) \\left| \\nabla_\\mathbf{x} \\mathbf{z} \\right| p_\\theta(\\mathbf{x}) = p_\\theta \\left( \\mathbf{z} \\right) \\left| \\nabla_\\mathbf{x} \\mathbf{z} \\right| where \\mathbf{z} = \\Psi(\\mathbf{x}) \\mathbf{z} = \\Psi(\\mathbf{x}) plot_prob ( data , np . exp ( log_prob ), title = 'Probabilities' )","title":"Probability"},{"location":"notebooks/demo_3_marginal_gaussianization/#negative-log-likelihood_1","text":"We need to take the expected value (mean) of all log probabilities. \\text{nll} = \\frac{1}{N} \\sum_{n=1}^{N} \\log p_\\theta(\\mathbf{x}) \\text{nll} = \\frac{1}{N} \\sum_{n=1}^{N} \\log p_\\theta(\\mathbf{x}) <span><span class=\"MathJax_Preview\">\\text{nll} = \\frac{1}{N} \\sum_{n=1}^{N} \\log p_\\theta(\\mathbf{x})</span><script type=\"math/tex\">\\text{nll} = \\frac{1}{N} \\sum_{n=1}^{N} \\log p_\\theta(\\mathbf{x}) score = mg_transformer . score ( data ) print ( f \"Negative Log-Likelihood Score: { score : .4f } \" ) Negative Log-Likelihood Score: -2.0724","title":"Negative Log-Likelihood"},{"location":"notebooks/demo_4_rbig_model/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import sys from pyprojroot import here sys . path . append ( str ( here ())) # RBIG Packages from rbig.data import ToyData from rbig.layers import RBIGLayer from rbig.models import GaussianizationModel from typing import Iterable , Optional , Dict , NamedTuple , Tuple , Union import numpy as np import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 1. Marginal Uni 2. Inverse CDF Gauss 1: * Histogram * KDE * Mixture Dist 1 + 2: * Quantiles * ... Data \u00b6 def plot_2d_joint ( data , color = 'blue' , title = 'Original Data' ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = color ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . suptitle ( title ) plt . tight_layout () plt . show () def plot_prob ( data , probs , title = 'Probabilities' ): fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = probs , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( title ) plt . show () seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig' n_samples = 10_000 n_features = 2 noise = 0.25 random_state = 1 clusters = 2 data = ToyData ( dataset = dataset , n_samples = n_samples , n_features = n_features , noise = noise , random_state = random_state , clusters = clusters , ) . generate_samples () plot_2d_joint ( data , title = 'Data' ) <Figure size 360x360 with 0 Axes> RBIG Layers \u00b6 An RBIGLayer is a block represents the sequence of RBIG Transforms: Marginal Gaussianization Transformation Rotation Transformation You need to pick two components and then initialize an RBIG Layer. For this example, we'll show it using the Histogram transformation but any method within the package. Current methods available: Histogram KDE (Exact, FFT) Quantile # import RBIG Layer from rbig.layers.rbig_layer import RBIGLayer from rbig.transform.gaussianization import MarginalGaussianization from rbig.transform.uniformization import KDEUniformization , HistogramUniformization from rbig.transform.linear import OrthogonalTransform # Step 1 - Pick a Uniformization Transformer uniform_clf = HistogramUniformization ( bins = 100 , support_extension = 10 , alpha = 1e-4 , n_quantiles = None ) # uniform_clf = KDEUniformization(n_quantiles=50, method='fft', ) # Step 2 - Initialize Marginal Gaussianization Transformer mg_gaussianizer = MarginalGaussianization ( uniform_clf ) # Step 3 - Pick Rotation transformer orth_transform = OrthogonalTransform ( 'pca' ) # Step 4 - Initialize RBIG Block rbig_block = RBIGLayer ( mg_gaussianizer , orth_transform ) X_trans = rbig_block . transform ( data ) X_trans , dX_trans = rbig_block . transform ( data , return_jacobian = True ) plot_2d_joint ( X_trans , title = 'Transformed' ) plot_2d_joint ( dX_trans , title = 'Jacobian' ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> data_approx = rbig_block . inverse_transform ( X_trans ) plot_2d_joint ( data_approx , title = 'Data (approx)' ) <Figure size 360x360 with 0 Axes> 2. Multiple Layers \u00b6 When dealing with multiple layers, we need to initialize a GaussanizationModel with the following components: RBIGLayer as we have seen above StoppingCriteria As this is an iterative model, we need some sort of stopping criteria to ensure we converge. Without an appropriate stopping criteria, this method will not converge as it won't be able to distinguish between noise and information. So let's go over 2 easy stopping criteria that have already been implemented: MaxLayer - this simply states the maximum number of layers to use. InfoLoss - this calculates the difference in total correlation between layers from the original RBIG paper. we will do a demonstration using both methods. from rbig.stopping import InfoLoss , MaxLayers # Step 1 - Pick a Uniformization Transformer uniform_clf = HistogramUniformization ( bins = 100 , support_extension = 10 , alpha = 1e-4 , n_quantiles = None ) # uniform_clf = KDEUniformization(n_quantiles=50, method='fft', ) # Step 2 - Initialize Marginal Gaussianization Transformer mg_gaussianizer = MarginalGaussianization ( uniform_clf ) # Step 3 - Pick Rotation transformer orth_transform = OrthogonalTransform ( 'pca' ) # Step 4 - Initialize RBIG Block rbig_block = RBIGLayer ( mg_gaussianizer , orth_transform ) # Step 5 - Initialize loss function rbig_loss = MaxLayers ( n_layers = 50 ) # rbig_loss = InfoLoss(tol_layers=50) # Step 6 - Intialize Gaussianization Model rbig_model = GaussianizationModel ( rbig_block , rbig_loss ) # fit model to data rbig_model . fit ( data ); Z , X_slogdzet = rbig_model . fit_transform ( data ) Viz - Forward Transformation, Latent Space Z \u00b6 Z , X_slogdzet = rbig_model . transform ( data ) plot_2d_joint ( Z , title = 'Latent Space' ) X_priorprob = stats . norm () . logpdf ( Z ) plot_prob ( Z , X_priorprob . sum ( axis = 1 ), title = 'Latent Space (Log Prob)' ) plot_prob ( Z , np . exp ( X_priorprob . sum ( axis = 1 )), title = 'Latent Space (Prob)' ) <Figure size 360x360 with 0 Axes> Viz - Inverse Transformation, Data Space X \u00b6 data_approx = rbig_model . inverse_transform ( Z ) plot_2d_joint ( data_approx , title = 'Data Approx' ) <Figure size 360x360 with 0 Axes> Viz - Sampling, Data Space X \u00b6 data_approx = rbig_model . sample ( 10_000 ) plot_2d_joint ( data_approx , title = 'Data Approx' ) <Figure size 360x360 with 0 Axes> Viz - Probabilities, Data Space X \u00b6 X_prob = rbig_model . score_samples ( data ) plot_prob ( data , X_prob , title = 'Data (Log Prob)' ) plot_prob ( data , np . exp ( X_prob ), title = 'Data (Prob)' ) X_nll = rbig_model . score ( data , None ) print ( 'Negative Log-Likelihood:' , X_nll ) print ( 'Negative Likelihood:' , np . exp ( X_nll )) Negative Log-Likelihood: -36.36117768111992 Negative Likelihood: 1.6163714749339333e-16 X_nll = rbig_model . score ( data , None ) print ( 'Negative Log-Likelihood:' , X_nll ) print ( 'Negative Likelihood:' , np . exp ( X_nll )) Negative Log-Likelihood: -36.36117768111992 Negative Likelihood: 1.6163714749339333e-16 Viz - Negative Log-Likelihood \u00b6 plt . plot ( rbig_model . losses_ ) [<matplotlib.lines.Line2D at 0x7f37c8f3a190>]","title":"Demo 4 rbig model"},{"location":"notebooks/demo_4_rbig_model/#data","text":"def plot_2d_joint ( data , color = 'blue' , title = 'Original Data' ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = color ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . suptitle ( title ) plt . tight_layout () plt . show () def plot_prob ( data , probs , title = 'Probabilities' ): fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = probs , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( title ) plt . show () seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig' n_samples = 10_000 n_features = 2 noise = 0.25 random_state = 1 clusters = 2 data = ToyData ( dataset = dataset , n_samples = n_samples , n_features = n_features , noise = noise , random_state = random_state , clusters = clusters , ) . generate_samples () plot_2d_joint ( data , title = 'Data' ) <Figure size 360x360 with 0 Axes>","title":"Data"},{"location":"notebooks/demo_4_rbig_model/#rbig-layers","text":"An RBIGLayer is a block represents the sequence of RBIG Transforms: Marginal Gaussianization Transformation Rotation Transformation You need to pick two components and then initialize an RBIG Layer. For this example, we'll show it using the Histogram transformation but any method within the package. Current methods available: Histogram KDE (Exact, FFT) Quantile # import RBIG Layer from rbig.layers.rbig_layer import RBIGLayer from rbig.transform.gaussianization import MarginalGaussianization from rbig.transform.uniformization import KDEUniformization , HistogramUniformization from rbig.transform.linear import OrthogonalTransform # Step 1 - Pick a Uniformization Transformer uniform_clf = HistogramUniformization ( bins = 100 , support_extension = 10 , alpha = 1e-4 , n_quantiles = None ) # uniform_clf = KDEUniformization(n_quantiles=50, method='fft', ) # Step 2 - Initialize Marginal Gaussianization Transformer mg_gaussianizer = MarginalGaussianization ( uniform_clf ) # Step 3 - Pick Rotation transformer orth_transform = OrthogonalTransform ( 'pca' ) # Step 4 - Initialize RBIG Block rbig_block = RBIGLayer ( mg_gaussianizer , orth_transform ) X_trans = rbig_block . transform ( data ) X_trans , dX_trans = rbig_block . transform ( data , return_jacobian = True ) plot_2d_joint ( X_trans , title = 'Transformed' ) plot_2d_joint ( dX_trans , title = 'Jacobian' ) <Figure size 360x360 with 0 Axes> <Figure size 360x360 with 0 Axes> data_approx = rbig_block . inverse_transform ( X_trans ) plot_2d_joint ( data_approx , title = 'Data (approx)' ) <Figure size 360x360 with 0 Axes>","title":"RBIG Layers"},{"location":"notebooks/demo_4_rbig_model/#2-multiple-layers","text":"When dealing with multiple layers, we need to initialize a GaussanizationModel with the following components: RBIGLayer as we have seen above StoppingCriteria As this is an iterative model, we need some sort of stopping criteria to ensure we converge. Without an appropriate stopping criteria, this method will not converge as it won't be able to distinguish between noise and information. So let's go over 2 easy stopping criteria that have already been implemented: MaxLayer - this simply states the maximum number of layers to use. InfoLoss - this calculates the difference in total correlation between layers from the original RBIG paper. we will do a demonstration using both methods. from rbig.stopping import InfoLoss , MaxLayers # Step 1 - Pick a Uniformization Transformer uniform_clf = HistogramUniformization ( bins = 100 , support_extension = 10 , alpha = 1e-4 , n_quantiles = None ) # uniform_clf = KDEUniformization(n_quantiles=50, method='fft', ) # Step 2 - Initialize Marginal Gaussianization Transformer mg_gaussianizer = MarginalGaussianization ( uniform_clf ) # Step 3 - Pick Rotation transformer orth_transform = OrthogonalTransform ( 'pca' ) # Step 4 - Initialize RBIG Block rbig_block = RBIGLayer ( mg_gaussianizer , orth_transform ) # Step 5 - Initialize loss function rbig_loss = MaxLayers ( n_layers = 50 ) # rbig_loss = InfoLoss(tol_layers=50) # Step 6 - Intialize Gaussianization Model rbig_model = GaussianizationModel ( rbig_block , rbig_loss ) # fit model to data rbig_model . fit ( data ); Z , X_slogdzet = rbig_model . fit_transform ( data )","title":"2. Multiple Layers"},{"location":"notebooks/demo_4_rbig_model/#viz-forward-transformation-latent-space-z","text":"Z , X_slogdzet = rbig_model . transform ( data ) plot_2d_joint ( Z , title = 'Latent Space' ) X_priorprob = stats . norm () . logpdf ( Z ) plot_prob ( Z , X_priorprob . sum ( axis = 1 ), title = 'Latent Space (Log Prob)' ) plot_prob ( Z , np . exp ( X_priorprob . sum ( axis = 1 )), title = 'Latent Space (Prob)' ) <Figure size 360x360 with 0 Axes>","title":"Viz - Forward Transformation, Latent Space Z"},{"location":"notebooks/demo_4_rbig_model/#viz-inverse-transformation-data-space-x","text":"data_approx = rbig_model . inverse_transform ( Z ) plot_2d_joint ( data_approx , title = 'Data Approx' ) <Figure size 360x360 with 0 Axes>","title":"Viz - Inverse Transformation, Data Space X"},{"location":"notebooks/demo_4_rbig_model/#viz-sampling-data-space-x","text":"data_approx = rbig_model . sample ( 10_000 ) plot_2d_joint ( data_approx , title = 'Data Approx' ) <Figure size 360x360 with 0 Axes>","title":"Viz - Sampling, Data Space X"},{"location":"notebooks/demo_4_rbig_model/#viz-probabilities-data-space-x","text":"X_prob = rbig_model . score_samples ( data ) plot_prob ( data , X_prob , title = 'Data (Log Prob)' ) plot_prob ( data , np . exp ( X_prob ), title = 'Data (Prob)' ) X_nll = rbig_model . score ( data , None ) print ( 'Negative Log-Likelihood:' , X_nll ) print ( 'Negative Likelihood:' , np . exp ( X_nll )) Negative Log-Likelihood: -36.36117768111992 Negative Likelihood: 1.6163714749339333e-16 X_nll = rbig_model . score ( data , None ) print ( 'Negative Log-Likelihood:' , X_nll ) print ( 'Negative Likelihood:' , np . exp ( X_nll )) Negative Log-Likelihood: -36.36117768111992 Negative Likelihood: 1.6163714749339333e-16","title":"Viz - Probabilities, Data Space X"},{"location":"notebooks/demo_4_rbig_model/#viz-negative-log-likelihood","text":"plt . plot ( rbig_model . losses_ ) [<matplotlib.lines.Line2D at 0x7f37c8f3a190>]","title":"Viz - Negative Log-Likelihood"},{"location":"notebooks/demo_5_rbig_loss/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) # RBIG Packages from rbig.data import ToyData from rbig.layers import RBIGBlock , RBIGParams from rbig.models import GaussianizationModel from typing import Iterable , Optional , Dict , NamedTuple , Tuple , Union import numpy as np from sklearn.base import BaseEstimator , TransformerMixin import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 Data \u00b6 def plot_2d_joint ( data , color = 'blue' , title = 'Original Data' ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = color ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . suptitle ( title ) plt . tight_layout () plt . show () def plot_prob ( data , probs , title = 'Probabilities' ): fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = probs , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( title ) plt . show () seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig' n_samples = 10_000 n_features = 2 noise = 0.25 random_state = 1 clusters = 2 data = ToyData ( dataset = dataset , n_samples = n_samples , n_features = n_features , noise = noise , random_state = random_state , clusters = clusters , ) . generate_samples () plot_2d_joint ( data , title = 'Data' ) <Figure size 360x360 with 0 Axes> Options \u00b6 RBIG 1.1 - \"Stopping\" * Max # of layers * Information Loss * Negative Log Likelihood * Non-Gaussianity (?) * Difference in Non-Gaussianity? (norm(y) - norm(x) - log abs( dy/dx)) RBIG 2.0 - \"Loss\" * Negative Log Likelihood * Non-Gaussianity (?) RBIG 2.0 - Stopping : * Max Layers (epochs) * Information Loss(Xtrans, X) * Difference in Non-Gaussianity? (norm(y) - norm(x) - log abs( dy/dx)) Loss Function I - Maximum # layers \u00b6 from rbig.losses import MaxLayersLoss # stoppers from rbig.layers import RBIGParams # initialize params rbig_params = RBIGParams () # initialize rbig model rbig_loss = MaxLayersLoss ( n_layers = 50 ) rbig_model = GaussianizationModel ( rbig_params , rbig_loss ) # transform data to latent space Z , _ = rbig_model . fit_transform ( data ) # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( Z , title = 'Data' ) <Figure size 360x360 with 0 Axes> Loss Function I.I - Custom Loss Function \u00b6 def negative_log_likelihood ( Z , X , X_slogdet ): # calculate the log probability in latent space Z_logprob = stats . norm () . logpdf ( Z ) # calculate the probability of the transform X_logprob = Z_logprob . sum ( axis = 1 ) + X_slogdet . sum ( axis = 1 ) # return the nll return np . mean ( X_logprob ) def difference_nongaussian ( Z , X , X_slogdet ): delta_ng = . 5 * np . linalg . norm ( Z , 2 ) ** 2 - . 5 * np . linalg . norm ( X , 2 ) ** 2 - X_slogdet . sum ( axis = 1 ) return np . mean ( delta_ng ) # initialize params rbig_params = RBIGParams () # initialize rbig model rbig_loss = MaxLayersLoss ( n_layers = 50 , loss_func = difference_nongaussian ) rbig_model = GaussianizationModel ( rbig_params , rbig_loss ) # transform data to latent space Z , _ = rbig_model . fit_transform ( data ) # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( Z , title = 'Data' ) <Figure size 360x360 with 0 Axes> Loss II - Information Reduction \u00b6 from rbig.losses import InformationLoss # initialize params rbig_params = RBIGParams ( nbins = 500 ) # initialize the RBIG Loss function rbig_loss = InformationLoss ( tol_layers = 70 , method = 'histogram' ) # initialize rbig model rbig_model = GaussianizationModel ( rbig_params , rbig_loss ) # transform data X_trans , _ = rbig_model . fit_transform ( data ) # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( X_trans , title = 'Data' ) <Figure size 360x360 with 0 Axes> plt . plot ( np . cumsum ( rbig_model . losses_ )) print ( \"Total Correlation:\" , np . sum ( rbig_model . losses_ )) Total Correlation: 0.6925604350208086 Difference in Entropy Estimators \u00b6 for estimator in [ 'kde' , 'histogram' , 'knn' , 'gauss' ]: H_delta = InformationLoss ( method = estimator ) . calculate_loss ( X_trans , data ) print ( f \"Estimator: { estimator } \\n Delta H: { H_delta : .4f } \" ,) Estimator: kde Delta H: 0.6177 Estimator: histogram Delta H: 0.6774 Estimator: knn Delta H: 0.6908 Estimator: gauss Delta H: 0.8229 Speed of Entropy Estimators \u00b6 % timeit _ = InformationLoss ( method = 'kde' ) . calculate_loss ( X_trans , data ) % timeit _ = InformationLoss ( method = 'knn' ) . calculate_loss ( X_trans , data ) % timeit _ = InformationLoss ( method = 'histogram' ) . calculate_loss ( X_trans , data ) % timeit _ = InformationLoss ( method = 'gauss' ) . calculate_loss ( X_trans , data ) 882 ms \u00b1 1.57 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) 64.6 ms \u00b1 101 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 6.04 ms \u00b1 148 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 4.22 ms \u00b1 21.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Loss III - Neg Entropy \u00b6 from rbig.losses import NegEntropyLoss # initialize params rbig_params = RBIGParams () # initialize rbig model rbig_loss = NegEntropyLoss ( tol_layers = 50 ) # rbig_loss = NegEntropyLoss(tol_layers=10) rbig_model = GaussianizationModel ( rbig_params , rbig_loss ) # transform data to latent space Z , X_slogdet = rbig_model . fit_transform ( data ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-22-c7a35f3cc2fa> in <module> 9 10 # transform data to latent space ---> 11 Z , X_slogdet = rbig_model . fit_transform ( data ) ~/.conda/envs/rbig_dev/lib/python3.8/site-packages/sklearn/base.py in fit_transform (self, X, y, **fit_params) 569 if y is None : 570 # fit method of arity 1 (unsupervised transformation) --> 571 return self . fit ( X , ** fit_params ) . transform ( X ) 572 else : 573 # fit method of arity 2 (supervised transformation) ~/code/rbig/rbig/models/gaussianization.py in fit (self, X, y) 41 42 # regularize the jacobian ---> 43 dX [ dX > 0.0 ] = 0.0 44 45 X_logdetjacobian += dX KeyboardInterrupt : # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( X_trans , title = 'Data' ) <Figure size 360x360 with 0 Axes>","title":"Demo 5 rbig loss"},{"location":"notebooks/demo_5_rbig_loss/#data","text":"def plot_2d_joint ( data , color = 'blue' , title = 'Original Data' ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = color ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . suptitle ( title ) plt . tight_layout () plt . show () def plot_prob ( data , probs , title = 'Probabilities' ): fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = probs , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( title ) plt . show () seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig' n_samples = 10_000 n_features = 2 noise = 0.25 random_state = 1 clusters = 2 data = ToyData ( dataset = dataset , n_samples = n_samples , n_features = n_features , noise = noise , random_state = random_state , clusters = clusters , ) . generate_samples () plot_2d_joint ( data , title = 'Data' ) <Figure size 360x360 with 0 Axes>","title":"Data"},{"location":"notebooks/demo_5_rbig_loss/#options","text":"RBIG 1.1 - \"Stopping\" * Max # of layers * Information Loss * Negative Log Likelihood * Non-Gaussianity (?) * Difference in Non-Gaussianity? (norm(y) - norm(x) - log abs( dy/dx)) RBIG 2.0 - \"Loss\" * Negative Log Likelihood * Non-Gaussianity (?) RBIG 2.0 - Stopping : * Max Layers (epochs) * Information Loss(Xtrans, X) * Difference in Non-Gaussianity? (norm(y) - norm(x) - log abs( dy/dx))","title":"Options"},{"location":"notebooks/demo_5_rbig_loss/#loss-function-i-maximum-layers","text":"from rbig.losses import MaxLayersLoss # stoppers from rbig.layers import RBIGParams # initialize params rbig_params = RBIGParams () # initialize rbig model rbig_loss = MaxLayersLoss ( n_layers = 50 ) rbig_model = GaussianizationModel ( rbig_params , rbig_loss ) # transform data to latent space Z , _ = rbig_model . fit_transform ( data ) # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( Z , title = 'Data' ) <Figure size 360x360 with 0 Axes>","title":"Loss Function I - Maximum # layers"},{"location":"notebooks/demo_5_rbig_loss/#loss-function-ii-custom-loss-function","text":"def negative_log_likelihood ( Z , X , X_slogdet ): # calculate the log probability in latent space Z_logprob = stats . norm () . logpdf ( Z ) # calculate the probability of the transform X_logprob = Z_logprob . sum ( axis = 1 ) + X_slogdet . sum ( axis = 1 ) # return the nll return np . mean ( X_logprob ) def difference_nongaussian ( Z , X , X_slogdet ): delta_ng = . 5 * np . linalg . norm ( Z , 2 ) ** 2 - . 5 * np . linalg . norm ( X , 2 ) ** 2 - X_slogdet . sum ( axis = 1 ) return np . mean ( delta_ng ) # initialize params rbig_params = RBIGParams () # initialize rbig model rbig_loss = MaxLayersLoss ( n_layers = 50 , loss_func = difference_nongaussian ) rbig_model = GaussianizationModel ( rbig_params , rbig_loss ) # transform data to latent space Z , _ = rbig_model . fit_transform ( data ) # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( Z , title = 'Data' ) <Figure size 360x360 with 0 Axes>","title":"Loss Function I.I - Custom Loss Function"},{"location":"notebooks/demo_5_rbig_loss/#loss-ii-information-reduction","text":"from rbig.losses import InformationLoss # initialize params rbig_params = RBIGParams ( nbins = 500 ) # initialize the RBIG Loss function rbig_loss = InformationLoss ( tol_layers = 70 , method = 'histogram' ) # initialize rbig model rbig_model = GaussianizationModel ( rbig_params , rbig_loss ) # transform data X_trans , _ = rbig_model . fit_transform ( data ) # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( X_trans , title = 'Data' ) <Figure size 360x360 with 0 Axes> plt . plot ( np . cumsum ( rbig_model . losses_ )) print ( \"Total Correlation:\" , np . sum ( rbig_model . losses_ )) Total Correlation: 0.6925604350208086","title":"Loss II - Information Reduction"},{"location":"notebooks/demo_5_rbig_loss/#difference-in-entropy-estimators","text":"for estimator in [ 'kde' , 'histogram' , 'knn' , 'gauss' ]: H_delta = InformationLoss ( method = estimator ) . calculate_loss ( X_trans , data ) print ( f \"Estimator: { estimator } \\n Delta H: { H_delta : .4f } \" ,) Estimator: kde Delta H: 0.6177 Estimator: histogram Delta H: 0.6774 Estimator: knn Delta H: 0.6908 Estimator: gauss Delta H: 0.8229","title":"Difference in Entropy Estimators"},{"location":"notebooks/demo_5_rbig_loss/#speed-of-entropy-estimators","text":"% timeit _ = InformationLoss ( method = 'kde' ) . calculate_loss ( X_trans , data ) % timeit _ = InformationLoss ( method = 'knn' ) . calculate_loss ( X_trans , data ) % timeit _ = InformationLoss ( method = 'histogram' ) . calculate_loss ( X_trans , data ) % timeit _ = InformationLoss ( method = 'gauss' ) . calculate_loss ( X_trans , data ) 882 ms \u00b1 1.57 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) 64.6 ms \u00b1 101 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 6.04 ms \u00b1 148 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 4.22 ms \u00b1 21.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)","title":"Speed of Entropy Estimators"},{"location":"notebooks/demo_5_rbig_loss/#loss-iii-neg-entropy","text":"from rbig.losses import NegEntropyLoss # initialize params rbig_params = RBIGParams () # initialize rbig model rbig_loss = NegEntropyLoss ( tol_layers = 50 ) # rbig_loss = NegEntropyLoss(tol_layers=10) rbig_model = GaussianizationModel ( rbig_params , rbig_loss ) # transform data to latent space Z , X_slogdet = rbig_model . fit_transform ( data ) --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-22-c7a35f3cc2fa> in <module> 9 10 # transform data to latent space ---> 11 Z , X_slogdet = rbig_model . fit_transform ( data ) ~/.conda/envs/rbig_dev/lib/python3.8/site-packages/sklearn/base.py in fit_transform (self, X, y, **fit_params) 569 if y is None : 570 # fit method of arity 1 (unsupervised transformation) --> 571 return self . fit ( X , ** fit_params ) . transform ( X ) 572 else : 573 # fit method of arity 2 (supervised transformation) ~/code/rbig/rbig/models/gaussianization.py in fit (self, X, y) 41 42 # regularize the jacobian ---> 43 dX [ dX > 0.0 ] = 0.0 44 45 X_logdetjacobian += dX KeyboardInterrupt : # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( X_trans , title = 'Data' ) <Figure size 360x360 with 0 Axes>","title":"Loss III - Neg Entropy"},{"location":"notebooks/demo_6_it_measures/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) # RBIG Packages from rbig.data import ToyData from rbig.layers import RBIGBlock , RBIGParams from rbig.models import RBIGModel from rbig.losses import InformationLoss from typing import Iterable , Optional , Dict , NamedTuple , Tuple , Union import numpy as np from sklearn.base import BaseEstimator , TransformerMixin from rbig.base import DensityTransformerMixin , ScoreMixin import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 Data \u00b6 def plot_2d_joint ( data , color = 'blue' , title = 'Original Data' ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = color ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . suptitle ( title ) plt . tight_layout () plt . show () def plot_prob ( data , probs , title = 'Probabilities' ): fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = probs , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( title ) plt . show () seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig' n_samples = 10_000 noise = 0.1 random_state = 123 n_features = 2 data = ToyData ( dataset = dataset , n_samples = n_samples , n_features = n_features , noise = noise , random_state = random_state ) . generate_samples () plot_2d_joint ( data , title = 'Data' ) <Figure size 360x360 with 0 Axes> Model: RBIG, TC Loss, \u00b6 # initialize params rbig_params = RBIGParams () # initialize rbig model rbig_loss = InformationLoss ( tol_layers = 50 ) rbig_model = RBIGModel ( rbig_params , rbig_loss ) # transform data X_trans , _ = rbig_model . fit_transform ( data ) # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( X_trans , title = 'Data' ) <Figure size 360x360 with 0 Axes> Information Theoryz \u00b6 Measure 0 - Information \u00b6 X_logprob = rbig_model . score_samples ( data ) Measure I - Total Correlation (Multi-Information) \u00b6 data_tc = np . sum ( rbig_model . losses_ ) print ( f \"Total Correlation:\" , data_tc ) Total Correlation: 0.9799065640409519 Measure II - Entropy \u00b6 from rbig.information.entropy import marginal_entropy H_data = marginal_entropy ( data ) . sum () + data_tc print ( \"Entropy:\" , H_data ) Entropy: 2.881777074081217 Measure III - Mutual Information \u00b6 Measure IV - Kullback-Leibler Divergence \u00b6","title":"Demo 6 it measures"},{"location":"notebooks/demo_6_it_measures/#data","text":"def plot_2d_joint ( data , color = 'blue' , title = 'Original Data' ): fig = plt . figure ( figsize = ( 5 , 5 )) g = sns . jointplot ( x = data [:, 0 ], y = data [:, 1 ], kind = 'hex' , color = color ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . suptitle ( title ) plt . tight_layout () plt . show () def plot_prob ( data , probs , title = 'Probabilities' ): fig , ax = plt . subplots () h = ax . scatter ( data [:, 0 ], data [:, 1 ], s = 1 , c = probs , cmap = 'Reds' ) ax . set_xlabel ( 'X' ) ax . set_ylabel ( 'Y' ) cbar = plt . colorbar ( h , ) ax . set_title ( title ) plt . show () seed = 123 rng = np . random . RandomState ( seed = seed ) dataset = 'rbig' n_samples = 10_000 noise = 0.1 random_state = 123 n_features = 2 data = ToyData ( dataset = dataset , n_samples = n_samples , n_features = n_features , noise = noise , random_state = random_state ) . generate_samples () plot_2d_joint ( data , title = 'Data' ) <Figure size 360x360 with 0 Axes>","title":"Data"},{"location":"notebooks/demo_6_it_measures/#model-rbig-tc-loss","text":"# initialize params rbig_params = RBIGParams () # initialize rbig model rbig_loss = InformationLoss ( tol_layers = 50 ) rbig_model = RBIGModel ( rbig_params , rbig_loss ) # transform data X_trans , _ = rbig_model . fit_transform ( data ) # plot loss plt . plot ( rbig_model . losses_ ) plot_2d_joint ( X_trans , title = 'Data' ) <Figure size 360x360 with 0 Axes>","title":"Model: RBIG, TC Loss,"},{"location":"notebooks/demo_6_it_measures/#information-theoryz","text":"","title":"Information Theoryz"},{"location":"notebooks/demo_6_it_measures/#measure-0-information","text":"X_logprob = rbig_model . score_samples ( data )","title":"Measure 0 - Information"},{"location":"notebooks/demo_6_it_measures/#measure-i-total-correlation-multi-information","text":"data_tc = np . sum ( rbig_model . losses_ ) print ( f \"Total Correlation:\" , data_tc ) Total Correlation: 0.9799065640409519","title":"Measure I - Total Correlation (Multi-Information)"},{"location":"notebooks/demo_6_it_measures/#measure-ii-entropy","text":"from rbig.information.entropy import marginal_entropy H_data = marginal_entropy ( data ) . sum () + data_tc print ( \"Entropy:\" , H_data ) Entropy: 2.881777074081217","title":"Measure II - Entropy"},{"location":"notebooks/demo_6_it_measures/#measure-iii-mutual-information","text":"","title":"Measure III - Mutual Information"},{"location":"notebooks/demo_6_it_measures/#measure-iv-kullback-leibler-divergence","text":"","title":"Measure IV - Kullback-Leibler Divergence"},{"location":"notebooks/demo_7.1_pdf_estimation/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Distribution Estimation \u00b6 import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) # RBIG Packages from rbig.data import ToyData from typing import Iterable , Optional , Dict , NamedTuple , Tuple , Union import numpy as np from sklearn.base import BaseEstimator , TransformerMixin import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl # plt.style.use([\"seaborn-paper\"]) # mpl.rcParams['lines.linewidth'] = 4 # mpl.rcParams['legend.fontsize'] = 12 # mpl.rcParams['axes.labelsize'] = 12 import matplotlib.pyplot as plt import seaborn as sns # plt.style.use(['seaborn-paper']) sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline % load_ext autoreload % autoreload 2 Data \u00b6 seed = 123 n_samples = 1_000 n_tsamples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) X_new = data_dist . rvs ( size = ( n_tsamples , 1 ), random_state = seed ) H_real = data_dist . entropy () print ( f \"Entropy: { data_dist . entropy () : .4f } \" ) # plot distribution sns . distplot ( X ) Entropy: 2.0234 <matplotlib.axes._subplots.AxesSubplot at 0x7f79e78b1e20> sns . distplot ( data_dist . logpdf ( X )) <matplotlib.axes._subplots.AxesSubplot at 0x7f79e6fc53a0> PDF Estimation \u00b6 # from rbig.information.histogram import ScipyHistogram, hist_entropy, QuantileHistogram from rbig.density.histogram import ScipyHistogram , QuantileHistogram from rbig.density.kde import KDEScipy , KDESklearn , KDEFFT pdf_results = {} KDE - Exact \u00b6 data = X . copy () n_new_samples = 10_000 data_new = np . linspace ( - 1 , 20 , n_new_samples ) # calculate gaussian kde estimator = stats . gaussian_kde ( data . squeeze (), bw_method = 'scott' ) # estimate cdf function pdf_results [ 'kde' ] = estimator . logpdf ( data_new ) plt . hist ( pdf_results [ 'kde' ]); KDE - FFT \u00b6 # calculate gaussian kde estimator = sm . nonparametric . KDEUnivariate ( X . squeeze ()) estimator . fit ( kernel = \"gau\" , bw = 'scott' , fft = True , gridsize = 1_000 , ) # estimate cdf function pdf_results [ 'kde_fft' ] = np . log ( estimator . evaluate ( data_new )) plt . hist ( pdf_results [ 'kde_fft' ]); CDF Estimation \u00b6 from rbig.density.utils import kde_cdf from statsmodels.distributions.empirical_distribution import ECDF from sklearn.preprocessing import QuantileTransformer import statsmodels.api as sm from scipy import stats , special cdf_results = {} KDE \u00b6 data = X . copy () n_new_samples = 10_000 data_new = np . linspace ( - 1 , 20 , n_new_samples ) # calculate gaussian kde estimator = stats . gaussian_kde ( data . squeeze (), bw_method = 'scott' ) # estimate cdf function cdf_results [ 'kde' ] = kde_cdf ( data , data_new , estimator . factor ) % timeit kde_cdf ( data , data_new , estimator . factor ) 660 ms \u00b1 13.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Empirical CDF Function \u00b6 data = X . copy () # calculate ecdf ecdf_est = ECDF ( data . squeeze ()) # estimate cdf function cdf_results [ 'ecdf' ] = ecdf_est ( data_new ) % timeit ecdf_est ( data_new ) 103 \u00b5s \u00b1 368 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) Quantile Function \u00b6 data = X . copy () # calculate ecdf quantile_clf = QuantileTransformer ( n_quantiles = 100 , ) . fit ( data ) # estimate cdf function cdf_results [ 'quantile' ] = quantile_clf . transform ( data_new . reshape ( - 1 , 1 )) % timeit quantile_clf . transform ( data_new . reshape ( - 1 , 1 )) 288 \u00b5s \u00b1 2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) plt . plot ( data_new , cdf_results [ 'kde' ]) plt . plot ( data_new , cdf_results [ 'ecdf' ], linestyle = '-.' ) plt . plot ( data_new , cdf_results [ 'quantile' ], linestyle = '--' ) [<matplotlib.lines.Line2D at 0x7f79e0c379a0>] # Vector implementation kde_results [ 'scipy' ] = np . vectorize ( lambda x : estimator . integrate_box_1d ( - np . inf , x ))( data ) # ====================== # estimate covariance factor # ====================== covariance = np . cov ( data . T , rowvar = 1 ) * estimator . factor ** 2 # ====================== # Estimate the weights # ====================== weights = ( 1 / n_samples ) np . testing . assert_almost_equal ( weights , estimator . _weights . mean ()) # print(stdev) normalized_low = np . ravel (( - np . inf - data ) / stdev ) normalized_high = np . ravel (( data - data ) / stdev ) print ( normalized_low . shape , normalized_high . shape ) (1000,) (1000,) np . testing . assert_almost_equal ( covariance , estimator . covariance [ 0 ][ 0 ]) # assert covariance == estimator.covariance[0][0], f\"my: {covariance:.10f}, scipy: {estimator.covariance[0][0]:.10f}\" stdev = np . sqrt ( covariance ) n_samples = data . shape [ 0 ] np . testing . assert_almost_equal ( stdev , np . ravel ( np . sqrt ( estimator . covariance ))[ 0 ]) x_cdf_m = list () for idatapoint in data : x_datapoint = idatapoint . reshape ( - 1 , 1 ) # print(normalized_low.shape, normalized_high.shape) x_cdf_m . append ( np . sum ( weights * ( special . ndtr ( normalized_high ) - special . ndtr ( normalized_low )))) x_cdf_m = np . array ( x_cdf_m ) . reshape ( - 1 , 1 ) print ( x_cdf_m . shape ) np . testing . assert_array_almost_equal ( x_cdf , x_cdf_m ) # x_cdf_m = calculate_cdf(data, data, estimator.factor) plt . figure () plt . hist ( x_cdf ); plt . figure () plt . hist ( x_cdf_m ); # from rbig.information.histogram import ScipyHistogram, hist_entropy, QuantileHistogram from rbig.density.histogram import ScipyHistogram , QuantileHistogram from rbig.density.kde import KDEScipy , KDESklearn , KDEFFT density_clf_fft = KDEFFT ( n_quantiles = 100 , bw_method = 'scott' , support_extension = 10 ) . fit ( X . squeeze ()) density_clf_sci = KDEScipy ( n_quantiles = 50 , bw_method = 'scott' , support_extension = 10 ) . fit ( X . squeeze ()) here density_clf_fft . estimator_ . bw , (0.49782030682145245, 0.251188643150958) X . shape (1000, 1) data_covariance = np . cov ( X . T , rowvar = 1 ) density_clf_sci . estimator_ . factor , density_clf_sci . estimator_ . covariance , (0.251188643150958, array([[0.2393458]])) data_covariance * density_clf_sci . estimator_ . factor ** 2 0.2393457965271931 1. Histogram (Generic, Fast) \u00b6 # from rbig.information.histogram import ScipyHistogram, hist_entropy, QuantileHistogram from rbig.density.histogram import ScipyHistogram , QuantileHistogram from rbig.density.kde import KDEScipy , KDESklearn , KDEFFT # density_clf = QuantileHistogram(bins=100, support_extension=10, alpha=1e-4, n_quantiles=100) density_clf = KDEScipy ( n_quantiles = 50 , bw_method = 'scott' , support_extension = 10 ) # density_clf = KDESklearn(n_quantiles=100, support_extension=10) density_fft_clf = KDEFFT ( n_quantiles = 50 , support_extension = 10 ) density_fft_clf . fit ( X . squeeze ()) density_clf . fit ( X . squeeze ()) KDEScipy(bw_method='scott', n_quantiles=50, support_extension=10) % timeit KDEFFT ( n_quantiles = 50 , support_extension = 10 ) . fit ( X . squeeze ()) % timeit KDEScipy ( n_quantiles = 50 , bw_method = 'scott' , support_extension = 10 ) . fit ( X_new . squeeze ()) 3.11 ms \u00b1 97.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 49.1 ms \u00b1 338 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) density_clf . hbins_ . shape , density_clf . hpdf_ . shape , density_clf . hcdf_ . shape , density_clf . endog . shape ((100,), (100,), (128,), (100,)) % timeit ScipyHistogram ( bins = 50 , support_extension = 10 , alpha = 1e-4 ) . fit ( X_new . squeeze ()) % timeit QuantileHistogram ( bins = 50 , support_extension = 10 , alpha = 1e-4 , n_quantiles = 100 ) . fit ( X_new . squeeze ()) % timeit KDEFFT ( n_quantiles = 50 , support_extension = 10 ) . fit ( X . squeeze ()) % timeit KDEScipy ( n_quantiles = 50 , bw_method = 'scott' , support_extension = 10 ) . fit ( X_new . squeeze ()) % timeit KDESklearn ( n_quantiles = 50 , support_extension = 10 ) . fit ( X_new . squeeze ()) 970 \u00b5s \u00b1 17.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 1.35 ms \u00b1 29.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 5.34 ms \u00b1 68 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 49.9 ms \u00b1 590 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 39.7 ms \u00b1 432 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) % timeit ScipyHistogram ( bins = 500 , support_extension = 10 , alpha = 1e-4 ) . fit ( X_new . squeeze ()) % timeit QuantileHistogram ( bins = 500 , support_extension = 10 , alpha = 1e-4 , n_quantiles = 100 ) . fit ( X_new . squeeze ()) % timeit KDEFFT ( n_quantiles = 500 , support_extension = 10 ) . fit ( X . squeeze ()) % timeit KDEScipy ( n_quantiles = 500 , bw_method = 'scott' , support_extension = 10 ) . fit ( X_new . squeeze ()) % timeit KDESklearn ( n_quantiles = 500 , support_extension = 10 ) . fit ( X_new . squeeze ()) 968 \u00b5s \u00b1 1.06 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 1.33 ms \u00b1 6.49 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 45.4 ms \u00b1 695 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 471 ms \u00b1 2.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) 327 ms \u00b1 1.27 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) X_prob = density_clf . pdf ( X_new ) Log PDF \u00b6 X_lprob = density_clf . logpdf ( X_new ) X_lprobf = density_fft_clf . logpdf ( X_new ) sns . distplot ( X_lprob , color = 'blue' , label = 'Estimated' ) sns . distplot ( X_lprobf , color = 'red' , label = 'Estimated' ) plt . legend () <matplotlib.legend.Legend at 0x7f79dd7390a0> CDF \u00b6 X_cdf = density_clf . cdf ( X_new ) sns . distplot ( X_cdf , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7f79dd8f7460> PPF (Inverse CDF) \u00b6 X_approx = density_clf . ppf ( X_cdf ) sns . distplot ( X_approx , bins = 100 , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7f79dd8a24c0> 2. KDE - Scipy Implementation (Small Data) \u00b6 from scipy import stats from rbig.information.kde import KDEScipy , kde_entropy_uni kde_scipy_clf = KDEScipy ( bw_method = 'scott' , n_quantiles = 100 , support_extension = 10 ) kde_scipy_clf . fit ( X ) KDEScipy(bw_method='scott', interp_kind='linear', n_quantiles=100, support_extension=10) Log PDF \u00b6 X_lprob = kde_scipy_clf . logpdf ( X_new ) # %timeit _ = kde_scipy_clf.logpdf(X_new) Viz - Distribution \u00b6 sns . distplot ( X_lprob , color = 'blue' , label = 'Estimated' ) sns . distplot ( data_dist . logpdf ( X )) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7f13036bf9d0> Viz - PDF Lines \u00b6 plt . plot ( kde_scipy_clf . hbins_ , kde_scipy_clf . hpdf_ , linewidth = 3 , label = 'KDE Estimated PDF' ) plt . plot ( kde_scipy_clf . hbins_ , data_dist . pdf ( kde_scipy_clf . hbins_ ), linewidth = 3 , label = 'True PDF' ) plt . legend () <matplotlib.legend.Legend at 0x7f1303d6f370> Viz - Log PDF Lines \u00b6 plt . plot ( kde_scipy_clf . hbins_ , np . log ( kde_scipy_clf . hpdf_ ), linewidth = 3 , label = 'KDE Estimated Log PDF' ) plt . plot ( kde_scipy_clf . hbins_ , data_dist . logpdf ( kde_scipy_clf . hbins_ ), linewidth = 3 , label = 'True Log PDF' ) plt . legend ( fontsize = 12 ) plt . show () CDF Function \u00b6 X_cdf = kde_scipy_clf . cdf ( X_new ) % timeit _ = kde_scipy_clf . cdf ( X_new ) 339 \u00b5s \u00b1 474 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) Viz - Distribution \u00b6 sns . distplot ( X_cdf , color = 'blue' , label = 'Estimated' ) <matplotlib.axes._subplots.AxesSubplot at 0x7f1303d41490> Viz - CDF Lines \u00b6 plt . plot ( kde_scipy_clf . hcdf_ ) [<matplotlib.lines.Line2D at 0x7f1303c621f0>] plt . plot ( kde_scipy_clf . hbins_ , kde_scipy_clf . hcdf_ , label = 'KDE Estimated CDF' ) plt . plot ( kde_scipy_clf . hbins_ , data_dist . cdf ( kde_scipy_clf . hbins_ ), label = 'True CDF' ) plt . legend ( fontsize = 12 ) <matplotlib.legend.Legend at 0x7f1303c28850> PPF Function \u00b6 X_ppf = kde_scipy_clf . ppf ( X_cdf ) % timeit _ = kde_scipy_clf . ppf ( X_cdf ) 339 \u00b5s \u00b1 739 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) Viz - Distribution \u00b6 sns . distplot ( X_ppf , color = 'blue' , label = 'Estimated' ) <matplotlib.axes._subplots.AxesSubplot at 0x7f1303b72160> Viz - Line \u00b6 plt . plot ( kde_scipy_clf . hcdf_ , kde_scipy_clf . hbins_ ) [<matplotlib.lines.Line2D at 0x7f1303ae94c0>] 3. KDE (Knn Approximation) \u00b6 from scipy import stats from rbig.information.kde import KDESklearn , kde_entropy_uni algorithm = 'kd_tree' n_qauntiles = 100 support_extesion = 10 kde_sk_clf = KDESklearn ( algorithm = algorithm , n_quantiles = n_qauntiles , support_extension = support_extesion ) kde_sk_clf . fit ( X ) KDESklearn(algorithm='kd_tree', kernel='gaussian', kwargs={}, metric='euclidean', n_quantiles=100, support_extension=10) Log PDF \u00b6 X_lprob = kde_sk_clf . logpdf ( X ) % timeit _ = kde_sk_clf . logpdf ( X ) 59.3 \u00b5s \u00b1 25.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) sns . distplot ( X_lprob , color = 'blue' , label = 'Estimated' ) sns . distplot ( data_dist . logpdf ( X )) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7f13041e5a60> plt . plot ( kde_sk_clf . hbins_ , kde_sk_clf . hpdf_ , linewidth = 3 , label = 'KDE (knn) Estimated PDF' ) plt . plot ( kde_sk_clf . hbins_ , data_dist . pdf ( kde_sk_clf . hbins_ ), linewidth = 3 , label = 'True PDF' ) plt . legend () <matplotlib.legend.Legend at 0x7f130405cf10> plt . plot ( kde_sk_clf . hbins_ , np . log ( kde_sk_clf . hpdf_ ), linewidth = 3 , label = 'KDE (knn) Estimated Log PDF' ) plt . plot ( kde_sk_clf . hbins_ , data_dist . logpdf ( kde_sk_clf . hbins_ ), linewidth = 3 , label = 'True Log PDF' ) plt . legend ( fontsize = 12 ) plt . show () CDF Function \u00b6 X_cdf = kde_sk_clf . cdf ( X ) % timeit _ = kde_sk_clf . cdf ( X ) 23.3 \u00b5s \u00b1 1.18 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) sns . distplot ( X_cdf , color = 'blue' , label = 'Estimated' ) <matplotlib.axes._subplots.AxesSubplot at 0x7f130402d430> plt . plot ( kde_sk_clf . hbins_ , kde_sk_clf . hcdf_ , label = 'KDE (knn) Estimated CDF' ) plt . plot ( kde_sk_clf . hbins_ , data_dist . cdf ( kde_sk_clf . hbins_ ), label = 'True CDF' ) plt . legend ( fontsize = 12 ) <matplotlib.legend.Legend at 0x7f95af4b7eb0> PPF Function \u00b6 X_ppf = kde_sk_clf . ppf ( X_cdf ) % timeit _ = kde_sk_clf . ppf ( X ) 44.5 \u00b5s \u00b1 35 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) sns . distplot ( X_ppf , color = 'blue' , label = 'Estimated' ) <matplotlib.axes._subplots.AxesSubplot at 0x7f95bd241310> plt . plot ( kde_sk_clf . hcdf_ , kde_sk_clf . hbins_ , label = 'KDE (knn) Estimated PPF' ); plt . plot ( kde_sk_clf . hcdf_ , data_dist . ppf ( kde_sk_clf . hcdf_ ), label = 'True PPF' ); plt . xlabel ( r '$\\mathbf {U} \\sim \\mathcal {U} ([0, 1])$' ,) plt . ylabel ( r '$F_\\theta^{-1}(\\mathbf {U} )$' ,) plt . legend () <matplotlib.legend.Legend at 0x7f95afdd1f40>","title":"Demo 7.1 pdf estimation"},{"location":"notebooks/demo_7.1_pdf_estimation/#distribution-estimation","text":"import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) # RBIG Packages from rbig.data import ToyData from typing import Iterable , Optional , Dict , NamedTuple , Tuple , Union import numpy as np from sklearn.base import BaseEstimator , TransformerMixin import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl # plt.style.use([\"seaborn-paper\"]) # mpl.rcParams['lines.linewidth'] = 4 # mpl.rcParams['legend.fontsize'] = 12 # mpl.rcParams['axes.labelsize'] = 12 import matplotlib.pyplot as plt import seaborn as sns # plt.style.use(['seaborn-paper']) sns . reset_defaults () #sns.set_style('whitegrid') #sns.set_context('talk') sns . set_context ( context = 'talk' , font_scale = 0.7 ) % matplotlib inline % load_ext autoreload % autoreload 2","title":"Distribution Estimation"},{"location":"notebooks/demo_7.1_pdf_estimation/#data","text":"seed = 123 n_samples = 1_000 n_tsamples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) X_new = data_dist . rvs ( size = ( n_tsamples , 1 ), random_state = seed ) H_real = data_dist . entropy () print ( f \"Entropy: { data_dist . entropy () : .4f } \" ) # plot distribution sns . distplot ( X ) Entropy: 2.0234 <matplotlib.axes._subplots.AxesSubplot at 0x7f79e78b1e20> sns . distplot ( data_dist . logpdf ( X )) <matplotlib.axes._subplots.AxesSubplot at 0x7f79e6fc53a0>","title":"Data"},{"location":"notebooks/demo_7.1_pdf_estimation/#pdf-estimation","text":"# from rbig.information.histogram import ScipyHistogram, hist_entropy, QuantileHistogram from rbig.density.histogram import ScipyHistogram , QuantileHistogram from rbig.density.kde import KDEScipy , KDESklearn , KDEFFT pdf_results = {}","title":"PDF Estimation"},{"location":"notebooks/demo_7.1_pdf_estimation/#kde-exact","text":"data = X . copy () n_new_samples = 10_000 data_new = np . linspace ( - 1 , 20 , n_new_samples ) # calculate gaussian kde estimator = stats . gaussian_kde ( data . squeeze (), bw_method = 'scott' ) # estimate cdf function pdf_results [ 'kde' ] = estimator . logpdf ( data_new ) plt . hist ( pdf_results [ 'kde' ]);","title":"KDE - Exact"},{"location":"notebooks/demo_7.1_pdf_estimation/#kde-fft","text":"# calculate gaussian kde estimator = sm . nonparametric . KDEUnivariate ( X . squeeze ()) estimator . fit ( kernel = \"gau\" , bw = 'scott' , fft = True , gridsize = 1_000 , ) # estimate cdf function pdf_results [ 'kde_fft' ] = np . log ( estimator . evaluate ( data_new )) plt . hist ( pdf_results [ 'kde_fft' ]);","title":"KDE - FFT"},{"location":"notebooks/demo_7.1_pdf_estimation/#cdf-estimation","text":"from rbig.density.utils import kde_cdf from statsmodels.distributions.empirical_distribution import ECDF from sklearn.preprocessing import QuantileTransformer import statsmodels.api as sm from scipy import stats , special cdf_results = {}","title":"CDF Estimation"},{"location":"notebooks/demo_7.1_pdf_estimation/#kde","text":"data = X . copy () n_new_samples = 10_000 data_new = np . linspace ( - 1 , 20 , n_new_samples ) # calculate gaussian kde estimator = stats . gaussian_kde ( data . squeeze (), bw_method = 'scott' ) # estimate cdf function cdf_results [ 'kde' ] = kde_cdf ( data , data_new , estimator . factor ) % timeit kde_cdf ( data , data_new , estimator . factor ) 660 ms \u00b1 13.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)","title":"KDE"},{"location":"notebooks/demo_7.1_pdf_estimation/#empirical-cdf-function","text":"data = X . copy () # calculate ecdf ecdf_est = ECDF ( data . squeeze ()) # estimate cdf function cdf_results [ 'ecdf' ] = ecdf_est ( data_new ) % timeit ecdf_est ( data_new ) 103 \u00b5s \u00b1 368 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)","title":"Empirical CDF Function"},{"location":"notebooks/demo_7.1_pdf_estimation/#quantile-function","text":"data = X . copy () # calculate ecdf quantile_clf = QuantileTransformer ( n_quantiles = 100 , ) . fit ( data ) # estimate cdf function cdf_results [ 'quantile' ] = quantile_clf . transform ( data_new . reshape ( - 1 , 1 )) % timeit quantile_clf . transform ( data_new . reshape ( - 1 , 1 )) 288 \u00b5s \u00b1 2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) plt . plot ( data_new , cdf_results [ 'kde' ]) plt . plot ( data_new , cdf_results [ 'ecdf' ], linestyle = '-.' ) plt . plot ( data_new , cdf_results [ 'quantile' ], linestyle = '--' ) [<matplotlib.lines.Line2D at 0x7f79e0c379a0>] # Vector implementation kde_results [ 'scipy' ] = np . vectorize ( lambda x : estimator . integrate_box_1d ( - np . inf , x ))( data ) # ====================== # estimate covariance factor # ====================== covariance = np . cov ( data . T , rowvar = 1 ) * estimator . factor ** 2 # ====================== # Estimate the weights # ====================== weights = ( 1 / n_samples ) np . testing . assert_almost_equal ( weights , estimator . _weights . mean ()) # print(stdev) normalized_low = np . ravel (( - np . inf - data ) / stdev ) normalized_high = np . ravel (( data - data ) / stdev ) print ( normalized_low . shape , normalized_high . shape ) (1000,) (1000,) np . testing . assert_almost_equal ( covariance , estimator . covariance [ 0 ][ 0 ]) # assert covariance == estimator.covariance[0][0], f\"my: {covariance:.10f}, scipy: {estimator.covariance[0][0]:.10f}\" stdev = np . sqrt ( covariance ) n_samples = data . shape [ 0 ] np . testing . assert_almost_equal ( stdev , np . ravel ( np . sqrt ( estimator . covariance ))[ 0 ]) x_cdf_m = list () for idatapoint in data : x_datapoint = idatapoint . reshape ( - 1 , 1 ) # print(normalized_low.shape, normalized_high.shape) x_cdf_m . append ( np . sum ( weights * ( special . ndtr ( normalized_high ) - special . ndtr ( normalized_low )))) x_cdf_m = np . array ( x_cdf_m ) . reshape ( - 1 , 1 ) print ( x_cdf_m . shape ) np . testing . assert_array_almost_equal ( x_cdf , x_cdf_m ) # x_cdf_m = calculate_cdf(data, data, estimator.factor) plt . figure () plt . hist ( x_cdf ); plt . figure () plt . hist ( x_cdf_m ); # from rbig.information.histogram import ScipyHistogram, hist_entropy, QuantileHistogram from rbig.density.histogram import ScipyHistogram , QuantileHistogram from rbig.density.kde import KDEScipy , KDESklearn , KDEFFT density_clf_fft = KDEFFT ( n_quantiles = 100 , bw_method = 'scott' , support_extension = 10 ) . fit ( X . squeeze ()) density_clf_sci = KDEScipy ( n_quantiles = 50 , bw_method = 'scott' , support_extension = 10 ) . fit ( X . squeeze ()) here density_clf_fft . estimator_ . bw , (0.49782030682145245, 0.251188643150958) X . shape (1000, 1) data_covariance = np . cov ( X . T , rowvar = 1 ) density_clf_sci . estimator_ . factor , density_clf_sci . estimator_ . covariance , (0.251188643150958, array([[0.2393458]])) data_covariance * density_clf_sci . estimator_ . factor ** 2 0.2393457965271931","title":"Quantile Function"},{"location":"notebooks/demo_7.1_pdf_estimation/#1-histogram-generic-fast","text":"# from rbig.information.histogram import ScipyHistogram, hist_entropy, QuantileHistogram from rbig.density.histogram import ScipyHistogram , QuantileHistogram from rbig.density.kde import KDEScipy , KDESklearn , KDEFFT # density_clf = QuantileHistogram(bins=100, support_extension=10, alpha=1e-4, n_quantiles=100) density_clf = KDEScipy ( n_quantiles = 50 , bw_method = 'scott' , support_extension = 10 ) # density_clf = KDESklearn(n_quantiles=100, support_extension=10) density_fft_clf = KDEFFT ( n_quantiles = 50 , support_extension = 10 ) density_fft_clf . fit ( X . squeeze ()) density_clf . fit ( X . squeeze ()) KDEScipy(bw_method='scott', n_quantiles=50, support_extension=10) % timeit KDEFFT ( n_quantiles = 50 , support_extension = 10 ) . fit ( X . squeeze ()) % timeit KDEScipy ( n_quantiles = 50 , bw_method = 'scott' , support_extension = 10 ) . fit ( X_new . squeeze ()) 3.11 ms \u00b1 97.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 49.1 ms \u00b1 338 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) density_clf . hbins_ . shape , density_clf . hpdf_ . shape , density_clf . hcdf_ . shape , density_clf . endog . shape ((100,), (100,), (128,), (100,)) % timeit ScipyHistogram ( bins = 50 , support_extension = 10 , alpha = 1e-4 ) . fit ( X_new . squeeze ()) % timeit QuantileHistogram ( bins = 50 , support_extension = 10 , alpha = 1e-4 , n_quantiles = 100 ) . fit ( X_new . squeeze ()) % timeit KDEFFT ( n_quantiles = 50 , support_extension = 10 ) . fit ( X . squeeze ()) % timeit KDEScipy ( n_quantiles = 50 , bw_method = 'scott' , support_extension = 10 ) . fit ( X_new . squeeze ()) % timeit KDESklearn ( n_quantiles = 50 , support_extension = 10 ) . fit ( X_new . squeeze ()) 970 \u00b5s \u00b1 17.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 1.35 ms \u00b1 29.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 5.34 ms \u00b1 68 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 49.9 ms \u00b1 590 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 39.7 ms \u00b1 432 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) % timeit ScipyHistogram ( bins = 500 , support_extension = 10 , alpha = 1e-4 ) . fit ( X_new . squeeze ()) % timeit QuantileHistogram ( bins = 500 , support_extension = 10 , alpha = 1e-4 , n_quantiles = 100 ) . fit ( X_new . squeeze ()) % timeit KDEFFT ( n_quantiles = 500 , support_extension = 10 ) . fit ( X . squeeze ()) % timeit KDEScipy ( n_quantiles = 500 , bw_method = 'scott' , support_extension = 10 ) . fit ( X_new . squeeze ()) % timeit KDESklearn ( n_quantiles = 500 , support_extension = 10 ) . fit ( X_new . squeeze ()) 968 \u00b5s \u00b1 1.06 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 1.33 ms \u00b1 6.49 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each) 45.4 ms \u00b1 695 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) 471 ms \u00b1 2.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) 327 ms \u00b1 1.27 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) X_prob = density_clf . pdf ( X_new )","title":"1. Histogram (Generic, Fast)"},{"location":"notebooks/demo_7.1_pdf_estimation/#log-pdf","text":"X_lprob = density_clf . logpdf ( X_new ) X_lprobf = density_fft_clf . logpdf ( X_new ) sns . distplot ( X_lprob , color = 'blue' , label = 'Estimated' ) sns . distplot ( X_lprobf , color = 'red' , label = 'Estimated' ) plt . legend () <matplotlib.legend.Legend at 0x7f79dd7390a0>","title":"Log PDF"},{"location":"notebooks/demo_7.1_pdf_estimation/#cdf","text":"X_cdf = density_clf . cdf ( X_new ) sns . distplot ( X_cdf , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7f79dd8f7460>","title":"CDF"},{"location":"notebooks/demo_7.1_pdf_estimation/#ppf-inverse-cdf","text":"X_approx = density_clf . ppf ( X_cdf ) sns . distplot ( X_approx , bins = 100 , color = 'blue' , label = 'Estimated' ) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7f79dd8a24c0>","title":"PPF (Inverse CDF)"},{"location":"notebooks/demo_7.1_pdf_estimation/#2-kde-scipy-implementation-small-data","text":"from scipy import stats from rbig.information.kde import KDEScipy , kde_entropy_uni kde_scipy_clf = KDEScipy ( bw_method = 'scott' , n_quantiles = 100 , support_extension = 10 ) kde_scipy_clf . fit ( X ) KDEScipy(bw_method='scott', interp_kind='linear', n_quantiles=100, support_extension=10)","title":"2. KDE - Scipy Implementation (Small Data)"},{"location":"notebooks/demo_7.1_pdf_estimation/#log-pdf_1","text":"X_lprob = kde_scipy_clf . logpdf ( X_new ) # %timeit _ = kde_scipy_clf.logpdf(X_new)","title":"Log PDF"},{"location":"notebooks/demo_7.1_pdf_estimation/#viz-distribution","text":"sns . distplot ( X_lprob , color = 'blue' , label = 'Estimated' ) sns . distplot ( data_dist . logpdf ( X )) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7f13036bf9d0>","title":"Viz - Distribution"},{"location":"notebooks/demo_7.1_pdf_estimation/#viz-pdf-lines","text":"plt . plot ( kde_scipy_clf . hbins_ , kde_scipy_clf . hpdf_ , linewidth = 3 , label = 'KDE Estimated PDF' ) plt . plot ( kde_scipy_clf . hbins_ , data_dist . pdf ( kde_scipy_clf . hbins_ ), linewidth = 3 , label = 'True PDF' ) plt . legend () <matplotlib.legend.Legend at 0x7f1303d6f370>","title":"Viz - PDF Lines"},{"location":"notebooks/demo_7.1_pdf_estimation/#viz-log-pdf-lines","text":"plt . plot ( kde_scipy_clf . hbins_ , np . log ( kde_scipy_clf . hpdf_ ), linewidth = 3 , label = 'KDE Estimated Log PDF' ) plt . plot ( kde_scipy_clf . hbins_ , data_dist . logpdf ( kde_scipy_clf . hbins_ ), linewidth = 3 , label = 'True Log PDF' ) plt . legend ( fontsize = 12 ) plt . show ()","title":"Viz - Log PDF Lines"},{"location":"notebooks/demo_7.1_pdf_estimation/#cdf-function","text":"X_cdf = kde_scipy_clf . cdf ( X_new ) % timeit _ = kde_scipy_clf . cdf ( X_new ) 339 \u00b5s \u00b1 474 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)","title":"CDF Function"},{"location":"notebooks/demo_7.1_pdf_estimation/#viz-distribution_1","text":"sns . distplot ( X_cdf , color = 'blue' , label = 'Estimated' ) <matplotlib.axes._subplots.AxesSubplot at 0x7f1303d41490>","title":"Viz - Distribution"},{"location":"notebooks/demo_7.1_pdf_estimation/#viz-cdf-lines","text":"plt . plot ( kde_scipy_clf . hcdf_ ) [<matplotlib.lines.Line2D at 0x7f1303c621f0>] plt . plot ( kde_scipy_clf . hbins_ , kde_scipy_clf . hcdf_ , label = 'KDE Estimated CDF' ) plt . plot ( kde_scipy_clf . hbins_ , data_dist . cdf ( kde_scipy_clf . hbins_ ), label = 'True CDF' ) plt . legend ( fontsize = 12 ) <matplotlib.legend.Legend at 0x7f1303c28850>","title":"Viz - CDF Lines"},{"location":"notebooks/demo_7.1_pdf_estimation/#ppf-function","text":"X_ppf = kde_scipy_clf . ppf ( X_cdf ) % timeit _ = kde_scipy_clf . ppf ( X_cdf ) 339 \u00b5s \u00b1 739 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)","title":"PPF Function"},{"location":"notebooks/demo_7.1_pdf_estimation/#viz-distribution_2","text":"sns . distplot ( X_ppf , color = 'blue' , label = 'Estimated' ) <matplotlib.axes._subplots.AxesSubplot at 0x7f1303b72160>","title":"Viz - Distribution"},{"location":"notebooks/demo_7.1_pdf_estimation/#viz-line","text":"plt . plot ( kde_scipy_clf . hcdf_ , kde_scipy_clf . hbins_ ) [<matplotlib.lines.Line2D at 0x7f1303ae94c0>]","title":"Viz - Line"},{"location":"notebooks/demo_7.1_pdf_estimation/#3-kde-knn-approximation","text":"from scipy import stats from rbig.information.kde import KDESklearn , kde_entropy_uni algorithm = 'kd_tree' n_qauntiles = 100 support_extesion = 10 kde_sk_clf = KDESklearn ( algorithm = algorithm , n_quantiles = n_qauntiles , support_extension = support_extesion ) kde_sk_clf . fit ( X ) KDESklearn(algorithm='kd_tree', kernel='gaussian', kwargs={}, metric='euclidean', n_quantiles=100, support_extension=10)","title":"3. KDE (Knn Approximation)"},{"location":"notebooks/demo_7.1_pdf_estimation/#log-pdf_2","text":"X_lprob = kde_sk_clf . logpdf ( X ) % timeit _ = kde_sk_clf . logpdf ( X ) 59.3 \u00b5s \u00b1 25.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) sns . distplot ( X_lprob , color = 'blue' , label = 'Estimated' ) sns . distplot ( data_dist . logpdf ( X )) plt . legend ( fontsize = 20 ) <matplotlib.legend.Legend at 0x7f13041e5a60> plt . plot ( kde_sk_clf . hbins_ , kde_sk_clf . hpdf_ , linewidth = 3 , label = 'KDE (knn) Estimated PDF' ) plt . plot ( kde_sk_clf . hbins_ , data_dist . pdf ( kde_sk_clf . hbins_ ), linewidth = 3 , label = 'True PDF' ) plt . legend () <matplotlib.legend.Legend at 0x7f130405cf10> plt . plot ( kde_sk_clf . hbins_ , np . log ( kde_sk_clf . hpdf_ ), linewidth = 3 , label = 'KDE (knn) Estimated Log PDF' ) plt . plot ( kde_sk_clf . hbins_ , data_dist . logpdf ( kde_sk_clf . hbins_ ), linewidth = 3 , label = 'True Log PDF' ) plt . legend ( fontsize = 12 ) plt . show ()","title":"Log PDF"},{"location":"notebooks/demo_7.1_pdf_estimation/#cdf-function_1","text":"X_cdf = kde_sk_clf . cdf ( X ) % timeit _ = kde_sk_clf . cdf ( X ) 23.3 \u00b5s \u00b1 1.18 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) sns . distplot ( X_cdf , color = 'blue' , label = 'Estimated' ) <matplotlib.axes._subplots.AxesSubplot at 0x7f130402d430> plt . plot ( kde_sk_clf . hbins_ , kde_sk_clf . hcdf_ , label = 'KDE (knn) Estimated CDF' ) plt . plot ( kde_sk_clf . hbins_ , data_dist . cdf ( kde_sk_clf . hbins_ ), label = 'True CDF' ) plt . legend ( fontsize = 12 ) <matplotlib.legend.Legend at 0x7f95af4b7eb0>","title":"CDF Function"},{"location":"notebooks/demo_7.1_pdf_estimation/#ppf-function_1","text":"X_ppf = kde_sk_clf . ppf ( X_cdf ) % timeit _ = kde_sk_clf . ppf ( X ) 44.5 \u00b5s \u00b1 35 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each) sns . distplot ( X_ppf , color = 'blue' , label = 'Estimated' ) <matplotlib.axes._subplots.AxesSubplot at 0x7f95bd241310> plt . plot ( kde_sk_clf . hcdf_ , kde_sk_clf . hbins_ , label = 'KDE (knn) Estimated PPF' ); plt . plot ( kde_sk_clf . hcdf_ , data_dist . ppf ( kde_sk_clf . hcdf_ ), label = 'True PPF' ); plt . xlabel ( r '$\\mathbf {U} \\sim \\mathcal {U} ([0, 1])$' ,) plt . ylabel ( r '$F_\\theta^{-1}(\\mathbf {U} )$' ,) plt . legend () <matplotlib.legend.Legend at 0x7f95afdd1f40>","title":"PPF Function"},{"location":"notebooks/demo_7.2_boundary_issues/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Boundary Issues \u00b6 import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) # RBIG Packages from rbig.data import ToyData from rbig.information.kde import KDESklearn from typing import Iterable , Optional , Dict , NamedTuple , Tuple , Union import numpy as np from sklearn.base import BaseEstimator , TransformerMixin import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2 Data \u00b6 def plot_dist ( X_samples : np . ndarray , bins = 100 , * args , ** kwargs ): fig , ax = plt . subplots () sns . distplot ( X_samples , bins = bins , ax = ax , kde_kws = { 'linewidth' : 7 , 'color' : 'black' , }, hist_kws = { 'color' : 'green' } ) return fig , ax seed = 123 n_samples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) # plot distribution plot_dist ( X , bins = 100 ); CDF Estimation \u00b6 For the CDF estimation, I will use the quantile method. # number of quantiles n_quantiles = 1_000 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # calculate the reference values (support), [0, 1] references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate quantiles quantiles = np . percentile ( X , references * 100 ) 0.16885335848364358 18.275140139054425 Plot CDF Distribution \u00b6 # plot distribution plot_dist ( quantiles , nbins = 100 ); It looks really close to the original distribution. I think we can be happy with this. Plot - CDF Function \u00b6 # CDF plot fig , ax = plt . subplots () ax . plot ( quantiles , references , linestyle = '--' , color = 'black' ) ax . set_xlabel ( r 'Support Points' ) ax . set_ylabel ( r 'Empirical CDF' ) plt . show () This also looks OK. But what happens if we find some points that are outside of our distribution? In other words, what happens if I have points that lay outside of my support? print ( 'Data Boundaries:' , X . min (), X . max ()) print ( 'Quantile Boundaries:' , quantiles . min (), quantiles . max ()) Data Boundaries: 0.16885335848364358 18.275140139054425 Quantile Boundaries: 0.16885335848364358 18.275140139054425 So we can extend the boundary (the support) to ensure that we can accommadate points that lie outside of the region. Method I - Extend the Quantiles Boundaries \u00b6 support_extension = 10 domain = np . abs ( quantiles . max () - quantiles . min ()) domain_ext = ( 10 / 100 ) * domain lower_bound = quantiles . min () - domain_ext upper_bound = quantiles . max () + domain_ext print ( 'New Boundaries:' , lower_bound , upper_bound ) New Boundaries: -1.6417753195734346 20.085768817111504 new_quantiles = np . hstack ([ lower_bound , quantiles , upper_bound ]) # testing assert lower_bound == new_quantiles . min () assert upper_bound == new_quantiles . max () assert np . ndim ( new_quantiles ) == 1 Interpolation \u00b6 We will now interpolate to ensure that the support gets extended. new_references = np . interp ( new_quantiles , quantiles , references ) print ( new_references . min (), new_references . max ()) 0.0 1.0 # CDF plot fig , ax = plt . subplots () ax . plot ( new_quantiles , new_references , linestyle = '--' , color = 'red' , label = 'Extended' ) ax . plot ( quantiles , references , linestyle = '--' , color = 'black' , label = 'Old' ) ax . legend ( fontsize = 12 ) ax . set_xlabel ( r 'Support Points' ) ax . set_ylabel ( r 'Empirical CDF' ) plt . show () PDF Estimation \u00b6 quantiles . min (), quantiles . max () (0.16885335848364358, 18.275140139054425) plt . hist ( hist [ 0 ], bins = 100 ); hpdf , hbins = np . histogram ( X . squeeze (), bins = 100 , density = True ) plt . plot ( hbins [: - 1 ], hpdf ) [<matplotlib.lines.Line2D at 0x7f03b1fce400>] plt . plot ( quantiles ) [<matplotlib.lines.Line2D at 0x7f03b28f0c40>] hist = np . histogram ( X . squeeze (), bins = 100 , density = True ) # plt.hist(hist[0], bins=100); hist_clf = stats . rv_histogram ( hist ) plot_dist ( hist_clf . pdf ( X )); # plot distribution plot_dist ( data_dist . pdf ( X ), nbins = 100 ); hist , bin_edges = np . histogram ( X . squeeze (), bins = 'auto' ) hist_clf = stats . rv_histogram (( hist , bin_edges )) # plot distribution plot_dist ( hist_clf . pdf ( X ), nbins = 100 );","title":"Demo 7.2 boundary issues"},{"location":"notebooks/demo_7.2_boundary_issues/#boundary-issues","text":"import os , sys cwd = os . getcwd () # sys.path.insert(0, f\"{cwd}/../\") sys . path . insert ( 0 , \"/home/emmanuel/code/rbig\" ) # RBIG Packages from rbig.data import ToyData from rbig.information.kde import KDESklearn from typing import Iterable , Optional , Dict , NamedTuple , Tuple , Union import numpy as np from sklearn.base import BaseEstimator , TransformerMixin import numpy as np from scipy import stats # Plot Functions import seaborn as sns import matplotlib.pyplot as plt plt . style . use ([ \"seaborn-paper\" ]) % load_ext autoreload % autoreload 2","title":"Boundary Issues"},{"location":"notebooks/demo_7.2_boundary_issues/#data","text":"def plot_dist ( X_samples : np . ndarray , bins = 100 , * args , ** kwargs ): fig , ax = plt . subplots () sns . distplot ( X_samples , bins = bins , ax = ax , kde_kws = { 'linewidth' : 7 , 'color' : 'black' , }, hist_kws = { 'color' : 'green' } ) return fig , ax seed = 123 n_samples = 10_000 a = 4 # initialize data distribution data_dist = stats . gamma ( a = a ) # get some samples X = data_dist . rvs ( size = ( n_samples , 1 ), random_state = seed ) # plot distribution plot_dist ( X , bins = 100 );","title":"Data"},{"location":"notebooks/demo_7.2_boundary_issues/#cdf-estimation","text":"For the CDF estimation, I will use the quantile method. # number of quantiles n_quantiles = 1_000 n_quantiles = max ( 1 , min ( n_quantiles , n_samples )) # calculate the reference values (support), [0, 1] references = np . linspace ( 0 , 1 , n_quantiles , endpoint = True ) # calculate quantiles quantiles = np . percentile ( X , references * 100 ) 0.16885335848364358 18.275140139054425","title":"CDF Estimation"},{"location":"notebooks/demo_7.2_boundary_issues/#plot-cdf-distribution","text":"# plot distribution plot_dist ( quantiles , nbins = 100 ); It looks really close to the original distribution. I think we can be happy with this.","title":"Plot CDF Distribution"},{"location":"notebooks/demo_7.2_boundary_issues/#plot-cdf-function","text":"# CDF plot fig , ax = plt . subplots () ax . plot ( quantiles , references , linestyle = '--' , color = 'black' ) ax . set_xlabel ( r 'Support Points' ) ax . set_ylabel ( r 'Empirical CDF' ) plt . show () This also looks OK. But what happens if we find some points that are outside of our distribution? In other words, what happens if I have points that lay outside of my support? print ( 'Data Boundaries:' , X . min (), X . max ()) print ( 'Quantile Boundaries:' , quantiles . min (), quantiles . max ()) Data Boundaries: 0.16885335848364358 18.275140139054425 Quantile Boundaries: 0.16885335848364358 18.275140139054425 So we can extend the boundary (the support) to ensure that we can accommadate points that lie outside of the region.","title":"Plot - CDF Function"},{"location":"notebooks/demo_7.2_boundary_issues/#method-i-extend-the-quantiles-boundaries","text":"support_extension = 10 domain = np . abs ( quantiles . max () - quantiles . min ()) domain_ext = ( 10 / 100 ) * domain lower_bound = quantiles . min () - domain_ext upper_bound = quantiles . max () + domain_ext print ( 'New Boundaries:' , lower_bound , upper_bound ) New Boundaries: -1.6417753195734346 20.085768817111504 new_quantiles = np . hstack ([ lower_bound , quantiles , upper_bound ]) # testing assert lower_bound == new_quantiles . min () assert upper_bound == new_quantiles . max () assert np . ndim ( new_quantiles ) == 1","title":"Method I - Extend the Quantiles Boundaries"},{"location":"notebooks/demo_7.2_boundary_issues/#interpolation","text":"We will now interpolate to ensure that the support gets extended. new_references = np . interp ( new_quantiles , quantiles , references ) print ( new_references . min (), new_references . max ()) 0.0 1.0 # CDF plot fig , ax = plt . subplots () ax . plot ( new_quantiles , new_references , linestyle = '--' , color = 'red' , label = 'Extended' ) ax . plot ( quantiles , references , linestyle = '--' , color = 'black' , label = 'Old' ) ax . legend ( fontsize = 12 ) ax . set_xlabel ( r 'Support Points' ) ax . set_ylabel ( r 'Empirical CDF' ) plt . show ()","title":"Interpolation"},{"location":"notebooks/demo_7.2_boundary_issues/#pdf-estimation","text":"quantiles . min (), quantiles . max () (0.16885335848364358, 18.275140139054425) plt . hist ( hist [ 0 ], bins = 100 ); hpdf , hbins = np . histogram ( X . squeeze (), bins = 100 , density = True ) plt . plot ( hbins [: - 1 ], hpdf ) [<matplotlib.lines.Line2D at 0x7f03b1fce400>] plt . plot ( quantiles ) [<matplotlib.lines.Line2D at 0x7f03b28f0c40>] hist = np . histogram ( X . squeeze (), bins = 100 , density = True ) # plt.hist(hist[0], bins=100); hist_clf = stats . rv_histogram ( hist ) plot_dist ( hist_clf . pdf ( X )); # plot distribution plot_dist ( data_dist . pdf ( X ), nbins = 100 ); hist , bin_edges = np . histogram ( X . squeeze (), bins = 'auto' ) hist_clf = stats . rv_histogram (( hist , bin_edges )) # plot distribution plot_dist ( hist_clf . pdf ( X ), nbins = 100 );","title":"PDF Estimation"}]}