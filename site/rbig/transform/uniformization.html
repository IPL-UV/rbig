<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>rbig.transform.uniformization API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rbig.transform.uniformization</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Callable, Optional, Tuple, Union, Dict

import numpy as np
from numpy.random import RandomState
from scipy import stats
from rbig.density.histogram import ScipyHistogram, QuantileHistogram
from rbig.density.kde import KDEFFT, KDEScipy, KDESklearn

# Base classes
from sklearn.utils import check_array, check_random_state
from sklearn.base import clone

from rbig.transform.base import DensityMixin, BaseTransform
import matplotlib.pyplot as plt

# from rbig.density.base
from rbig.utils import (
    get_domain_extension,
    bin_estimation,
    make_interior,
    check_input_output_dims,
)

BOUNDS_THRESHOLD = 1e-7


class HistogramUniformization(BaseTransform, DensityMixin):
    def __init__(
        self,
        bins: Union[int, str] = &#34;auto&#34;,
        alpha: float = 1e-5,
        prob_tol: float = 1e-7,
        n_quantiles: Optional[int] = None,
        support_extension: Union[float, int] = 10,
        subsample: Optional[int] = 10_000,
        random_state: int = 10,
        kwargs: Dict = {},
    ) -&gt; None:
        self.bins = bins
        self.alpha = alpha
        self.prob_tol = prob_tol
        self.n_quantiles = n_quantiles
        self.support_extension = support_extension
        self.subsample = subsample
        self.random_state = random_state
        self.kwargs = kwargs

    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; None:

        # fit superclass
        if self.n_quantiles is not None:
            self.estimator_ = QuantileHistogram(
                bins=self.bins,
                alpha=self.alpha,
                n_quantiles=self.n_quantiles,
                subsample=self.subsample,
                random_state=self.random_state,
                support_extension=self.support_extension,
            ).fit(X)
        else:
            self.estimator_ = ScipyHistogram(
                bins=self.bins,
                alpha=self.alpha,
                prob_tol=self.prob_tol,
                support_extension=self.support_extension,
            ).fit(X)
        return self

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Forward transform which is the CDF function of
        the samples

            z  = P(x)
            P() - CDF
        
        Parameters
        ----------
        X : np.ndarray, (n_samples, 1)
            incomining samples

        Returns
        -------
        X : np.ndarray
            transformed data
        &#34;&#34;&#34;
        X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

        return self.estimator_.cdf(X)

    def inverse_transform(self, X: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Inverse transform which is the Inverse CDF function
        applied to the samples

            x = P^-1(z)
            P^-1() - Inverse CDF (PPF)

        Parameters
        ----------
        X : np.ndarray, (n_samples, 1)
        
        Returns
        -------
        X : np.ndarray, (n_samples, 1)
            Transformed data
        &#34;&#34;&#34;
        X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

        return self.estimator_.ppf(X)

    def score_samples(
        self, X: np.ndarray, y: Optional[np.ndarray] = None
    ) -&gt; float:
        &#34;&#34;&#34;Returns the log likelihood. It
        calculates the mean of the log probability.
        &#34;&#34;&#34;
        # log_prob = self.log_abs_det_jacobian(X, y).sum(axis=1).reshape(-1, 1)

        # check_input_output_dims(log_prob, (X.shape[0], 1), &#34;Histogram&#34;, &#34;Log Prob&#34;)

        # return log_prob
        raise NotImplementedError

    def log_abs_det_jacobian(
        self, X: np.ndarray, y: Optional[np.ndarray] = None
    ) -&gt; float:
        &#34;&#34;&#34;Returns the log likelihood. It
        calculates the mean of the log probability.
        
        Parameters
        ----------
        X : np.ndarray, (n_samples, 1)
         incoming samples
        
        Returns
        -------
        X_jacobian : (n_samples, n_features),
            the mean of the log probability
        &#34;&#34;&#34;
        X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

        return self.estimator_.logpdf(X)

    def sample(
        self,
        n_samples: int = 1,
        random_state: Optional[Union[RandomState, int]] = None,
    ) -&gt; np.ndarray:
        &#34;&#34;&#34;Generate random samples from this.
        
        Parameters
        ----------
        n_samples : int, default=1
            The number of samples to generate. 
        
        random_state : int, RandomState,None, Optional, default=None
            The int to be used as a seed to generate the random 
            uniform samples.
        
        Returns
        -------
        X : np.ndarray, (n_samples, )
        &#34;&#34;&#34;
        #
        rng = check_random_state(random_state)

        U = rng.rand(n_samples, self.n_features_)

        X = self.inverse_transform(U)
        return X


class KDEUniformization(HistogramUniformization):
    def __init__(
        self,
        bw_method: str = &#34;scott&#34;,
        n_quantiles: int = 1_000,
        support_extension: Union[float, int] = 10,
        method: str = &#34;fft&#34;,
        algorithm: str = &#34;kd_tree&#34;,
        kernel: str = &#34;gaussian&#34;,
        metric: str = &#34;euclidean&#34;,
        kwargs: Dict = {},
    ) -&gt; None:
        self.bw_method = bw_method
        self.n_quantiles = n_quantiles
        self.support_extension = support_extension
        self.method = method
        self.algorithm = algorithm
        self.metric = metric
        self.kernel = kernel
        self.kwargs = kwargs

    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; None:

        # fit superclass
        if self.method in [&#34;knn&#34;]:
            self.estimator_ = KDESklearn(
                n_quantiles=self.n_quantiles,
                algorithm=self.algorithm,
                kernel=self.kernel,
                metric=self.metric,
                support_extension=self.support_extension,
            ).fit(X)
        elif self.method in [&#34;fft&#34;]:
            self.estimator_ = KDEFFT(
                bw_method=self.bw_method,
                n_quantiles=self.n_quantiles,
                support_extension=self.support_extension,
            ).fit(X)
        elif self.method in [&#34;exact&#34;]:
            self.estimator_ = KDEScipy(
                bw_method=self.bw_method,
                n_quantiles=self.n_quantiles,
                support_extension=self.support_extension,
            ).fit(X)
        else:
            raise ValueError(f&#34;Unrecognized method: {self.method}&#34;)
        return self


class MarginalUniformization(BaseTransform, DensityMixin):
    def __init__(self, uni_transformer) -&gt; None:
        self.uni_transformer = uni_transformer

    def fit(self, X: np.ndarray) -&gt; None:
        X = check_array(X, ensure_2d=True, copy=True)

        transforms = []

        for feature_idx in range(X.shape[1]):
            transformer = clone(self.uni_transformer)
            transforms.append(transformer.fit(X[:, feature_idx]))

        self.transforms_ = transforms

        return self

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        X = check_array(X, ensure_2d=True, copy=True)

        for feature_idx in range(X.shape[1]):

            X[:, feature_idx] = (
                self.transforms_[feature_idx]
                .transform(X[:, feature_idx])
                .squeeze()
            )

        return X

    def inverse_transform(self, X: np.ndarray) -&gt; np.ndarray:
        X = check_array(X, ensure_2d=True, copy=True)

        for feature_idx in range(X.shape[1]):

            X[:, feature_idx] = (
                self.transforms_[feature_idx]
                .inverse_transform(X[:, feature_idx])
                .squeeze()
            )

        return X

    def log_abs_det_jacobian(
        self, X: np.ndarray, y: Optional[np.ndarray] = None
    ) -&gt; float:
        X = check_array(X, ensure_2d=True, copy=True)
        for feature_idx in range(X.shape[1]):

            X[:, feature_idx] = (
                self.transforms_[feature_idx]
                .log_abs_det_jacobian(X[:, feature_idx],)
                .squeeze()
            )
        return X</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rbig.transform.uniformization.HistogramUniformization"><code class="flex name class">
<span>class <span class="ident">HistogramUniformization</span></span>
<span>(</span><span>bins: Union[int, str] = 'auto', alpha: float = 1e-05, prob_tol: float = 1e-07, n_quantiles: Union[int, NoneType] = None, support_extension: Union[float, int] = 10, subsample: Union[int, NoneType] = 10000, random_state: int = 10, kwargs: Dict = {})</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HistogramUniformization(BaseTransform, DensityMixin):
    def __init__(
        self,
        bins: Union[int, str] = &#34;auto&#34;,
        alpha: float = 1e-5,
        prob_tol: float = 1e-7,
        n_quantiles: Optional[int] = None,
        support_extension: Union[float, int] = 10,
        subsample: Optional[int] = 10_000,
        random_state: int = 10,
        kwargs: Dict = {},
    ) -&gt; None:
        self.bins = bins
        self.alpha = alpha
        self.prob_tol = prob_tol
        self.n_quantiles = n_quantiles
        self.support_extension = support_extension
        self.subsample = subsample
        self.random_state = random_state
        self.kwargs = kwargs

    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; None:

        # fit superclass
        if self.n_quantiles is not None:
            self.estimator_ = QuantileHistogram(
                bins=self.bins,
                alpha=self.alpha,
                n_quantiles=self.n_quantiles,
                subsample=self.subsample,
                random_state=self.random_state,
                support_extension=self.support_extension,
            ).fit(X)
        else:
            self.estimator_ = ScipyHistogram(
                bins=self.bins,
                alpha=self.alpha,
                prob_tol=self.prob_tol,
                support_extension=self.support_extension,
            ).fit(X)
        return self

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Forward transform which is the CDF function of
        the samples

            z  = P(x)
            P() - CDF
        
        Parameters
        ----------
        X : np.ndarray, (n_samples, 1)
            incomining samples

        Returns
        -------
        X : np.ndarray
            transformed data
        &#34;&#34;&#34;
        X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

        return self.estimator_.cdf(X)

    def inverse_transform(self, X: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Inverse transform which is the Inverse CDF function
        applied to the samples

            x = P^-1(z)
            P^-1() - Inverse CDF (PPF)

        Parameters
        ----------
        X : np.ndarray, (n_samples, 1)
        
        Returns
        -------
        X : np.ndarray, (n_samples, 1)
            Transformed data
        &#34;&#34;&#34;
        X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

        return self.estimator_.ppf(X)

    def score_samples(
        self, X: np.ndarray, y: Optional[np.ndarray] = None
    ) -&gt; float:
        &#34;&#34;&#34;Returns the log likelihood. It
        calculates the mean of the log probability.
        &#34;&#34;&#34;
        # log_prob = self.log_abs_det_jacobian(X, y).sum(axis=1).reshape(-1, 1)

        # check_input_output_dims(log_prob, (X.shape[0], 1), &#34;Histogram&#34;, &#34;Log Prob&#34;)

        # return log_prob
        raise NotImplementedError

    def log_abs_det_jacobian(
        self, X: np.ndarray, y: Optional[np.ndarray] = None
    ) -&gt; float:
        &#34;&#34;&#34;Returns the log likelihood. It
        calculates the mean of the log probability.
        
        Parameters
        ----------
        X : np.ndarray, (n_samples, 1)
         incoming samples
        
        Returns
        -------
        X_jacobian : (n_samples, n_features),
            the mean of the log probability
        &#34;&#34;&#34;
        X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

        return self.estimator_.logpdf(X)

    def sample(
        self,
        n_samples: int = 1,
        random_state: Optional[Union[RandomState, int]] = None,
    ) -&gt; np.ndarray:
        &#34;&#34;&#34;Generate random samples from this.
        
        Parameters
        ----------
        n_samples : int, default=1
            The number of samples to generate. 
        
        random_state : int, RandomState,None, Optional, default=None
            The int to be used as a seed to generate the random 
            uniform samples.
        
        Returns
        -------
        X : np.ndarray, (n_samples, )
        &#34;&#34;&#34;
        #
        rng = check_random_state(random_state)

        U = rng.rand(n_samples, self.n_features_)

        X = self.inverse_transform(U)
        return X</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rbig.transform.base.BaseTransform" href="base.html#rbig.transform.base.BaseTransform">BaseTransform</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.TransformerMixin</li>
<li><a title="rbig.transform.base.DensityMixin" href="base.html#rbig.transform.base.DensityMixin">DensityMixin</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="rbig.transform.uniformization.KDEUniformization" href="#rbig.transform.uniformization.KDEUniformization">KDEUniformization</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="rbig.transform.uniformization.HistogramUniformization.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X: numpy.ndarray, y: Union[numpy.ndarray, NoneType] = None) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; None:

    # fit superclass
    if self.n_quantiles is not None:
        self.estimator_ = QuantileHistogram(
            bins=self.bins,
            alpha=self.alpha,
            n_quantiles=self.n_quantiles,
            subsample=self.subsample,
            random_state=self.random_state,
            support_extension=self.support_extension,
        ).fit(X)
    else:
        self.estimator_ = ScipyHistogram(
            bins=self.bins,
            alpha=self.alpha,
            prob_tol=self.prob_tol,
            support_extension=self.support_extension,
        ).fit(X)
    return self</code></pre>
</details>
</dd>
<dt id="rbig.transform.uniformization.HistogramUniformization.inverse_transform"><code class="name flex">
<span>def <span class="ident">inverse_transform</span></span>(<span>self, X: numpy.ndarray) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse transform which is the Inverse CDF function
applied to the samples</p>
<pre><code>x = P^-1(z)
P^-1() - Inverse CDF (PPF)
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray, (n_samples, 1)</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray, (n_samples, 1)</code></dt>
<dd>Transformed data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_transform(self, X: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Inverse transform which is the Inverse CDF function
    applied to the samples

        x = P^-1(z)
        P^-1() - Inverse CDF (PPF)

    Parameters
    ----------
    X : np.ndarray, (n_samples, 1)
    
    Returns
    -------
    X : np.ndarray, (n_samples, 1)
        Transformed data
    &#34;&#34;&#34;
    X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

    return self.estimator_.ppf(X)</code></pre>
</details>
</dd>
<dt id="rbig.transform.uniformization.HistogramUniformization.log_abs_det_jacobian"><code class="name flex">
<span>def <span class="ident">log_abs_det_jacobian</span></span>(<span>self, X: numpy.ndarray, y: Union[numpy.ndarray, NoneType] = None) -> float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the log likelihood. It
calculates the mean of the log probability.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray, (n_samples, 1)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>incoming samples</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_jacobian</code></strong> :&ensp;<code>(n_samples, n_features),</code></dt>
<dd>the mean of the log probability</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_abs_det_jacobian(
    self, X: np.ndarray, y: Optional[np.ndarray] = None
) -&gt; float:
    &#34;&#34;&#34;Returns the log likelihood. It
    calculates the mean of the log probability.
    
    Parameters
    ----------
    X : np.ndarray, (n_samples, 1)
     incoming samples
    
    Returns
    -------
    X_jacobian : (n_samples, n_features),
        the mean of the log probability
    &#34;&#34;&#34;
    X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

    return self.estimator_.logpdf(X)</code></pre>
</details>
</dd>
<dt id="rbig.transform.uniformization.HistogramUniformization.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self, n_samples: int = 1, random_state: Union[numpy.random.mtrand.RandomState, int, NoneType] = None) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Generate random samples from this.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>The number of samples to generate.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int, RandomState,None, Optional</code>, default=<code>None</code></dt>
<dd>The int to be used as a seed to generate the random
uniform samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray, (n_samples, )</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(
    self,
    n_samples: int = 1,
    random_state: Optional[Union[RandomState, int]] = None,
) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate random samples from this.
    
    Parameters
    ----------
    n_samples : int, default=1
        The number of samples to generate. 
    
    random_state : int, RandomState,None, Optional, default=None
        The int to be used as a seed to generate the random 
        uniform samples.
    
    Returns
    -------
    X : np.ndarray, (n_samples, )
    &#34;&#34;&#34;
    #
    rng = check_random_state(random_state)

    U = rng.rand(n_samples, self.n_features_)

    X = self.inverse_transform(U)
    return X</code></pre>
</details>
</dd>
<dt id="rbig.transform.uniformization.HistogramUniformization.score_samples"><code class="name flex">
<span>def <span class="ident">score_samples</span></span>(<span>self, X: numpy.ndarray, y: Union[numpy.ndarray, NoneType] = None) -> float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the log likelihood. It
calculates the mean of the log probability.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score_samples(
    self, X: np.ndarray, y: Optional[np.ndarray] = None
) -&gt; float:
    &#34;&#34;&#34;Returns the log likelihood. It
    calculates the mean of the log probability.
    &#34;&#34;&#34;
    # log_prob = self.log_abs_det_jacobian(X, y).sum(axis=1).reshape(-1, 1)

    # check_input_output_dims(log_prob, (X.shape[0], 1), &#34;Histogram&#34;, &#34;Log Prob&#34;)

    # return log_prob
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="rbig.transform.uniformization.HistogramUniformization.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X: numpy.ndarray) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Forward transform which is the CDF function of
the samples</p>
<pre><code>z  = P(x)
P() - CDF
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray, (n_samples, 1)</code></dt>
<dd>incomining samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>transformed data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Forward transform which is the CDF function of
    the samples

        z  = P(x)
        P() - CDF
    
    Parameters
    ----------
    X : np.ndarray, (n_samples, 1)
        incomining samples

    Returns
    -------
    X : np.ndarray
        transformed data
    &#34;&#34;&#34;
    X = check_array(X.reshape(-1, 1), ensure_2d=True, copy=True)

    return self.estimator_.cdf(X)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rbig.transform.base.DensityMixin" href="base.html#rbig.transform.base.DensityMixin">DensityMixin</a></b></code>:
<ul class="hlist">
<li><code><a title="rbig.transform.base.DensityMixin.score" href="base.html#rbig.transform.base.DensityMixin.score">score</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="rbig.transform.uniformization.KDEUniformization"><code class="flex name class">
<span>class <span class="ident">KDEUniformization</span></span>
<span>(</span><span>bw_method: str = 'scott', n_quantiles: int = 1000, support_extension: Union[float, int] = 10, method: str = 'fft', algorithm: str = 'kd_tree', kernel: str = 'gaussian', metric: str = 'euclidean', kwargs: Dict = {})</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KDEUniformization(HistogramUniformization):
    def __init__(
        self,
        bw_method: str = &#34;scott&#34;,
        n_quantiles: int = 1_000,
        support_extension: Union[float, int] = 10,
        method: str = &#34;fft&#34;,
        algorithm: str = &#34;kd_tree&#34;,
        kernel: str = &#34;gaussian&#34;,
        metric: str = &#34;euclidean&#34;,
        kwargs: Dict = {},
    ) -&gt; None:
        self.bw_method = bw_method
        self.n_quantiles = n_quantiles
        self.support_extension = support_extension
        self.method = method
        self.algorithm = algorithm
        self.metric = metric
        self.kernel = kernel
        self.kwargs = kwargs

    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; None:

        # fit superclass
        if self.method in [&#34;knn&#34;]:
            self.estimator_ = KDESklearn(
                n_quantiles=self.n_quantiles,
                algorithm=self.algorithm,
                kernel=self.kernel,
                metric=self.metric,
                support_extension=self.support_extension,
            ).fit(X)
        elif self.method in [&#34;fft&#34;]:
            self.estimator_ = KDEFFT(
                bw_method=self.bw_method,
                n_quantiles=self.n_quantiles,
                support_extension=self.support_extension,
            ).fit(X)
        elif self.method in [&#34;exact&#34;]:
            self.estimator_ = KDEScipy(
                bw_method=self.bw_method,
                n_quantiles=self.n_quantiles,
                support_extension=self.support_extension,
            ).fit(X)
        else:
            raise ValueError(f&#34;Unrecognized method: {self.method}&#34;)
        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rbig.transform.uniformization.HistogramUniformization" href="#rbig.transform.uniformization.HistogramUniformization">HistogramUniformization</a></li>
<li><a title="rbig.transform.base.BaseTransform" href="base.html#rbig.transform.base.BaseTransform">BaseTransform</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.TransformerMixin</li>
<li><a title="rbig.transform.base.DensityMixin" href="base.html#rbig.transform.base.DensityMixin">DensityMixin</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="rbig.transform.uniformization.KDEUniformization.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X: numpy.ndarray, y: Union[numpy.ndarray, NoneType] = None) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; None:

    # fit superclass
    if self.method in [&#34;knn&#34;]:
        self.estimator_ = KDESklearn(
            n_quantiles=self.n_quantiles,
            algorithm=self.algorithm,
            kernel=self.kernel,
            metric=self.metric,
            support_extension=self.support_extension,
        ).fit(X)
    elif self.method in [&#34;fft&#34;]:
        self.estimator_ = KDEFFT(
            bw_method=self.bw_method,
            n_quantiles=self.n_quantiles,
            support_extension=self.support_extension,
        ).fit(X)
    elif self.method in [&#34;exact&#34;]:
        self.estimator_ = KDEScipy(
            bw_method=self.bw_method,
            n_quantiles=self.n_quantiles,
            support_extension=self.support_extension,
        ).fit(X)
    else:
        raise ValueError(f&#34;Unrecognized method: {self.method}&#34;)
    return self</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rbig.transform.uniformization.HistogramUniformization" href="#rbig.transform.uniformization.HistogramUniformization">HistogramUniformization</a></b></code>:
<ul class="hlist">
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.inverse_transform" href="#rbig.transform.uniformization.HistogramUniformization.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.log_abs_det_jacobian" href="#rbig.transform.uniformization.HistogramUniformization.log_abs_det_jacobian">log_abs_det_jacobian</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.sample" href="#rbig.transform.uniformization.HistogramUniformization.sample">sample</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.score" href="base.html#rbig.transform.base.DensityMixin.score">score</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.score_samples" href="#rbig.transform.uniformization.HistogramUniformization.score_samples">score_samples</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.transform" href="#rbig.transform.uniformization.HistogramUniformization.transform">transform</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="rbig.transform.uniformization.MarginalUniformization"><code class="flex name class">
<span>class <span class="ident">MarginalUniformization</span></span>
<span>(</span><span>uni_transformer)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all estimators in scikit-learn</p>
<h2 id="notes">Notes</h2>
<p>All estimators should specify all the parameters that can be set
at the class level in their <code>__init__</code> as explicit keyword
arguments (no <code>*args</code> or <code>**kwargs</code>).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MarginalUniformization(BaseTransform, DensityMixin):
    def __init__(self, uni_transformer) -&gt; None:
        self.uni_transformer = uni_transformer

    def fit(self, X: np.ndarray) -&gt; None:
        X = check_array(X, ensure_2d=True, copy=True)

        transforms = []

        for feature_idx in range(X.shape[1]):
            transformer = clone(self.uni_transformer)
            transforms.append(transformer.fit(X[:, feature_idx]))

        self.transforms_ = transforms

        return self

    def transform(self, X: np.ndarray) -&gt; np.ndarray:
        X = check_array(X, ensure_2d=True, copy=True)

        for feature_idx in range(X.shape[1]):

            X[:, feature_idx] = (
                self.transforms_[feature_idx]
                .transform(X[:, feature_idx])
                .squeeze()
            )

        return X

    def inverse_transform(self, X: np.ndarray) -&gt; np.ndarray:
        X = check_array(X, ensure_2d=True, copy=True)

        for feature_idx in range(X.shape[1]):

            X[:, feature_idx] = (
                self.transforms_[feature_idx]
                .inverse_transform(X[:, feature_idx])
                .squeeze()
            )

        return X

    def log_abs_det_jacobian(
        self, X: np.ndarray, y: Optional[np.ndarray] = None
    ) -&gt; float:
        X = check_array(X, ensure_2d=True, copy=True)
        for feature_idx in range(X.shape[1]):

            X[:, feature_idx] = (
                self.transforms_[feature_idx]
                .log_abs_det_jacobian(X[:, feature_idx],)
                .squeeze()
            )
        return X</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rbig.transform.base.BaseTransform" href="base.html#rbig.transform.base.BaseTransform">BaseTransform</a></li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.base.TransformerMixin</li>
<li><a title="rbig.transform.base.DensityMixin" href="base.html#rbig.transform.base.DensityMixin">DensityMixin</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="rbig.transform.uniformization.MarginalUniformization.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X: numpy.ndarray) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X: np.ndarray) -&gt; None:
    X = check_array(X, ensure_2d=True, copy=True)

    transforms = []

    for feature_idx in range(X.shape[1]):
        transformer = clone(self.uni_transformer)
        transforms.append(transformer.fit(X[:, feature_idx]))

    self.transforms_ = transforms

    return self</code></pre>
</details>
</dd>
<dt id="rbig.transform.uniformization.MarginalUniformization.inverse_transform"><code class="name flex">
<span>def <span class="ident">inverse_transform</span></span>(<span>self, X: numpy.ndarray) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_transform(self, X: np.ndarray) -&gt; np.ndarray:
    X = check_array(X, ensure_2d=True, copy=True)

    for feature_idx in range(X.shape[1]):

        X[:, feature_idx] = (
            self.transforms_[feature_idx]
            .inverse_transform(X[:, feature_idx])
            .squeeze()
        )

    return X</code></pre>
</details>
</dd>
<dt id="rbig.transform.uniformization.MarginalUniformization.log_abs_det_jacobian"><code class="name flex">
<span>def <span class="ident">log_abs_det_jacobian</span></span>(<span>self, X: numpy.ndarray, y: Union[numpy.ndarray, NoneType] = None) -> float</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_abs_det_jacobian(
    self, X: np.ndarray, y: Optional[np.ndarray] = None
) -&gt; float:
    X = check_array(X, ensure_2d=True, copy=True)
    for feature_idx in range(X.shape[1]):

        X[:, feature_idx] = (
            self.transforms_[feature_idx]
            .log_abs_det_jacobian(X[:, feature_idx],)
            .squeeze()
        )
    return X</code></pre>
</details>
</dd>
<dt id="rbig.transform.uniformization.MarginalUniformization.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X: numpy.ndarray) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X: np.ndarray) -&gt; np.ndarray:
    X = check_array(X, ensure_2d=True, copy=True)

    for feature_idx in range(X.shape[1]):

        X[:, feature_idx] = (
            self.transforms_[feature_idx]
            .transform(X[:, feature_idx])
            .squeeze()
        )

    return X</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rbig.transform.base.DensityMixin" href="base.html#rbig.transform.base.DensityMixin">DensityMixin</a></b></code>:
<ul class="hlist">
<li><code><a title="rbig.transform.base.DensityMixin.sample" href="base.html#rbig.transform.base.DensityMixin.sample">sample</a></code></li>
<li><code><a title="rbig.transform.base.DensityMixin.score" href="base.html#rbig.transform.base.DensityMixin.score">score</a></code></li>
<li><code><a title="rbig.transform.base.DensityMixin.score_samples" href="base.html#rbig.transform.base.DensityMixin.score_samples">score_samples</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rbig.transform" href="index.html">rbig.transform</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rbig.transform.uniformization.HistogramUniformization" href="#rbig.transform.uniformization.HistogramUniformization">HistogramUniformization</a></code></h4>
<ul class="">
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.fit" href="#rbig.transform.uniformization.HistogramUniformization.fit">fit</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.inverse_transform" href="#rbig.transform.uniformization.HistogramUniformization.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.log_abs_det_jacobian" href="#rbig.transform.uniformization.HistogramUniformization.log_abs_det_jacobian">log_abs_det_jacobian</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.sample" href="#rbig.transform.uniformization.HistogramUniformization.sample">sample</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.score_samples" href="#rbig.transform.uniformization.HistogramUniformization.score_samples">score_samples</a></code></li>
<li><code><a title="rbig.transform.uniformization.HistogramUniformization.transform" href="#rbig.transform.uniformization.HistogramUniformization.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="rbig.transform.uniformization.KDEUniformization" href="#rbig.transform.uniformization.KDEUniformization">KDEUniformization</a></code></h4>
<ul class="">
<li><code><a title="rbig.transform.uniformization.KDEUniformization.fit" href="#rbig.transform.uniformization.KDEUniformization.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="rbig.transform.uniformization.MarginalUniformization" href="#rbig.transform.uniformization.MarginalUniformization">MarginalUniformization</a></code></h4>
<ul class="">
<li><code><a title="rbig.transform.uniformization.MarginalUniformization.fit" href="#rbig.transform.uniformization.MarginalUniformization.fit">fit</a></code></li>
<li><code><a title="rbig.transform.uniformization.MarginalUniformization.inverse_transform" href="#rbig.transform.uniformization.MarginalUniformization.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="rbig.transform.uniformization.MarginalUniformization.log_abs_det_jacobian" href="#rbig.transform.uniformization.MarginalUniformization.log_abs_det_jacobian">log_abs_det_jacobian</a></code></li>
<li><code><a title="rbig.transform.uniformization.MarginalUniformization.transform" href="#rbig.transform.uniformization.MarginalUniformization.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>