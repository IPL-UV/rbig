<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>rbig.losses.information API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rbig.losses.information</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Dict, Optional

import numpy as np
from scipy.interpolate import interp1d
from sklearn.utils import check_array

from rbig.information.entropy import marginal_entropy
from rbig.losses import RBIGLoss


class InformationLoss(RBIGLoss):
    def __init__(
        self,
        method: str = &#34;histogram&#34;,
        p_value: float = 0.25,
        kwargs: Dict = {},
        tol_layers: Optional[int] = None,
    ) -&gt; None:

        super().__init__(n_layers=None, tol_layers=tol_layers)
        self.method = method
        self.p_value = p_value
        self.kwargs = kwargs
        self.info_losses_ = list()

    def calculate_loss(
        self,
        Xtrans: np.ndarray,
        X: np.ndarray,
        dX: Optional[np.ndarray] = None,
    ) -&gt; float:

        delta_tc = information_reduction(
            X,
            Xtrans,
            method=self.method,
            p_value=self.p_value,
            kwargs=self.kwargs,
        )
        # add loss values
        self.loss_vals.append(delta_tc)

        # return change in marginal entropy
        return delta_tc

    def check_tolerance(self, current_layer: int) -&gt; bool:
        if current_layer &lt;= self.tol_layers:
            return True
        else:
            # get the abs sum of the last n layers
            tol = np.sum(self.loss_vals[-self.tol_layers :])

            # calculate the changes
            if tol == 0:
                # no changes, don&#39;t use the last n layers
                self.loss_vals = self.loss_vals[: self.tol_layers]
                return False
            else:
                # continue
                return True


def information_reduction(
    X: np.ndarray,
    Y: np.ndarray,
    method: str = &#34;histogram&#34;,
    p_value: float = 0.25,
    kwargs: Optional[Dict] = {},
) -&gt; float:
    &#34;&#34;&#34;Calculates the information reduction between two datasets

    We find the sum of the marginal entropy measures for each dataset and 
    then we calculate the difference between the two.
    We assume that some transformation was applied to Y and it stems from X, 
        e.g. Y = AX.

    **Note**:
    * if X,Y is 1D then this is change in entropy
    * if X,Y is 2D then this is the change in mutual information
    * if X,Y is &gt;=3D then this is the change in total correlation

    Parameters
    ----------
    X : np.ndarray, (n_samples, n_features)
        the inputs of the array

    Y : np.narray, (n_samples, n_features)
        the transformed variable

    method : str, default=&#39;histogram&#39;
        the univariate entropy estimator

    p_value : float, default=0.25
        a tolerance indicator for the change in entropy

    kwargs : Dict, default={}
        extra keyword arguments for the marginal estimators
        please see rbig.information.entropy.marginal_entropy for 
        more details

    Returns
    -------
    delta_I : float,
        the change in information (reduction of information)
        between the two inputs
    &#34;&#34;&#34;
    X = check_array(X, ensure_2d=True)
    Y = check_array(Y, ensure_2d=True)

    # get tolerance dimensions
    n_samples, n_dimensions = X.shape
    tol = get_reduction_tolerance(n_samples)

    # calculate the marginal entropy

    H_x = marginal_entropy(X, method=method, **kwargs)
    H_y = marginal_entropy(Y, method=method, **kwargs)
    # print(H_x, H_y)

    # Find change in information content
    tc_term1 = np.sum(H_y) - np.sum(H_x)
    tc_term2 = np.sqrt(np.sum((H_x - H_y) ** 2))
    # print(tc_term1, tc_term2)

    if tc_term2 &lt; np.sqrt(n_dimensions * p_value * tol ** 2) or tc_term1 &lt; 0:
        delta_info = 0
    else:
        delta_info = tc_term1

    return delta_info


def get_reduction_tolerance(n_samples: int) -&gt; float:
    &#34;&#34;&#34;A tolerance indicator for the information reduction&#34;&#34;&#34;
    xxx = np.logspace(2, 8, 7)
    yyy = [0.1571, 0.0468, 0.0145, 0.0046, 0.0014, 0.0001, 0.00001]
    tol = interp1d(xxx, yyy)(n_samples)
    return tol</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="rbig.losses.information.get_reduction_tolerance"><code class="name flex">
<span>def <span class="ident">get_reduction_tolerance</span></span>(<span>n_samples: int) -> float</span>
</code></dt>
<dd>
<div class="desc"><p>A tolerance indicator for the information reduction</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_reduction_tolerance(n_samples: int) -&gt; float:
    &#34;&#34;&#34;A tolerance indicator for the information reduction&#34;&#34;&#34;
    xxx = np.logspace(2, 8, 7)
    yyy = [0.1571, 0.0468, 0.0145, 0.0046, 0.0014, 0.0001, 0.00001]
    tol = interp1d(xxx, yyy)(n_samples)
    return tol</code></pre>
</details>
</dd>
<dt id="rbig.losses.information.information_reduction"><code class="name flex">
<span>def <span class="ident">information_reduction</span></span>(<span>X: numpy.ndarray, Y: numpy.ndarray, method: str = 'histogram', p_value: float = 0.25, kwargs: Union[Dict, NoneType] = {}) -> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the information reduction between two datasets</p>
<p>We find the sum of the marginal entropy measures for each dataset and
then we calculate the difference between the two.
We assume that some transformation was applied to Y and it stems from X,
e.g. Y = AX.</p>
<p><strong>Note</strong>:
* if X,Y is 1D then this is change in entropy
* if X,Y is 2D then this is the change in mutual information
* if X,Y is &gt;=3D then this is the change in total correlation</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray, (n_samples, n_features)</code></dt>
<dd>the inputs of the array</dd>
<dt><strong><code>Y</code></strong> :&ensp;<code>np.narray, (n_samples, n_features)</code></dt>
<dd>the transformed variable</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code>, default=<code>'histogram'</code></dt>
<dd>the univariate entropy estimator</dd>
<dt><strong><code>p_value</code></strong> :&ensp;<code>float</code>, default=<code>0.25</code></dt>
<dd>a tolerance indicator for the change in entropy</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>Dict</code>, default=<code>{}</code></dt>
<dd>extra keyword arguments for the marginal estimators
please see rbig.information.entropy.marginal_entropy for
more details</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>delta_I</code></strong> :&ensp;<code>float,</code></dt>
<dd>the change in information (reduction of information)
between the two inputs</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def information_reduction(
    X: np.ndarray,
    Y: np.ndarray,
    method: str = &#34;histogram&#34;,
    p_value: float = 0.25,
    kwargs: Optional[Dict] = {},
) -&gt; float:
    &#34;&#34;&#34;Calculates the information reduction between two datasets

    We find the sum of the marginal entropy measures for each dataset and 
    then we calculate the difference between the two.
    We assume that some transformation was applied to Y and it stems from X, 
        e.g. Y = AX.

    **Note**:
    * if X,Y is 1D then this is change in entropy
    * if X,Y is 2D then this is the change in mutual information
    * if X,Y is &gt;=3D then this is the change in total correlation

    Parameters
    ----------
    X : np.ndarray, (n_samples, n_features)
        the inputs of the array

    Y : np.narray, (n_samples, n_features)
        the transformed variable

    method : str, default=&#39;histogram&#39;
        the univariate entropy estimator

    p_value : float, default=0.25
        a tolerance indicator for the change in entropy

    kwargs : Dict, default={}
        extra keyword arguments for the marginal estimators
        please see rbig.information.entropy.marginal_entropy for 
        more details

    Returns
    -------
    delta_I : float,
        the change in information (reduction of information)
        between the two inputs
    &#34;&#34;&#34;
    X = check_array(X, ensure_2d=True)
    Y = check_array(Y, ensure_2d=True)

    # get tolerance dimensions
    n_samples, n_dimensions = X.shape
    tol = get_reduction_tolerance(n_samples)

    # calculate the marginal entropy

    H_x = marginal_entropy(X, method=method, **kwargs)
    H_y = marginal_entropy(Y, method=method, **kwargs)
    # print(H_x, H_y)

    # Find change in information content
    tc_term1 = np.sum(H_y) - np.sum(H_x)
    tc_term2 = np.sqrt(np.sum((H_x - H_y) ** 2))
    # print(tc_term1, tc_term2)

    if tc_term2 &lt; np.sqrt(n_dimensions * p_value * tol ** 2) or tc_term1 &lt; 0:
        delta_info = 0
    else:
        delta_info = tc_term1

    return delta_info</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rbig.losses.information.InformationLoss"><code class="flex name class">
<span>class <span class="ident">InformationLoss</span></span>
<span>(</span><span>method: str = 'histogram', p_value: float = 0.25, kwargs: Dict = {}, tol_layers: Union[int, NoneType] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InformationLoss(RBIGLoss):
    def __init__(
        self,
        method: str = &#34;histogram&#34;,
        p_value: float = 0.25,
        kwargs: Dict = {},
        tol_layers: Optional[int] = None,
    ) -&gt; None:

        super().__init__(n_layers=None, tol_layers=tol_layers)
        self.method = method
        self.p_value = p_value
        self.kwargs = kwargs
        self.info_losses_ = list()

    def calculate_loss(
        self,
        Xtrans: np.ndarray,
        X: np.ndarray,
        dX: Optional[np.ndarray] = None,
    ) -&gt; float:

        delta_tc = information_reduction(
            X,
            Xtrans,
            method=self.method,
            p_value=self.p_value,
            kwargs=self.kwargs,
        )
        # add loss values
        self.loss_vals.append(delta_tc)

        # return change in marginal entropy
        return delta_tc

    def check_tolerance(self, current_layer: int) -&gt; bool:
        if current_layer &lt;= self.tol_layers:
            return True
        else:
            # get the abs sum of the last n layers
            tol = np.sum(self.loss_vals[-self.tol_layers :])

            # calculate the changes
            if tol == 0:
                # no changes, don&#39;t use the last n layers
                self.loss_vals = self.loss_vals[: self.tol_layers]
                return False
            else:
                # continue
                return True</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rbig.losses.base.RBIGLoss" href="base.html#rbig.losses.base.RBIGLoss">RBIGLoss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rbig.losses.base.RBIGLoss" href="base.html#rbig.losses.base.RBIGLoss">RBIGLoss</a></b></code>:
<ul class="hlist">
<li><code><a title="rbig.losses.base.RBIGLoss.calculate_loss" href="base.html#rbig.losses.base.RBIGLoss.calculate_loss">calculate_loss</a></code></li>
<li><code><a title="rbig.losses.base.RBIGLoss.check_tolerance" href="base.html#rbig.losses.base.RBIGLoss.check_tolerance">check_tolerance</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rbig.losses" href="index.html">rbig.losses</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="rbig.losses.information.get_reduction_tolerance" href="#rbig.losses.information.get_reduction_tolerance">get_reduction_tolerance</a></code></li>
<li><code><a title="rbig.losses.information.information_reduction" href="#rbig.losses.information.information_reduction">information_reduction</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rbig.losses.information.InformationLoss" href="#rbig.losses.information.InformationLoss">InformationLoss</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>